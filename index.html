<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Awesome 3D Gaussian Splatting Paper List</title>

    <!-- Preconnects -->
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    <link rel="preconnect" href="https://raw.githubusercontent.com">

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    
    <!-- Site CSS -->
    <style>
        /* Root Variables */
:root {
    --primary-color: #1772d0;
    --hover-color: #f09228;
    --bg-color: #ffffff;
    --card-bg: #ffffff;
    --border-color: #e5e7eb;
    --text-color: #1f2937;
}

/* Base Styles */
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    margin: 0;
    padding: 20px;
    background-color: var(--bg-color);
    color: var(--text-color);
    line-height: 1.5;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 20px;
    position: relative;
}

h1 {
    text-align: center;
    font-size: 2.5rem;
    margin-bottom: 2rem;
    color: var(--text-color);
}

/* Donation Box */
.donate-box {
    background-color: #f8fafc;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    padding: 1.5rem;
    margin-bottom: 2rem;
    text-align: center;
}

.donate-box h3 {
    margin-top: 0;
    color: var(--primary-color);
    font-size: 1.25rem;
    margin-bottom: 0.5rem;
}

.donate-box p {
    margin: 0.5rem 0 1rem;
    color: #4b5563;
}

.bitcoin-info {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 1rem;
    flex-wrap: wrap;
    margin-top: 1rem;
}

.bitcoin-label {
    font-weight: 600;
    color: #4b5563;
}

.bitcoin-address {
    background: #fff;
    padding: 0.5rem 1rem;
    border-radius: 0.25rem;
    border: 1px solid var(--border-color);
    font-family: monospace;
    font-size: 0.9rem;
}

.copy-button, .sponsor-button {
    background-color: var(--primary-color);
    color: white;
    border: none;
    border-radius: 0.25rem;
    padding: 0.5rem 1rem;
    cursor: pointer;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    transition: background-color 0.2s;
    text-decoration: none;
    font-size: 0.9rem;
}

.copy-button:hover, .sponsor-button:hover {
    background-color: var(--hover-color);
}

/* Filter Info */
.filter-info {
    background-color: #f8fafc;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    padding: 1rem 1.5rem;
    margin-bottom: 2rem;
}

.filter-info h3 {
    margin-top: 0;
    color: var(--primary-color);
    font-size: 1.1rem;
}

.filter-info p {
    margin: 0.5rem 0;
    color: #4b5563;
}

/* Search and Filters */
.filters {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    margin-bottom: 2rem;
    align-items: center;
    justify-content: space-between;
}

.search-wrapper {
    position: relative;
    flex: 1;
    min-width: 200px;
    max-width: calc(100% - 20px);
}

.search-box {
    width: 100%;
    padding: 0.75rem 1rem;
    padding-right: 2.5rem;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    font-size: 1rem;
    box-sizing: border-box;
}

.clear-search-btn {
    position: absolute;
    right: 10px;
    top: 50%;
    transform: translateY(-50%);
    background: none;
    border: none;
    cursor: pointer;
    color: #9ca3af;
    font-size: 1.2rem;
    padding: 0.25rem;
    z-index: 1;
    line-height: 1;
}

.clear-search-btn:hover {
    color: #dc2626;
}

.filter-select {
    padding: 0.75rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    background-color: white;
}

/* Tag Filters */
.tag-filters {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    margin-bottom: 2rem;
}

.tag-filter {
    background: #f3f4f6;
    border: none;
    padding: 0.5rem 1rem;
    border-radius: 0.5rem;
    cursor: pointer;
    font-size: 0.9rem;
    transition: all 0.2s;
    color: var(--text-color);
}

.tag-filter:hover {
    background: #e5e7eb;
}

.tag-filter.include {
    background: var(--primary-color);
    color: white;
}

.tag-filter.exclude {
    background: #dc2626;
    color: white;
}

/* Floating Navigation */
.floating-nav {
    position: fixed;
    bottom: 180px;
    right: 2rem;
    display: flex;
    flex-direction: column;
    gap: 1rem;
    z-index: 1000;
}

.floating-nav button {
    width: 48px;
    height: 48px;
    border-radius: 50%;
    background: rgba(255, 255, 255, 0.9);
    border: 1px solid var(--border-color);
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    transition: all 0.2s;
}

.floating-nav button:hover {
    background: white;
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

.scroll-progress {
    width: 48px;
    height: 48px;
    border-radius: 50%;
    background: rgba(255, 255, 255, 0.9);
    border: 1px solid var(--border-color);
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 0.875rem;
    font-weight: 500;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

/* Selection Preview Bar */
.selection-preview {
    position: sticky;
    top: 0;
    left: 0;
    right: 0;
    width: 100%;
    background: white;
    border-bottom: 1px solid var(--border-color);
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    z-index: 1000;
    display: none;
    margin-bottom: 2rem;
}

.selection-mode .selection-preview {
    display: block;
}

/* Filter Status Bar */
.filter-status {
    position: sticky;
    top: 0;
    left: 0;
    right: 0;
    width: 100%;
    background: white;
    border-bottom: 1px solid var(--border-color);
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    z-index: 999;
    margin-bottom: 2rem;
}

/* Preview Header (shared by both bars) */
.preview-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 0.75rem 1rem;
    background: #f8fafc;
    border-bottom: 1px solid var(--border-color);
}

.preview-header-left {
    font-weight: 600;
    color: var(--text-color);
    display: flex;
    align-items: center;
    gap: 1rem;
}

.preview-header-right {
    display: flex;
    align-items: center;
    gap: 0.75rem;
}

.selection-counter,
.filter-counter {
    color: #6b7280;
    font-size: 0.9rem;
}

.preview-container {
    padding: 0.75rem 1rem;
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

/* Preview Items (shared style for selection and filter tags) */
.preview-item,
.filter-tag {
    flex: 0 0 auto;
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.75rem;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    background: white;
    max-width: 300px;
    margin-bottom: 0.5rem;
}

.preview-content,
.filter-tag-content {
    flex: 1;
    min-width: 0;
    cursor: pointer;
}

.preview-title,
.filter-tag-title {
    font-weight: 600;
    margin-bottom: 0.25rem;
    font-size: 0.9rem;
    color: var(--text-color);
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

.preview-authors,
.filter-tag-info {
    font-size: 0.8rem;
    color: #4b5563;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

/* Filter Tag Variations */
.filter-tag.search {
    background: #e6f3ff;
    border-color: #1a73e8;
}

.filter-tag.year {
    background: #e6f7e6;
    border-color: #1e8e1e;
}

.filter-tag.tag {
    background: #f3e6ff;
    border-color: #8e1eb8;
}

/* Remove Button (shared) */
.preview-remove {
    background: none;
    border: none;
    cursor: pointer;
    padding: 2px;
    opacity: 0.7;
    transition: opacity 0.2s;
    display: inline-flex;
    align-items: center;
    justify-content: center;
}

.preview-remove:hover {
    opacity: 1;
}

/* Control Buttons */
.control-button {
    padding: 0.5rem 1rem;
    border-radius: 0.5rem;
    border: none;
    cursor: pointer;
    font-size: 0.9rem;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    transition: all 0.2s;
}

.control-button.primary {
    background-color: var(--primary-color);
    color: white;
}

.control-button.secondary {
    background-color: #f3f4f6;
    color: var(--text-color);
}

.control-button:hover {
    opacity: 0.9;
    transform: translateY(-1px);
}

/* Selection Mode Adjustments */
.selection-mode .floating-nav {
    bottom: 180px;
}

.selection-mode .filter-status {
    top: 84px; /* Height of selection preview */
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    .floating-nav button,
    .scroll-progress {
        background: rgba(30, 41, 59, 0.9);
        border-color: rgba(255, 255, 255, 0.1);
        color: rgba(255, 255, 255, 0.9);
    }

    .floating-nav button:hover {
        background: rgb(30, 41, 59);
    }
}

/* Responsive Design */
@media (max-width: 768px) {
    .container {
        padding: 0 1rem;
    }

    .filters {
        flex-direction: column;
        align-items: stretch;
    }

    .search-wrapper {
        width: 100%;
    }

    .preview-header {
        flex-direction: column;
        gap: 0.5rem;
    }

    .preview-container {
        flex-direction: column;
    }

    .preview-item,
    .filter-tag {
        max-width: none;
        width: 100%;
    }

    .floating-nav {
        bottom: 140px;
        right: 1rem;
    }

    .floating-nav button,
    .scroll-progress {
        width: 40px;
        height: 40px;
    }
}

/* Print Styles */
@media print {
    .floating-nav,
    .filter-status,
    .selection-preview {
        display: none !important;
    }
}
.selected-only-mode-toggle {
    position: fixed;
    bottom: 6rem;
    right: 2rem;
    background: var(--primary-color);
    color: white;
    padding: 1rem;
    border-radius: 50%;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
    cursor: pointer;
    border: none;
    z-index: 1000;
    width: 60px;
    height: 60px;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: transform 0.2s;
}

.selected-only-mode-toggle:hover {
    transform: scale(1.1);
}

.selected-only-mode-toggle .tooltip {
    position: absolute;
    bottom: 100%;
    right: 0;
    background: rgba(0, 0, 0, 0.8);
    color: white;
    padding: 0.5rem 1rem;
    border-radius: 0.25rem;
    font-size: 0.875rem;
    white-space: nowrap;
    margin-bottom: 0.5rem;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.2s, visibility 0.2s;
}

.selected-only-mode-toggle:hover .tooltip {
    opacity: 1;
    visibility: visible;
}

.selected-only-mode .selection-mode-toggle .tooltip {
    content: "Exit Selection Mode";
}

.selection-mode .selection-checkbox {
    display: block !important;
    pointer-events: auto !important;
}

/* Selection Controls */
.selection-mode-toggle {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    background: var(--primary-color);
    color: white;
    padding: 1rem;
    border-radius: 50%;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
    cursor: pointer;
    border: none;
    z-index: 1000;
    width: 60px;
    height: 60px;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: transform 0.2s;
}

.selection-mode-toggle:hover {
    transform: scale(1.1);
}

.selection-mode-toggle .tooltip {
    position: absolute;
    bottom: 100%;
    right: 0;
    background: rgba(0, 0, 0, 0.8);
    color: white;
    padding: 0.5rem 1rem;
    border-radius: 0.25rem;
    font-size: 0.875rem;
    white-space: nowrap;
    margin-bottom: 0.5rem;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.2s, visibility 0.2s;
}

.selection-mode-toggle:hover .tooltip {
    opacity: 1;
    visibility: visible;
}

.selection-mode .selection-mode-toggle .tooltip {
    content: "Exit Selection Mode";
}

.selection-checkbox {
    display: none;
    position: absolute;
    top: 1rem;
    right: 1rem;
    width: 2rem;
    height: 2rem;
    z-index: 2;
    cursor: pointer;
    opacity: 1;
    appearance: none;
    -webkit-appearance: none;
    background: white;
    border: 2px solid #e5e7eb;
    border-radius: 50%;
    transition: all 0.2s ease;
}

.selection-checkbox:checked {
    background: #10b981;
    border-color: #10b981;
}

.selection-checkbox:checked::before {
    content: '✓';
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    color: white;
    font-size: 1.2rem;
}

.selection-mode .selection-checkbox {
    display: block !important;
    pointer-events: auto !important;
}

/* Paper Cards */
.papers-grid {
    display: grid;
    gap: 2rem;
}

.paper-card {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.75rem;
    padding: 2rem;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
    transition: transform 0.2s ease, box-shadow 0.2s ease;
    display: flex;
    gap: 1.5rem;
    position: relative;
}

.paper-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

.paper-card.selected {
    border: 2px solid var(--primary-color);
    box-shadow: 0 0 0 1px var(--primary-color);
}

.paper-number {
    position: absolute;
    top: -1rem;
    left: -1rem;
    background-color: var(--primary-color);
    color: white;
    width: 2rem;
    height: 2rem;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: bold;
    z-index: 1;
}

.paper-thumbnail {
    flex: 0 0 240px;
    height: 200px;
    border-radius: 0.5rem;
    overflow: hidden;
    border: 1px solid var(--border-color);
    background-color: #f3f4f6;
    position: relative;
    display: flex;
    align-items: center;
    justify-content: center;
}

.paper-thumbnail img {
    width: 100%;
    height: 100%;
    object-fit: cover;
    transition: transform 0.2s;
}

.paper-thumbnail img:hover {
    transform: scale(1.05);
}

.paper-content {
    flex: 1;
    min-width: 0;
}

.paper-title {
    font-size: 1.25rem;
    font-weight: 600;
    margin: 0 0 1rem 0;
    color: var(--text-color);
}

.paper-authors {
    color: #4b5563;
    margin-bottom: 1rem;
}

.paper-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    margin-bottom: 1rem;
}

.paper-tag {
    background: #f3f4f6;
    padding: 0.25rem 0.75rem;
    border-radius: 0.5rem;
    font-size: 0.85rem;
    color: #4b5563;
}

.paper-links {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    margin-top: 1rem;
    align-items: center;
}

.paper-link, .abstract-toggle {
    color: var(--primary-color);
    text-decoration: none;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.5rem 1rem;
    background: #f3f4f6;
    border: none;
    border-radius: 0.5rem;
    transition: all 0.2s;
    font-size: 0.9rem;
    cursor: pointer;
    white-space: nowrap;
}

.paper-link:hover, .abstract-toggle:hover {
    background: #e5e7eb;
    color: var(--hover-color);
}

.paper-abstract {
    margin-top: 1rem;
    display: none;
    background: #f9fafb;
    padding: 1rem;
    border-radius: 0.5rem;
    color: #4b5563;
    line-height: 1.6;
    border: 1px solid var(--border-color);
}

.paper-abstract.show {
    display: block;
}

.paper-row {
    display: none;
}

.paper-row.visible {
    display: block;
}

/* Selection Preview Bar */
.selection-preview {
    position: sticky;
    top: 0;
    left: 0;
    right: 0;
    width: 100%;
    background: white;
    border-bottom: 1px solid var(--border-color);
    border-radius: 0;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    z-index: 1000;
    display: none;
    flex-direction: column;
    margin-bottom: 2rem;
}

.selection-mode .selection-preview {
    display: flex;
}

.preview-header {
    display: flex;
    align-items: center;
    justify-content: space-between;
    padding: 0.75rem 1rem;
    border-bottom: 1px solid var(--border-color);
    background: #f8fafc;
}

.preview-header-left {
    font-weight: 600;
    color: var(--text-color);
    display: flex;
    align-items: center;
    gap: 1rem;
}

.preview-header-right {
    display: flex;
    align-items: center;
    gap: 0.75rem;
}

.selection-counter {
    color: #6b7280;
    font-size: 0.9rem;
}

.preview-container {
    padding: 1rem;
    overflow-x: auto;
    display: flex;
    gap: 1rem;
    flex-wrap: nowrap;
    max-height: 200px;
    scrollbar-width: thin;
}

.preview-container::-webkit-scrollbar {
    height: 8px;
}

.preview-container::-webkit-scrollbar-track {
    background: #f1f1f1;
    border-radius: 4px;
}

.preview-container::-webkit-scrollbar-thumb {
    background: #888;
    border-radius: 4px;
}

.preview-item {
    flex: 0 0 300px;
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    padding: 0.75rem;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    background: white;
    max-width: 300px;
    margin-bottom: 0.5rem;
}

.preview-content {
    flex: 1;
    min-width: 0;
    cursor: pointer;
}

.preview-title {
    font-weight: 600;
    margin-bottom: 0.25rem;
    font-size: 0.9rem;
    color: var(--text-color);
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

.preview-authors {
    font-size: 0.8rem;
    color: #4b5563;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
}

.preview-remove {
    background: none;
    border: none;
    color: #6b7280;
    cursor: pointer;
    width: 24px;
    height: 24px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s ease;
    margin-left: 8px;
    flex-shrink: 0;
}

.preview-remove:hover {
    background-color: #fee2e2;
    color: #dc2626;
}

/* Share Modal */
.share-modal {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.5);
    z-index: 1000;
    justify-content: center;
    align-items: center;
}

.share-modal.show {
    display: flex;
}

.share-modal-content {
    background-color: white;
    padding: 2rem;
    border-radius: 0.75rem;
    max-width: 600px;
    width: 90%;
    position: relative;
}

.share-modal-header {
    margin-bottom: 1.5rem;
}

.share-modal-header h2 {
    margin: 0;
    font-size: 1.5rem;
    color: var(--text-color);
}

.share-url-container {
    display: flex;
    gap: 1rem;
    margin-bottom: 1.5rem;
}

.share-url-input {
    flex: 1;
    padding: 0.75rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    font-size: 0.9rem;
}

.share-modal-close {
    position: absolute;
    top: 1rem;
    right: 1rem;
    background: none;
    border: none;
    font-size: 1.5rem;
    cursor: pointer;
    color: #6b7280;
}

/* Control Buttons */
.control-button {
    padding: 0.5rem 1rem;
    border-radius: 0.5rem;
    border: none;
    cursor: pointer;
    font-size: 0.9rem;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    transition: all 0.2s;
}

.control-button.primary {
    background-color: var(--primary-color);
    color: white;
}

.control-button.secondary {
    background-color: #f3f4f6;
    color: var(--text-color);
}

.control-button:hover {
    opacity: 0.9;
    transform: translateY(-1px);
}

/* Mobile Responsiveness */
@media (max-width: 768px) {
    .preview-item {
        flex: 0 0 250px;
    }
    
    .preview-container {
        max-height: 150px;
    }
    
    .paper-card {
        flex-direction: column;
    }

    .paper-thumbnail {
        width: 100%;
        max-height: 200px;
        flex: none;
        aspect-ratio: 1.2/1;
    }

    .selection-controls {
        flex-direction: column;
        align-items: stretch;
    }

    .paper-links {
        flex-direction: column;
    }
}
@media (max-width: 1024px) {
    .container {
        padding: 0 1rem;
    }

    .selection-preview {
        display: none !important;
    }
}

@media (max-width: 768px) {
    .filters {
        flex-direction: column;
        align-items: stretch;
    }

    .search-wrapper {
        width: 100%;
    }

    .paper-card {
        flex-direction: column;
    }

    .paper-thumbnail {
        width: 100%;
        height: 200px;
        flex: none;
        aspect-ratio: 1.2/1;
    }

    .bitcoin-info {
        flex-direction: column;
    }

    .selection-controls {
        flex-direction: column;
        align-items: stretch;
    }

    .share-modal-content {
        padding: 1rem;
        width: 95%;
    }

    .share-url-container {
        flex-direction: column;
    }

    .paper-links {
        flex-direction: column;
    }
}

@media (max-width: 480px) {
    body {
        padding: 10px;
    }

    h1 {
        font-size: 1.8rem;
    }

    .paper-card {
        padding: 1rem;
    }

    .tag-filters {
        gap: 0.25rem;
    }

    .tag-filter {
        font-size: 0.8rem;
        padding: 0.25rem 0.5rem;
    }
}

        /* Common button styling for donation box */
        .copy-button, .sponsor-button, .website-button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            background: linear-gradient(to bottom, #4CAF50, #45a049);
            color: white;
            border: none;
            border-radius: 4px;
            padding: 8px 16px;
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            text-decoration: none;
            margin-left: 8px;
            transition: all 0.2s;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            min-width: 100px;
            text-align: center;
        }
        
        .copy-button:hover, .sponsor-button:hover, .website-button:hover {
            background: linear-gradient(to bottom, #45a049, #409343);
            box-shadow: 0 3px 6px rgba(0, 0, 0, 0.15);
        }
        
        .copy-button i, .sponsor-button i, .website-button i {
            margin-right: 5px;
        }
        
        /* Button group container */
        .button-group {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 0;
            justify-content: flex-start;
        }
        
        .button-group .copy-button,
        .button-group .sponsor-button,
        .button-group .website-button {
            margin-left: 0;
        }

        /* Bitcoin info styling */
        .bitcoin-info {
            display: flex;
            flex-direction: column;
            gap: 15px;
            margin: 15px 0;
        }

        .bitcoin-address-container {
            display: flex;
            align-items: center;
            background: #f5f5f5;
            border-radius: 4px;
            padding: 8px 16px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            border: 1px solid #e0e0e0;
        }

        .bitcoin-label {
            font-weight: 600;
            margin-right: 10px;
            white-space: nowrap;
        }

        .address-field {
            flex: 1;
            overflow: hidden;
        }

        .bitcoin-address {
            font-family: monospace;
            background: transparent;
            border: none;
            padding: 0;
            font-size: 14px;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
            display: block;
            width: 100%;
        }

        /* Remove any individual button styling that might be in the original styles */
        /* Website button styling - keeping this for reference but it will be overridden by the common styling above */
        .website-button {
            /* All styling now handled by the common button class above */
        }
    </style>

    <!-- Lazy load -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/17.8.3/lazyload.min.js"></script>
</head>
<body>
    <div class="container">
        <!-- Floating Navigation -->
        <div class="floating-nav">
            <button onclick="scrollToTop()" aria-label="Scroll to top">
                <i class="fas fa-arrow-up"></i>
            </button>
            <button onclick="scrollToBottom()" aria-label="Scroll to bottom">
                <i class="fas fa-arrow-down"></i>
            </button>
            <div class="scroll-progress">0%</div>
        </div>

        <!-- Selection Mode Toggle Button -->
        <div class="selection-buttons">
            <!-- Show Selected Only -->
            <button class="selected-only-mode-toggle" onclick="toggleSelectedOnly()">
                <i class="fas fa-filter"></i>
                <span class="tooltip">Show Selected Only</span>
            </button>

            <!-- Enter Selection Mode -->
            <button class="selection-mode-toggle" onclick="toggleSelectionMode()">
                <i class="fas fa-list-check"></i>
                <span class="tooltip">Enter Selection Mode</span>
            </button>
        </div>

        <!-- Selection Preview Bar -->
        <div class="selection-preview">
            <div class="preview-header">
                <div class="preview-header-left">
                    <span>Selected Papers</span>
                    <span class="selection-counter">0 papers selected</span>
                </div>
                <div class="preview-header-right">
                    <button class="control-button secondary" onclick="clearSelection()">
                        <i class="fas fa-trash"></i> Clear
                    </button>
                    <button class="control-button primary" onclick="showShareModal()">
                        <i class="fas fa-share"></i> Share
                    </button>
                </div>
            </div>
            <div class="preview-container" id="selectionPreview"></div>
        </div>

        <!-- Filter Status Bar -->
        <div class="filter-status">
            <div class="preview-header">
                <div class="preview-header-left">
                    <span>Active Filters</span>
                    <span class="filter-counter">Showing <span id="visibleCount">0</span> of <span id="totalCount">0</span> papers</span>
                </div>
                <div class="preview-header-right">
                    <button class="control-button secondary" onclick="clearAllFilters()">
                        <i class="fas fa-trash"></i> Clear All
                    </button>
                </div>
            </div>
            <div class="preview-container" id="activeFilters">
                <!-- Active filters will be inserted here -->
            </div>
        </div>

        <h1>MrNeRF's Awesome-3D-Gaussian-Splatting-Paper-List</h1>

        <!-- Donation Box -->
        <div class="donate-box">
            <h3>Support This Project</h3>
            <p>If you find this resource helpful, consider supporting its development and maintenance.</p>
            <div class="bitcoin-info">
                <div class="bitcoin-address-container">
                    <span class="bitcoin-label">Bitcoin:</span>
                    <div class="address-field">
                        <code class="bitcoin-address">bc1qz7z4c2cn46t7rkgsh7mr8tw9ssgctepzxrtqfw</code>
                    </div>
                </div>
                <div class="button-group">
                    <button class="copy-button" onclick="copyBitcoinAddress()">
                        <i class="fas fa-copy"></i> Copy
                    </button>
                    <a href="https://github.com/sponsors/MrNeRF" class="sponsor-button" target="_blank" rel="noopener">
                        <i class="fas fa-heart"></i> Sponsor
                    </a>
                    <a href="https://www.mrnerf.com" class="website-button" target="_blank" rel="noopener">
                        <i class="fas fa-globe"></i> MrNeRF.com
                    </a>
                </div>
                <!-- Discord Badge -->
                <div class="discord-badge">
                    <a href="https://discord.gg/TbxJST2BbC">
                        <img src="https://dcbadge.limes.pink/api/server/https://discord.gg/TbxJST2BbC" alt="Discord Badge" />
                    </a>
                </div>
            </div>
        </div>


        <!-- Filter Info -->
        <div class="filter-info">
            <h3>Filter Options</h3>
            <p><strong>Search:</strong> Enter paper title or author names, then click <i class="fas fa-times"></i> to clear.</p>
            <p><strong>Year:</strong> Filter by publication year</p>
            <p><strong>Tags:</strong> Click once to include (blue), twice to exclude (red), third time to remove filter</p>
            <p><strong>Selection:</strong> Use selection mode to pick and share specific papers</p>
        </div>

        <!-- Share Modal -->
        <div class="share-modal" id="shareModal">
            <div class="share-modal-content">
                <button class="share-modal-close" onclick="hideShareModal()">&times;</button>
                <div class="share-modal-header">
                    <h2>Share Selected Papers</h2>
                </div>
                <div class="share-url-container">
                    <input type="text" class="share-url-input" id="shareUrl" readonly>
                    <button class="control-button primary" onclick="copyShareLink()">
                        <i class="fas fa-copy"></i> Copy Link
                    </button>
                </div>
            </div>
        </div>

        <!-- Filters Bar -->
        <div class="filters">
            <div class="search-wrapper">
                <input type="text" id="searchInput" class="search-box" placeholder="Search papers by title or authors...">
                <button class="clear-search-btn" onclick="clearSearch()" title="Clear search">
                    <i class="fas fa-times"></i>
                </button>
            </div>
            <select id="yearFilter" class="filter-select">
                <option value="all">All Years</option>
                <option value="2025">2025</option>
<option value="2024">2024</option>
<option value="2023">2023</option>
            </select>
        </div>

        <!-- Tag Filters -->
        <div class="tag-filters" id="tagFilters">
            <div class="tag-filter" data-tag="2DGS">2DGS</div>
<div class="tag-filter" data-tag="360 degree">360 degree</div>
<div class="tag-filter" data-tag="3ster-based">3ster-based</div>
<div class="tag-filter" data-tag="Acceleration">Acceleration</div>
<div class="tag-filter" data-tag="Antialiasing">Antialiasing</div>
<div class="tag-filter" data-tag="Autonomous Driving">Autonomous Driving</div>
<div class="tag-filter" data-tag="Avatar">Avatar</div>
<div class="tag-filter" data-tag="Classic Work">Classic Work</div>
<div class="tag-filter" data-tag="Code">Code</div>
<div class="tag-filter" data-tag="Compression">Compression</div>
<div class="tag-filter" data-tag="Deblurring">Deblurring</div>
<div class="tag-filter" data-tag="Densification">Densification</div>
<div class="tag-filter" data-tag="Diffusion">Diffusion</div>
<div class="tag-filter" data-tag="Distributed">Distributed</div>
<div class="tag-filter" data-tag="Dynamic">Dynamic</div>
<div class="tag-filter" data-tag="Editing">Editing</div>
<div class="tag-filter" data-tag="Feed-Forward">Feed-Forward</div>
<div class="tag-filter" data-tag="GAN">GAN</div>
<div class="tag-filter" data-tag="Gaussian Video">Gaussian Video</div>
<div class="tag-filter" data-tag="In the Wild">In the Wild</div>
<div class="tag-filter" data-tag="Inpainting">Inpainting</div>
<div class="tag-filter" data-tag="Language Embedding">Language Embedding</div>
<div class="tag-filter" data-tag="Large-Scale">Large-Scale</div>
<div class="tag-filter" data-tag="Lidar">Lidar</div>
<div class="tag-filter" data-tag="LoD">LoD</div>
<div class="tag-filter" data-tag="Medicine">Medicine</div>
<div class="tag-filter" data-tag="Meshing">Meshing</div>
<div class="tag-filter" data-tag="Misc">Misc</div>
<div class="tag-filter" data-tag="Monocular">Monocular</div>
<div class="tag-filter" data-tag="Object Detection">Object Detection</div>
<div class="tag-filter" data-tag="Optimization">Optimization</div>
<div class="tag-filter" data-tag="Perspective-correct">Perspective-correct</div>
<div class="tag-filter" data-tag="Physics">Physics</div>
<div class="tag-filter" data-tag="Point Cloud">Point Cloud</div>
<div class="tag-filter" data-tag="Poses">Poses</div>
<div class="tag-filter" data-tag="Project">Project</div>
<div class="tag-filter" data-tag="Ray Tracing">Ray Tracing</div>
<div class="tag-filter" data-tag="Relight">Relight</div>
<div class="tag-filter" data-tag="Rendering">Rendering</div>
<div class="tag-filter" data-tag="Review">Review</div>
<div class="tag-filter" data-tag="Robotics">Robotics</div>
<div class="tag-filter" data-tag="SLAM">SLAM</div>
<div class="tag-filter" data-tag="Segmentation">Segmentation</div>
<div class="tag-filter" data-tag="Sparse">Sparse</div>
<div class="tag-filter" data-tag="Stereo">Stereo</div>
<div class="tag-filter" data-tag="Style Transfer">Style Transfer</div>
<div class="tag-filter" data-tag="Super Resolution">Super Resolution</div>
<div class="tag-filter" data-tag="Texturing">Texturing</div>
<div class="tag-filter" data-tag="Transformer">Transformer</div>
<div class="tag-filter" data-tag="Uncertainty">Uncertainty</div>
<div class="tag-filter" data-tag="Video">Video</div>
<div class="tag-filter" data-tag="Virtual Reality">Virtual Reality</div>
<div class="tag-filter" data-tag="World Generation">World Generation</div>
        </div>

        <!-- Paper Cards -->
        <div class="papers-grid">
            <div class="paper-row" data-id="chang2025meshsplat" data-title="MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting" data-authors="Hanzhi Chang, Ruijie Zhu, Wenjie Chang, Mulin Yu, Yanzhe Liang, Jiahao Lu, Zhuoyuan Li, Tianzhu Zhang" data-year="2025" data-tags='["2DGS", "Feed-Forward", "Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chang2025meshsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chang2025meshsplat.jpg" data-fallback="None" alt="Paper thumbnail for MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Hanzhi Chang, Ruijie Zhu, Wenjie Chang, Mulin Yu, Yanzhe Liang, Jiahao Lu, Zhuoyuan Li, Tianzhu Zhang</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2508.17811.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hanzhichang.github.io/meshsplat_web/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://hanzhichang.github.io/meshsplat_web/static/images/meshsplat/demo.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2025objectgs" data-title="ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting" data-authors="Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, Bo Dai" data-year="2025" data-tags='["Code", "Project", "Segmentation", "Language Embedding"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2025objectgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2025objectgs.jpg" data-fallback="None" alt="Paper thumbnail for ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Language Embedding</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2507.15454.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ruijiezhu94.github.io/ObjectGS_page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/RuijieZhu94/ObjectGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="howil2025clipgaussian" data-title="CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting" data-authors="Kornel Howil, Joanna Waczyńska, Piotr Borycki, Tadeusz Dziarmaga, Marcin Mazur, Przemysław Spurek" data-year="2025" data-tags='["Code", "Project", "Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'howil2025clipgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/howil2025clipgaussian.jpg" data-fallback="None" alt="Paper thumbnail for CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Kornel Howil, Joanna Waczyńska, Piotr Borycki, Tadeusz Dziarmaga, Marcin Mazur, Przemysław Spurek</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2505.22854.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kornelhowil.github.io/CLIPGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href=" https://github.com/kornelhowil/CLIPGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussians, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussians approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving a model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussians as a universal and efficient solution for multimodal style transfer.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2025abcgs" data-title="ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting" data-authors="Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li" data-year="2025" data-tags='["Code", "Project", "Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2025abcgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2025abcgs.jpg" data-fallback="None" alt="Paper thumbnail for ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2503.22218.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vpx-ecnu.github.io/ABC-GS-website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/vpx-ecnu/ABC-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2025stdloc" data-title="From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting" data-authors="Zhiwei Huang, Hailin Yu, Yichun Shentu, Jin Yuan, Guofeng Zhang" data-year="2025" data-tags='["Code", "Poses", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2025stdloc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2025from.jpg" data-fallback="None" alt="Paper thumbnail for From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Zhiwei Huang, Hailin Yu, Yichun Shentu, Jin Yuan, Guofeng Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2503.19358.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zju3dv.github.io/STDLoc/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zju3dv/STDLoc" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents a novel camera relocalization method, STDLoc, which leverages Feature Gaussian as scene representation. STDLoc is a full relocalization pipeline that can achieve accurate relocalization without relying on any pose prior. Unlike previous coarse-to-fine localization methods that require image retrieval first and then feature matching, we propose a novel sparse-to-dense localization paradigm. Based on this scene representation, we introduce a novel matching-oriented Gaussian sampling strategy and a scene-specific detector to achieve efficient and robust initial pose estimation. Furthermore, based on the initial localization results, we align the query feature map to the Gaussian feature field by dense feature matching to enable accurate localization. The experiments on indoor and outdoor datasets show that STDLoc outperforms current state-of-the-art localization methods in terms of localization accuracy and recall.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2025motion" data-title="Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction" data-authors="Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias" data-year="2025" data-tags='["Code", "Dynamic", "Editing", "Project", "Robotics", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2025motion', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2025motion.jpg" data-fallback="None" alt="Paper thumbnail for Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2503.09040.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mlzxy.github.io/motion-blender-gs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/mlzxy/motion-blender-gs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2025persistent" data-title="Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects" data-authors="Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg" data-year="2025" data-tags='["Code", "Language Embedding", "Project", "Robotics", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2025persistent', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2025persistent.jpg" data-fallback="None" alt="Paper thumbnail for Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2503.05189.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://berkeleyautomation.github.io/POGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/uynitsuj/pogs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Tracking and manipulating irregularly-shaped, previously unseen objects in dynamic environments is important for robotic applications in manufacturing, assembly, and logistics. Recently introduced Gaussian Splats efficiently model object geometry, but lack persistent state estimation for task-oriented manipulation. We present Persistent Object Gaussian Splat (POGS), a system that embeds semantics, self-supervised visual features, and object grouping features into a compact representation that can be continuously updated to estimate the pose of scanned objects. POGS updates object states without requiring expensive rescanning or prior CAD models of objects. After an initial multi-view scene capture and training phase, POGS uses a single stereo camera to integrate depth estimates along with self-supervised vision encoder features for object pose estimation. POGS supports grasping, reorientation, and natural language-driven manipulation by refining object pose estimates, facilitating sequential object reset operations with human-induced object perturbations and tool servoing, where robots recover tool pose despite tool perturbations of up to 30{\deg}. POGS achieves up to 12 consecutive successful object resets and recovers from 80% of in-grasp tool perturbations.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chacko2025lifting" data-title="Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation" data-authors="Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee" data-year="2025" data-tags='["Language Embedding", "Segmentation", "Virtual Reality"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chacko2025lifting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chacko2025lifting.jpg" data-fallback="None" alt="Paper thumbnail for Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee</p>
      <div class="paper-tags"><span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Virtual Reality</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2502.00173.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Lifting By Gaussians (LBG), a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently, 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM, etc.), together with features from CLIP and DINOv2, directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches, LBG requires no per-scene training, allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular, enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore, our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lin2025omniphysgs" data-title="OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation" data-authors="Yuchen Lin, Chenguo Lin, Jianjin Xu, Yadong Mu" data-year="2025" data-tags='["Code", "Dynamic", "Physics", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lin2025omniphysgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lin2025omniphysgs.jpg" data-fallback="None" alt="Paper thumbnail for OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yuchen Lin, Chenguo Lin, Jianjin Xu, Yadong Mu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.18982.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://wgsxm.github.io/projects/omniphysgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/wgsxm/omniphysgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://wgsxm.github.io/videos/omniphysgs.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, significant advancements have been made in the reconstruction and generation of 3D assets, including static cases and those with physical interactions. To recover the physical properties of 3D assets, existing methods typically assume that all materials belong to a specific predefined category (e.g., elasticity). However, such assumptions ignore the complex composition of multiple heterogeneous objects in real scenarios and tend to render less physically plausible animation given a wider range of objects. We propose OmniPhysGS for synthesizing a physics-based 3D dynamic scene composed of more general objects. A key design of OmniPhysGS is treating each 3D asset as a collection of constitutive 3D Gaussians. For each Gaussian, its physical material is represented by an ensemble of 12 physical domain-expert sub-models (rubber, metal, honey, water, etc.), which greatly enhances the flexibility of the proposed model. In the implementation, we define a scene by user-specified prompts and supervise the estimation of material weighting factors via a pretrained video diffusion model. Comprehensive experiments demonstrate that OmniPhysGS achieves more general and realistic physical dynamics across a broader spectrum of materials, including elastic, viscoelastic, plastic, and fluid substances, as well as interactions between different materials. Our method surpasses existing methods by approximately 3% to 16% in metrics of visual quality and text alignment.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guizilini2025zeroshot" data-title="Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion" data-authors="Vitor Guizilini, Muhammad Zubair Irshad, Dian Chen, Greg Shakhnarovich, Rares Ambrus" data-year="2025" data-tags='["360 degree", "Diffusion", "Feed-Forward", "Large-Scale", "Point Cloud", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guizilini2025zeroshot', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guizilini2025zeroshot.jpg" data-fallback="None" alt="Paper thumbnail for Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Vitor Guizilini, Muhammad Zubair Irshad, Dian Chen, Greg Shakhnarovich, Rares Ambrus</p>
      <div class="paper-tags"><span class="paper-tag">360 degree</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Point Cloud</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.18804.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mvgd.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lin2025diffsplat" data-title="DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation" data-authors="Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu" data-year="2025" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lin2025diffsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lin2025diffsplat.jpg" data-fallback="None" alt="Paper thumbnail for DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.16764.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://chenguolin.github.io/projects/DiffSplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/chenguolin/DiffSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffSplat in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="armagan2025trickgs" data-title="Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting" data-authors="Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Mateusz Nowak, Mehmet Kerim Yucel" data-year="2025" data-tags='["Acceleration"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'armagan2025trickgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/armagan2025trickgs.jpg" data-fallback="None" alt="Paper thumbnail for Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Mateusz Nowak, Mehmet Kerim Yucel</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.14534.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian splatting (GS) for 3D reconstruction has become quite popular due to their fast training, inference speeds and high quality reconstruction. However, GS-based reconstructions generally consist of millions of Gaussians, which makes them hard to use on computationally constrained devices such as smartphones. In this paper, we first propose a principled analysis of advances in efficient GS methods. Then, we propose Trick-GS, which is a careful combination of several strategies including (1) progressive training with resolution, noise and Gaussian scales, (2) learning to prune and mask primitives and SH bands by their significance, and (3) accelerated GS training framework. Trick-GS takes a large step towards resource-constrained GS, where faster run-time, smaller and faster-convergence of models is of paramount concern. Our results on three datasets show that Trick-GS achieves up to 2x faster training, 40x smaller disk size and 2x faster rendering speed compared to vanilla GS, while having comparable accuracy.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2025densesfm" data-title="Dense-SfM: Structure from Motion with Dense Consistent Matching" data-authors="JongMin Lee, Sungjoo Yoo" data-year="2025" data-tags='["Point Cloud", "Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2025densesfm', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2025densesfm.jpg" data-fallback="None" alt="Paper thumbnail for Dense-SfM: Structure from Motion with Dense Consistent Matching" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Dense-SfM: Structure from Motion with Dense Consistent Matching <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">JongMin Lee, Sungjoo Yoo</p>
      <div class="paper-tags"><span class="paper-tag">Point Cloud</span>
<span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.14277.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2025micromacro" data-title="Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images" data-authors="Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang" data-year="2025" data-tags='["In the Wild"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2025micromacro', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2025micromacro.jpg" data-fallback="None" alt="Paper thumbnail for Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang</p>
      <div class="paper-tags"><span class="paper-tag">In the Wild</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.14231.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations: Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; and Wavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2025hammer" data-title="HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting" data-authors="Javier Yu, Timothy Chen, Mac Schwager" data-year="2025" data-tags='["Project", "Robotics", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2025hammer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2025hammer.jpg" data-fallback="None" alt="Paper thumbnail for HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Javier Yu, Timothy Chen, Mac Schwager</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.14147.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hammer-project.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting offers expressive scene reconstruction, modeling a broad range of visual, geometric, and semantic information. However, efficient real-time map reconstruction with data streamed from multiple robots and devices remains a challenge. To that end, we propose HAMMER, a server-based collaborative Gaussian Splatting method that leverages widely available ROS communication infrastructure to generate 3D, metric-semantic maps from asynchronous robot data-streams with no prior knowledge of initial robot positions and varying on-device pose estimators. HAMMER consists of (i) a frame alignment module that transforms local SLAM poses and image data into a global frame and requires no prior relative pose knowledge, and (ii) an online module for training semantic 3DGS maps from streaming data. HAMMER handles mixed perception modes, adjusts automatically for variations in image pre-processing among different devices, and distills CLIP semantic codes into the 3D scene for open-vocabulary language queries. In our real-world experiments, HAMMER creates higher-fidelity maps (2x) compared to competing baselines and is useful for downstream tasks, such as semantic goal-conditioned navigation (e.g., ``go to the couch"). Accompanying content available at hammer-project.github.io.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2025fast3r" data-title="Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass" data-authors="Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli" data-year="2025" data-tags='["3ster-based", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2025fast3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2025fast3r.jpg" data-fallback="None" alt="Paper thumbnail for Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.13928.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fast3r-3d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Multi-view 3D reconstruction remains a core challenge in computer vision, particularly in applications requiring accurate and scalable representations across diverse perspectives. Current leading methods such as DUSt3R employ a fundamentally pairwise approach, processing images in pairs and necessitating costly global alignment procedures to reconstruct from multiple views. In this work, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view generalization to DUSt3R that achieves efficient and scalable 3D reconstruction by processing many views in parallel. Fast3R's Transformer-based architecture forwards N images in a single forward pass, bypassing the need for iterative alignment. Through extensive experiments on camera pose estimation and 3D reconstruction, Fast3R demonstrates state-of-the-art performance, with significant improvements in inference speed and reduced error accumulation. These results establish Fast3R as a robust alternative for multi-view applications, offering enhanced scalability without compromising reconstruction accuracy.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sario2025gode" data-title="GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression" data-authors="Francesco Di Sario, Riccardo Renzulli, Marco Grangetto, Akihiro Sugimoto, Enzo Tartaglione" data-year="2025" data-tags='["Compression", "LoD"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sario2025gode', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sario2025gode.jpg" data-fallback="None" alt="Paper thumbnail for GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Francesco Di Sario, Riccardo Renzulli, Marco Grangetto, Akihiro Sugimoto, Enzo Tartaglione</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">LoD</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.13558.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting enhances real-time performance in novel view synthesis by representing scenes with mixtures of Gaussians and utilizing differentiable rasterization. However, it typically requires large storage capacity and high VRAM, demanding the design of effective pruning and compression techniques. Existing methods, while effective in some scenarios, struggle with scalability and fail to adapt models based on critical factors such as computing capabilities or bandwidth, requiring to re-train the model under different configurations. In this work, we propose a novel, model-agnostic technique that organizes Gaussians into several hierarchical layers, enabling progressive Level of Detail (LoD) strategy. This method, combined with recent approach of compression of 3DGS, allows a single model to instantly scale across several compression ratios, with minimal to none impact to quality compared to a single non-scalable model and without requiring re-training. We validate our approach on typical datasets and benchmarks, showcasing low distortion and substantial gains in terms of scalability and adaptability.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lan20253dgs2" data-title="3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting" data-authors="Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang" data-year="2025" data-tags='["Optimization"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lan20253dgs2', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lan20253dgs2.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang</p>
      <div class="paper-tags"><span class="paper-tag">Optimization</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.13975.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over $10\times$ fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shi2025sketch" data-title="Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes" data-authors="Yuang Shi, Simone Gasparini, Géraldine Morin, Chenggang Yang, Wei Tsang Ooi" data-year="2025" data-tags='["Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shi2025sketch', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shi2025sketch.jpg" data-fallback="None" alt="Paper thumbnail for Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yuang Shi, Simone Gasparini, Géraldine Morin, Chenggang Yang, Wei Tsang Ooi</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.13045.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a promising representation for photorealistic rendering of 3D scenes. However, its high storage requirements pose significant challenges for practical applications. We observe that Gaussians exhibit distinct roles and characteristics that are analogous to traditional artistic techniques -- Like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features like edges and contours; While other Gaussians represent broader, smoother regions, that are analogous to broader brush strokes that add volume and depth to a painting. Based on this observation, we propose a novel hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians, which cover smooth regions. Sketch Gaussians are efficiently encoded using parametric models, leveraging their geometric coherence, while Patch Gaussians undergo optimized pruning, retraining, and vector quantization to maintain volumetric consistency and storage efficiency. Our comprehensive evaluation across diverse indoor and outdoor scenes demonstrates that this structure-aware approach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41% in LPIPS at equivalent model sizes, and correspondingly, for an indoor scene, our model maintains the visual quality with 2.3% of the original model size.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="arunan2025darbsplatting" data-title="DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions" data-authors="Vishagar Arunan, Saeedha Nazar, Hashiru Pramuditha, Vinasirajan Viruthshaan, Sameera Ramasinghe, Simon Lucey, Ranga Rodrigo" data-year="2025" data-tags='["Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'arunan2025darbsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/arunan2025darbsplatting.jpg" data-fallback="None" alt="Paper thumbnail for DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Vishagar Arunan, Saeedha Nazar, Hashiru Pramuditha, Vinasirajan Viruthshaan, Sameera Ramasinghe, Simon Lucey, Ranga Rodrigo</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.12369.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://randomnerds.github.io/darbs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Splatting-based 3D reconstruction methods have gained popularity with the advent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel views. These methods commonly resort to using exponential family functions, such as the Gaussian function, as reconstruction kernels due to their anisotropic nature, ease of projection, and differentiability in rasterization. However, the field remains restricted to variations within the exponential family, leaving generalized reconstruction kernels largely underexplored, partly due to the lack of easy integrability in 3D to 2D projections. In this light, we show that a class of decaying anisotropic radial basis functions (DARBFs), which are non-negative functions of the Mahalanobis distance, supports splatting by approximating the Gaussian function's closed-form integration advantage. With this fresh perspective, we demonstrate up to 34% faster convergence during training and a 15% reduction in memory consumption across various DARB reconstruction kernels, while maintaining comparable PSNR, SSIM, and LPIPS results. We will make the code available.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2025hac" data-title="HAC++: Towards 100X Compression of 3D Gaussian Splatting" data-authors="Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai" data-year="2025" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2025hac', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2025hac.jpg" data-fallback="None" alt="Paper thumbnail for HAC++: Towards 100X Compression of 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HAC++: Towards 100X Compression of 3D Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.12255.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yihangchen-ee.github.io/project_hac++/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/YihangChen-ee/HAC-plus" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To achieve a compact size, we propose HAC++, which leverages the relationships between unorganized anchors and a structured hash grid, utilizing their mutual information for context modeling. Additionally, HAC++ captures intra-anchor contextual relationships to further enhance compression performance. To facilitate entropy coding, we utilize Gaussian distributions to precisely estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Moreover, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Overall, HAC++ achieves a remarkable size reduction of over 100X compared to vanilla 3DGS when averaged on all datasets, while simultaneously improving fidelity. It also delivers more than 20X size reduction compared to Scaffold-GS. Our code is available at https://github.com/YihangChen-ee/HAC-plus.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2025cargs" data-title="Car-GS: Addressing Reflective and Transparent Surface Challenges in 3D Car Reconstruction" data-authors="Congcong Li, Jin Wang, Xiaomeng Wang, Xingchen Zhou, Wei Wu, Yuzhi Zhang, Tongyi Cao" data-year="2025" data-tags='["Code", "Meshing", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2025cargs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2025cargs.jpg" data-fallback="None" alt="Paper thumbnail for Car-GS: Addressing Reflective and Transparent Surface Challenges in 3D Car Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Car-GS: Addressing Reflective and Transparent Surface Challenges in 3D Car Reconstruction <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Congcong Li, Jin Wang, Xiaomeng Wang, Xingchen Zhou, Wei Wu, Yuzhi Zhang, Tongyi Cao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.11020.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lcc815.github.io/Car-GS/" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D car modeling is crucial for applications in autonomous driving systems, virtual and augmented reality, and gaming. However, due to the distinctive properties of cars, such as highly reflective and transparent surface materials, existing methods often struggle to achieve accurate 3D car reconstruction.To address these limitations, we propose Car-GS, a novel approach designed to mitigate the effects of specular highlights and the coupling of RGB and geometry in 3D geometric and shading reconstruction (3DGS). Our method incorporates three key innovations: First, we introduce view-dependent Gaussian primitives to effectively model surface reflections. Second, we identify the limitations of using a shared opacity parameter for both image rendering and geometric attributes when modeling transparent objects. To overcome this, we assign a learnable geometry-specific opacity to each 2D Gaussian primitive, dedicated solely to rendering depth and normals. Third, we observe that reconstruction errors are most prominent when the camera view is nearly orthogonal to glass surfaces. To address this issue, we develop a quality-aware supervision module that adaptively leverages normal priors from a pre-trained large-scale normal model.Experimental results demonstrate that Car-GS achieves precise reconstruction of car surfaces and significantly outperforms prior methods. The project page is available at https://lcc815.github.io/Car-GS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zheng2025gstar" data-title="GSTAR: Gaussian Surface Tracking and Reconstruction" data-authors="Chengwei Zheng, Lixin Xue, Juan Zarate, Jie Song" data-year="2025" data-tags='["Avatar", "Dynamic", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zheng2025gstar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zheng2025gstar.jpg" data-fallback="None" alt="Paper thumbnail for GSTAR: Gaussian Surface Tracking and Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSTAR: Gaussian Surface Tracking and Reconstruction <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Chengwei Zheng, Lixin Xue, Juan Zarate, Jie Song</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.10283.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="chengwei-zheng.github.io/GSTAR/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=Fwby4PrjFeM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GSTAR maintains the mesh topology and tracks the meshes using Gaussians. In regions where topology changes, GSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and the generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://eth-ait.github.io/GSTAR/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ma2025cityloc" data-title="CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation" data-authors="Qi Ma, Runyi Yang, Bin Ren, Ender Konukoglu, Luc Van Gool, Danda Pani Paudel" data-year="2025" data-tags='["Language Embedding", "Large-Scale"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ma2025cityloc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ma2025cityloc.jpg" data-fallback="None" alt="Paper thumbnail for CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Qi Ma, Runyi Yang, Bin Ren, Ender Konukoglu, Luc Van Gool, Danda Pani Paudel</p>
      <div class="paper-tags"><span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Large-Scale</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.08982.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city.   To facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description. To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations. The conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning.   We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets. Our source code and dataset will be made publicly available.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hong2025gslivo" data-title="GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping" data-authors="Sheng Hong, Chunran Zheng, Yishu Shen, Changze Li, Fu Zhang, Tong Qin, Shaojie Shen" data-year="2025" data-tags='["Large-Scale", "Lidar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hong2025gslivo', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hong2025gslivo.jpg" data-fallback="None" alt="Paper thumbnail for GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Sheng Hong, Chunran Zheng, Yishu Shen, Changze Li, Fu Zhang, Tong Qin, Shaojie Shen</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Lidar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.08672.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach. However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption. LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU. Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system. Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry. The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales. The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients. Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window. Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map. Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform. The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities. All implementation algorithms, hardware designs, and CAD models will be publicly available.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2025vingsmono" data-title="VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes" data-authors="Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding" data-year="2025" data-tags='["Large-Scale", "Meshing", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2025vingsmono', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2025vingsmono.jpg" data-fallback="None" alt="Paper thumbnail for VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.08286.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer, Score Manager, and Pose Refinement, which collectively improve mapping speed and localization accuracy. This enables the SLAM system to handle large-scale urban environments with up to 50 million Gaussian ellipsoids. To ensure global consistency in large-scale scenes, we design a Loop Closure module, which innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian Splatting for loop closure detection and correction of the Gaussian map. Additionally, we propose a Dynamic Eraser to address the inevitable presence of dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor and outdoor environments demonstrate that our approach achieves localization performance on par with Visual-Inertial Odometry while surpassing recent GS/NeRF SLAM methods. It also significantly outperforms all existing methods in terms of mapping and rendering quality. Furthermore, we developed a mobile app and verified that our framework can generate high-quality Gaussian maps in real time using only a smartphone camera and a low-frequency IMU sensor. To the best of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method capable of operating in outdoor environments and supporting kilometer-scale large scenes.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="rogge2025objectcentric" data-title="Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models" data-authors="Marcel Rogge, Didier Stricker" data-year="2025" data-tags='["Compression", "Densification", "Editing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'rogge2025objectcentric', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/rogge2025objectcentric.jpg" data-fallback="None" alt="Paper thumbnail for Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Marcel Rogge, Didier Stricker</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Editing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.08174.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Current Gaussian Splatting approaches are effective for reconstructing entire scenes but lack the option to target specific objects, making them computationally expensive and unsuitable for object-specific applications. We propose a novel approach that leverages object masks to enable targeted reconstruction, resulting in object-centric models. Additionally, we introduce an occlusion-aware pruning strategy to minimize the number of Gaussians without compromising quality. Our method reconstructs compact object models, yielding object-centric Gaussian and mesh representations that are up to 96\% smaller and up to 71\% faster to train compared to the baseline while retaining competitive quality. These representations are immediately usable for downstream applications such as appearance editing and physics simulation without additional processing.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2025uncommon" data-title="UnCommon Objects in 3D" data-authors="Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny" data-year="2025" data-tags='["Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2025uncommon', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2025uncommon.jpg" data-fallback="None" alt="Paper thumbnail for UnCommon Objects in 3D" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">UnCommon Objects in 3D <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.07574.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://uco3d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/facebookresearch/uco3d" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI. uCO3D is the largest publicly-available collection of high-resolution videos of objects with 3D annotations that ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of higher quality, due to extensive quality checks of both the collected videos and the 3D annotations. Similar to analogous datasets, uCO3D contains annotations for 3D camera poses, depth maps and sparse point clouds. In addition, each object is equipped with a caption and a 3D Gaussian Splat reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D and obtain superior results using the latter, showing that uCO3D is better for learning applications.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="stuart20253dgstopc" data-title="3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh" data-authors="Lewis A G Stuart, Michael P Pound" data-year="2025" data-tags='["Code", "Point Cloud"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'stuart20253dgstopc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/stuart20253dgstopc.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Lewis A G Stuart, Michael P Pound</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Point Cloud</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.07478.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Lewis-Stuart-11/3DGS-to-PC" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D reconstructions, but these scenes often require specialised renderers for effective visualisation. In contrast, point clouds are a widely used 3D representation and are compatible with most popular 3D processing software, yet converting 3DGS scenes into point clouds is a complex challenge. In this work we introduce 3DGS-to-PC, a flexible and highly customisable framework that is capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We sample points probabilistically from each Gaussian as a 3D density function. We additionally threshold new points using the Mahalanobis distance to the Gaussian centre, preventing extreme outliers. The result is a point cloud that closely represents the shape encoded into the 3D Gaussian scene. Individual Gaussians use spherical harmonics to adapt colours depending on view, and each point may contribute only subtle colour hints to the resulting rendered scene. To avoid spurious or incorrect colours that do not fit with the final point cloud, we recalculate Gaussian colours via a customised image rendering approach, assigning each Gaussian the colour of the pixel to which it contributes most across all views. 3DGS-to-PC also supports mesh generation through Poisson Surface Reconstruction, applied to points sampled from predicted surface Gaussians. This allows coloured meshes to be generated from 3DGS scenes without the need for re-training. This package is highly customisable and capability of simple integration into existing 3DGS pipelines. 3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud and surface-based formats.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2025evaluating" data-title="Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes" data-authors="Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang" data-year="2025" data-tags='["Dynamic"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2025evaluating', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2025evaluating.jpg" data-fallback="None" alt="Paper thumbnail for Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.08072.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two groundbreaking technologies that have revolutionized the field of Novel View Synthesis (NVS), enabling immersive photorealistic rendering and user experiences by synthesizing multiple viewpoints from a set of images of sparse views. The potential applications of NVS, such as high-quality virtual and augmented reality, detailed 3D modeling, and realistic medical organ imaging, underscore the importance of quality assessment of NVS methods from the perspective of human perception. Although some previous studies have explored subjective quality assessments for NVS technology, they still face several challenges, especially in NVS methods selection, scenario coverage, and evaluation methodology. To address these challenges, we conducted two subjective experiments for the quality assessment of NVS technologies containing both GS-based and NeRF-based methods, focusing on dynamic and real-world scenes. This study covers 360{\deg}, front-facing, and single-viewpoint videos while providing a richer and greater number of real scenes. Meanwhile, it's the first time to explore the impact of NVS methods in dynamic scenes with moving objects. The two types of subjective experiments help to fully comprehend the influences of different viewing paths from a human perception perspective and pave the way for future development of full-reference and no-reference quality metrics. In addition, we established a comprehensive benchmark of various state-of-the-art objective metrics on the proposed database, highlighting that existing methods still struggle to accurately capture subjective quality. The results give us some insights into the limitations of existing NVS methods and may promote the development of new NVS methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="peng2025rmavatar" data-title="RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians" data-authors="Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong" data-year="2025" data-tags='["Avatar", "Code", "Dynamic", "Meshing", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'peng2025rmavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/peng2025rmavatar.jpg" data-fallback="None" alt="Paper thumbnail for RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.07104.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://rm-avatar.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/RMAvatar/RMAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures low-frequency motion and surface deformation of the avatar. Due to the limitations of LBS formula, the human skeleton is hard to control complex non-rigid transformations. We then design a pose-related Gaussian rectification module to learn fine-detailed non-rigid deformations, further improving the realism and expressiveness of the avatar. We conduct extensive experiments on public datasets, RMAvatar shows state-of-the-art performance on both rendering quality and quantitative evaluations. Please see our project page at https://rm-avatar.github.io.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zielonka2025synthetic" data-title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion" data-authors="Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart" data-year="2025" data-tags='["Avatar", "Dynamic", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zielonka2025synthetic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zielonka2025synthetic.jpg" data-fallback="None" alt="Paper thumbnail for Synthetic Prior for Few-Shot Drivable Head Avatar Inversion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Synthetic Prior for Few-Shot Drivable Head Avatar Inversion <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.06903.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zielon.github.io/synshot/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=4KQQatkaSgc" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle two major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to state-of-the-art monocular methods that require thousands of real training images, SynShot significantly improves novel view and expression synthesis.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2025generalized" data-title="Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution" data-authors="Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang" data-year="2025" data-tags='["Project", "Super Resolution"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2025generalized', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2025generalized.jpg" data-fallback="None" alt="Paper thumbnail for Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale Super-Resolution <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Super Resolution</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.06838.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mt-cly.github.io/GSASR.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Equipped with the continuous representation capability of Multi-Layer Perceptron (MLP), Implicit Neural Representation (INR) has been successfully employed for Arbitrary-scale Super-Resolution (ASR). However, the limited receptive field of the linear layers in MLP restricts the representation capability of INR, while it is computationally expensive to query the MLP numerous times to render each pixel. Recently, Gaussian Splatting (GS) has shown its advantages over INR in both visual quality and rendering speed in 3D tasks, which motivates us to explore whether GS can be employed for the ASR task. However, directly applying GS to ASR is exceptionally challenging because the original GS is an optimization-based method through overfitting each single scene, while in ASR we aim to learn a single model that can generalize to different images and scaling factors. We overcome these challenges by developing two novel techniques. Firstly, to generalize GS for ASR, we elaborately design an architecture to predict the corresponding image-conditioned Gaussians of the input low-resolution image in a feed-forward manner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based scale-aware rasterization to render super-resolved images by sampling discrete RGB values from the predicted contiguous Gaussians. Via end-to-end training, our optimized network, namely GSASR, can perform ASR for any image and unseen scaling factors. Extensive experiments validate the effectiveness of our proposed method. The project page can be found at \url{https://mt-cly.github.io/GSASR.github.io/}.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2025f3dgaus" data-title="F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with Cycle-Consistent Gaussian Splatting" data-authors="Yuxin Wang, Qianyi Wu, Dan Xu" data-year="2025" data-tags='["Code", "Feed-Forward", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2025f3dgaus', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2025f3dgaus.jpg" data-fallback="None" alt="Paper thumbnail for F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with Cycle-Consistent Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with Cycle-Consistent Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Yuxin Wang, Qianyi Wu, Dan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.06714.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://arxiv.org/abs/2501.06714" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/W-Ted/F3D-Gaus" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper tackles the problem of generalizable 3D-aware generation from monocular datasets, e.g., ImageNet. The key challenge of this task is learning a robust 3D-aware representation without multi-view or dynamic data, while ensuring consistent texture and geometry across different viewpoints. Although some baseline methods are capable of 3D-aware generation, the quality of the generated images still lags behind state-of-the-art 2D generation approaches, which excel in producing high-quality, detailed images. To address this severe limitation, we propose a novel feed-forward pipeline based on pixel-aligned Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and reliable 3D renderings from monocular inputs. In addition, we introduce a self-supervised cycle-consistent constraint to enforce cross-view consistency in the learned 3D representation. This training strategy naturally allows aggregation of multiple aligned Gaussian primitives and significantly alleviates the interpolation limitations inherent in single-view pixel-aligned Gaussian Splatting. Furthermore, we incorporate video model priors to perform geometry-aware refinement, enhancing the generation of fine details in wide-viewpoint scenarios and improving the model's capability to capture intricate 3D textures. Extensive experiments demonstrate that our approach not only achieves high-quality, multi-view consistent 3D-aware generation from monocular datasets, but also significantly improves training and inference efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="asim2025met3r" data-title="MEt3R: Measuring Multi-View Consistency in Generated Images" data-authors="Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, Jan Eric Lenssen" data-year="2025" data-tags='["3ster-based", "Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'asim2025met3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/asim2025met3r.jpg" data-fallback="None" alt="Paper thumbnail for MEt3R: Measuring Multi-View Consistency in Generated Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MEt3R: Measuring Multi-View Consistency in Generated Images <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, Jan Eric Lenssen</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.06336.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://geometric-rl.mpi-inf.mpg.de/met3r/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/mohammadasim98/MEt3R" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://geometric-rl.mpi-inf.mpg.de/met3r/static/videos/teaser.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce MEt3R, a metric for multi-view consistency in generated images. Large-scale generative models for multi-view image generation are rapidly advancing the field of 3D inference from sparse observations. However, due to the nature of generative modeling, traditional reconstruction metrics are not suitable to measure the quality of generated outputs and metrics that are independent of the sampling procedure are desperately needed. In this work, we specifically address the aspect of consistency between generated multi-view images, which can be evaluated independently of the specific scene. Our approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a feed-forward manner, which are used to warp image contents from one view into the other. Then, feature maps of these images are compared to obtain a similarity score that is invariant to view-dependent effects. Using MEt3R, we evaluate the consistency of a large set of previous methods for novel view and video generation, including our open, multi-view latent diffusion model.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shin2025localityaware" data-title="Locality-aware Gaussian Compression for Fast and High-quality Rendering" data-authors="Seungjoo Shin, Jaesik Park, Sunghyun Cho" data-year="2025" data-tags='["Compression"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shin2025localityaware', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shin2025localityaware.jpg" data-fallback="None" alt="Paper thumbnail for Locality-aware Gaussian Compression for Fast and High-quality Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Locality-aware Gaussian Compression for Fast and High-quality Rendering <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Seungjoo Shin, Jaesik Park, Sunghyun Cho</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.05757.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework that exploits the spatial coherence of 3D Gaussians for compact modeling of volumetric scenes. To this end, we first analyze the local coherence of 3D Gaussian attributes, and propose a novel locality-aware 3D Gaussian representation that effectively encodes locally-coherent Gaussian attributes using a neural field representation with a minimal storage requirement. On top of the novel representation, LocoGS is carefully designed with additional components such as dense initialization, an adaptive spherical harmonics bandwidth scheme and different encoding schemes for different Gaussian attributes to maximize compression performance. Experimental results demonstrate that our approach outperforms the rendering quality of existing compact Gaussian representations for representative real-world 3D datasets while achieving from 54.6$\times$ to 96.6$\times$ compressed storage size and from 2.1$\times$ to 2.4$\times$ rendering speed than 3DGS. Even our approach also demonstrates an averaged 2.4$\times$ higher rendering speed than the state-of-the-art compression method with comparable compression performance.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yan2025consistent" data-title="Consistent Flow Distillation for Text-to-3D Generation" data-authors="Runjie Yan, Yinbo Chen, Xiaolong Wang" data-year="2025" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yan2025consistent', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yan2025consistent.jpg" data-fallback="None" alt="Paper thumbnail for Consistent Flow Distillation for Text-to-3D Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Consistent Flow Distillation for Text-to-3D Generation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Runjie Yan, Yinbo Chen, Xiaolong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.05445.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://runjie-yan.github.io/cfd/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/runjie-yan/ConsistentFlowDistillation" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="meng2025zero1tog" data-title="Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation" data-authors="Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu" data-year="2025" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'meng2025zero1tog', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/meng2025zero1tog.jpg" data-fallback="None" alt="Paper thumbnail for Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.05427.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mengxuyigit.github.io/projects/zero-1-to-G/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gerogiannis2025arc2avatar" data-title="Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance" data-authors="Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou" data-year="2025" data-tags='["Avatar", "Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gerogiannis2025arc2avatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gerogiannis2025arc2avatar.jpg" data-fallback="None" alt="Paper thumbnail for Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.05379.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tianci2025scaffoldslam" data-title="Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping" data-authors="Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun" data-year="2025" data-tags='["SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tianci2025scaffoldslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tianci2025scaffoldslam.jpg" data-fallback="None" alt="Paper thumbnail for Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.05242.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bond2025gaussianvideo" data-title="GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting" data-authors="Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem" data-year="2025" data-tags='["Gaussian Video", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bond2025gaussianvideo', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bond2025gaussianvideo.jpg" data-fallback="None" alt="Paper thumbnail for GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem</p>
      <div class="paper-tags"><span class="paper-tag">Gaussian Video</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.04782.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://cyberiada.github.io/GaussianVideo/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2025fatesgs" data-title="FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency" data-authors="Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu" data-year="2025" data-tags='["Meshing", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2025fatesgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2025fatesgs.jpg" data-fallback="None" alt="Paper thumbnail for FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.04628.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://alvin528.github.io/FatesGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, Gaussian Splatting has sparked a new trend in the field of computer vision. Apart from novel view synthesis, it has also been extended to the area of multi-view reconstruction. The latest methods facilitate complete, detailed surface reconstruction while ensuring fast training speed. However, these methods still require dense input views, and their output quality significantly degrades with sparse views. We observed that the Gaussian primitives tend to overfit the few training views, leading to noisy floaters and incomplete reconstruction surfaces. In this paper, we present an innovative sparse-view reconstruction framework that leverages intra-view depth and multi-view feature consistency to achieve remarkably accurate surface reconstruction. Specifically, we utilize monocular depth ranking information to supervise the consistency of depth distribution within patches and employ a smoothness loss to enhance the continuity of the distribution. To achieve finer surface reconstruction, we optimize the absolute position of depth through multi-view projection features. Extensive experiments on DTU and BlendedMVS demonstrate that our method outperforms state-of-the-art methods with a speedup of 60x to 200x, achieving swift and fine-grained mesh reconstruction without the need for costly pre-training.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kwak2025modecgs" data-title="MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting" data-authors="Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim" data-year="2025" data-tags='["Compression", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kwak2025modecgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kwak2025modecgs.jpg" data-fallback="None" alt="Paper thumbnail for MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.03714.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/5L6gzc5-cw8?si=L6v6XLZFQrYK50iV" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2025dehazegs" data-title="DehazeGS: Seeing Through Fog with 3D Gaussian Splatting" data-authors="Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang" data-year="2025" data-tags='["In the Wild", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2025dehazegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2025dehazegs.jpg" data-fallback="None" alt="Paper thumbnail for DehazeGS: Seeing Through Fog with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DehazeGS: Seeing Through Fog with 3D Gaussian Splatting <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang</p>
      <div class="paper-tags"><span class="paper-tag">In the Wild</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.03659.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Current novel view synthesis tasks primarily rely on high-quality and clear images. However, in foggy scenes, scattering and attenuation can significantly degrade the reconstruction and rendering quality. Although NeRF-based dehazing reconstruction algorithms have been developed, their use of deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Moreover, NeRF's implicit representation struggles to recover fine details from hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly modeling point clouds into 3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation to explain the foggy image formation process through a physically accurate forward rendering process. We introduce DehazeGS, a method capable of decomposing and rendering a fog-free background from participating media using only muti-view foggy images as input. We model the transmission within each Gaussian distribution to simulate the formation of fog. During this process, we jointly learn the atmospheric light and scattering coefficient while optimizing the Gaussian representation of the hazy scene. In the inference stage, we eliminate the effects of scattering and attenuation on the Gaussians and directly project them onto a 2D plane to obtain a clear view. Experiments on both synthetic and real-world foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance in terms of both rendering quality and computational efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2025compression" data-title="Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs" data-authors="Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge" data-year="2025" data-tags='["Compression"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2025compression', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2025compression.jpg" data-fallback="None" alt="Paper thumbnail for Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compression of 3D Gaussian Splatting with Optimized Feature Planes and Standard Video Codecs <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.03399.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting is a recognized method for 3D scene representation, known for its high rendering quality and speed. However, its substantial data requirements present challenges for practical applications. In this paper, we introduce an efficient compression technique that significantly reduces storage overhead by using compact representation. We propose a unified architecture that combines point cloud data and feature planes through a progressive tri-plane structure. Our method utilizes 2D feature planes, enabling continuous spatial representation. To further optimize these representations, we incorporate entropy modeling in the frequency domain, specifically designed for standard video codecs. We also propose channel-wise bit allocation to achieve a better trade-off between bitrate consumption and feature plane representation. Consequently, our model effectively leverages spatial correlations within the feature planes to enhance rate-distortion performance using standard, non-differentiable video codecs. Experimental results demonstrate that our method outperforms existing methods in data compactness while maintaining high rendering quality. Our project page is available at https://fraunhoferhhi.github.io/CodecGS
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="rajasegaran2025gaussian" data-title="Gaussian Masked Autoencoders" data-authors="Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar" data-year="2025" data-tags='["Code", "Transformer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'rajasegaran2025gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/rajasegaran2025gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Masked Autoencoders" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Masked Autoencoders <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Transformer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.03229.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/darshanmakwana412/gaussian-mae" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="nguyen2025pointmapconditioned" data-title="Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis" data-authors="Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry Tsishkou, Laurent Caraffa, Jean-Philippe Tarel, Roland Brémond" data-year="2025" data-tags='["Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'nguyen2025pointmapconditioned', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/nguyen2025pointmapconditioned.jpg" data-fallback="None" alt="Paper thumbnail for Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Thang-Anh-Quan Nguyen, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry Tsishkou, Laurent Caraffa, Jean-Philippe Tarel, Roland Brémond</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.02913.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present PointmapDiffusion, a novel framework for single-image novel view synthesis (NVS) that utilizes pre-trained 2D diffusion models. Our method is the first to leverage pointmaps (i.e. rasterized 3D scene coordinates) as a conditioning signal, capturing geometric prior from the reference images to guide the diffusion process. By embedding reference attention blocks and a ControlNet for pointmap features, our model balances between generative capability and geometric consistency, enabling accurate view synthesis across varying viewpoints. Extensive experiments on diverse real-world datasets demonstrate that PointmapDiffusion achieves high-quality, multi-view consistent results with significantly fewer trainable parameters compared to other baselines for single-image NVS tasks.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bian2025gsdit" data-title="GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking" data-authors="Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li" data-year="2025" data-tags='["Year 2025"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bian2025gsdit', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bian2025gsdit.jpg" data-fallback="None" alt="Paper thumbnail for GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li</p>
      <div class="paper-tags"></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.02690.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cong2025videolifter" data-title="VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment" data-authors="Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, Zhiwen Fan" data-year="2025" data-tags='["Acceleration", "Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cong2025videolifter', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cong2025videolifter.jpg" data-fallback="None" alt="Paper thumbnail for VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, Zhiwen Fan</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01949.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://videolifter.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/VideoLifter" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Efficiently reconstructing accurate 3D models from monocular video is a key challenge in computer vision, critical for advancing applications in virtual reality, robotics, and scene understanding. Existing approaches typically require pre-computed camera parameters and frame-by-frame reconstruction pipelines, which are prone to error accumulation and entail significant computational overhead. To address these limitations, we introduce VideoLifter, a novel framework that leverages geometric priors from a learnable model to incrementally optimize a globally sparse to dense 3D representation directly from video sequences. VideoLifter segments the video sequence into local windows, where it matches and registers frames, constructs consistent fragments, and aligns them hierarchically to produce a unified 3D model. By tracking and propagating sparse point correspondences across frames and fragments, VideoLifter incrementally refines camera poses and 3D structure, minimizing reprojection error for improved accuracy and robustness. This approach significantly accelerates the reconstruction process, reducing training time by over 82% while surpassing current state-of-the-art methods in visual fidelity and computational efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2025enerverse" data-title="EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation" data-authors="Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren" data-year="2025" data-tags='["Dynamic", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2025enerverse', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2025enerverse.jpg" data-fallback="None" alt="Paper thumbnail for EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01895.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://sites.google.com/view/enerverse" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="longhini2024clothsplatting" data-title="Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision" data-authors="Alberta Longhini, Marcel Büsching, Bardienus Pieter Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic" data-year="2024" data-tags='["Code", "Meshing", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'longhini2024clothsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/longhini2024clothsplatting.jpg" data-fallback="None" alt="Paper thumbnail for Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Alberta Longhini, Marcel Büsching, Bardienus Pieter Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01715.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kth-rpl.github.io/cloth-splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/KTH-RPL/cloth-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth's state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time by ~85%.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2025crossviewgs" data-title="CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction" data-authors="Chenhao Zhang, Yuanping Cao, Lei Zhang" data-year="2025" data-tags='["Large-Scale", "Optimization"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2025crossviewgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2025crossviewgs.jpg" data-fallback="None" alt="Paper thumbnail for CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Chenhao Zhang, Yuanping Cao, Lei Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Optimization</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01695.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene representation and reconstruction, leveraging densely distributed Gaussian primitives to enable real-time rendering of high-resolution images. While existing 3DGS methods perform well in scenes with minor view variation, large view changes in cross-view scenes pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction, based on dual-branch fusion. Our method independently reconstructs models from aerial and ground views as two independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during both initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of dual-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2025pgsag" data-title="PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping" data-authors="Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan" data-year="2025" data-tags='["Large-Scale", "Meshing", "Optimization"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2025pgsag', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2025pgsag.jpg" data-fallback="None" alt="Paper thumbnail for PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Optimization</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01677.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-scale urban areas without downsampling the original image resolution. First, the Cross-modal model - Language Segment Anything is leveraged to segment building masks. Then, the segmented building regions is grouped into sub-regions according to the visibility check across registered images. The Gaussian kernels for these sub-regions are optimized in parallel with masked pixels. In addition, the normal loss is re-formulated for the detected edges of masks to alleviate the ambiguities in normal vectors on edges. Finally, to improve the optimization of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts for the complexity of the corresponding scenes, effectively minimizing the thread waiting time in the pixel-parallel rendering stage as well as the reconstruction lost. Extensive experiments are tested on various urban datasets, the results demonstrated the superior performance of our PG-SAG on building surface reconstruction, compared to several state-of-the-art 3DGS-based methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gao2025easysplat" data-title="EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy" data-authors="Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang" data-year="2025" data-tags='["3ster-based", "Acceleration", "Densification", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gao2025easysplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gao2025easysplat.jpg" data-fallback="None" alt="Paper thumbnail for EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Acceleration</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.01003.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2024storm" data-title="STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes" data-authors="Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone" data-year="2024" data-tags='["Autonomous Driving", "Dynamic", "Large-Scale", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2024storm', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2024storm.jpg" data-fallback="None" alt="Paper thumbnail for STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.00602.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jiawei-yang.github.io/STORM/" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., "amodal") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="mao2024dreamdrive" data-title="DreamDrive: Generative 4D Scene Modeling from Street View Images" data-authors="Jiageng Mao, Boyi Li, Boris Ivanovic, Yuxiao Chen, Yan Wang, Yurong You, Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang" data-year="2024" data-tags='["Autonomous Driving", "Dynamic", "Feed-Forward"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'mao2024dreamdrive', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/mao2024dreamdrive.jpg" data-fallback="None" alt="Paper thumbnail for DreamDrive: Generative 4D Scene Modeling from Street View Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamDrive: Generative 4D Scene Modeling from Street View Images <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiageng Mao, Boyi Li, Boris Ivanovic, Yuxiao Chen, Yan Wang, Yurong You, Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Feed-Forward</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.00601.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024sgsplatting" data-title="SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians" data-authors="Yiwen Wang, Siyuan Chen, Ran Yi" data-year="2024" data-tags='["Acceleration"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024sgsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024sgsplatting.jpg" data-fallback="None" alt="Paper thumbnail for SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yiwen Wang, Siyuan Chen, Ran Yi</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2501.00342.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cha2024perse" data-title="PERSE: Personalized 3D Generative Avatars from A Single Portrait" data-authors="Hyunsoo Cha, Inhee Lee, Hanbyul Joo" data-year="2024" data-tags='["Avatar", "GAN", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cha2024perse', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cha2024perse.jpg" data-fallback="None" alt="Paper thumbnail for PERSE: Personalized 3D Generative Avatars from A Single Portrait" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PERSE: Personalized 3D Generative Avatars from A Single Portrait <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hyunsoo Cha, Inhee Lee, Hanbyul Joo</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">GAN</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.21206v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hyunsoocha.github.io/perse/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/zX881Zx03o4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang20244d" data-title="4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives" data-authors="Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip H. S. Torr" data-year="2024" data-tags='["Compression", "Dynamic", "Large-Scale"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang20244d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang20244d.jpg" data-fallback="None" alt="Paper thumbnail for 4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Yu-Gang Jiang, Philip H. S. Torr</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Large-Scale</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.20720v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Dynamic 3D scene representation and novel view synthesis from captured videos are crucial for enabling immersive experiences required by AR/VR and metaverse applications. However, this task is challenging due to the complexity of unconstrained real-world scenes and their temporal dynamics. In this paper, we frame dynamic scenes as a spatio-temporal 4D volume learning problem, offering a native explicit reformulation with minimal assumptions about motion, which serves as a versatile dynamic scene learning framework. Specifically, we represent a target dynamic scene using a collection of 4D Gaussian primitives with explicit geometry and appearance features, dubbed as 4D Gaussian splatting (4DGS). This approach can capture relevant information in space and time by fitting the underlying spatio-temporal volume. Modeling the spacetime as a whole with 4D Gaussians parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, our model can naturally learn view-dependent and time-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS model is the first solution that supports real-time rendering of high-resolution, photorealistic novel views for complex dynamic scenes. To enhance efficiency, we derive several compact variants that effectively reduce memory footprint and mitigate the risk of overfitting. Extensive experiments validate the superiority of 4DGS in terms of visual quality and efficiency across a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D generation, scene understanding) and scenarios (e.g., single object, indoor scenes, driving environments, synthetic and real data).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024maskgaussian" data-title="MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks" data-authors="Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun" data-year="2024" data-tags='["Code", "Compression", "Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024maskgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024maskgaussian.jpg" data-fallback="None" alt="Paper thumbnail for MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, Xiao Sun</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.20522.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/kaikai23/MaskGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zeller2024gsplatloc" data-title="GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting" data-authors="Atticus J. Zeller" data-year="2024" data-tags='["Code", "In the Wild", "Point Cloud", "Poses", "Robotics", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zeller2024gsplatloc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zeller2024gsplatloc.jpg" data-fallback="None" alt="Paper thumbnail for GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Atticus J. Zeller</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">In the Wild</span>
<span class="paper-tag">Point Cloud</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.20056.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/AtticusZeller/GsplatLoc" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GSplatLoc, a camera localization method that leverages the differentiable rendering capabilities of 3D Gaussian splatting for ultra-precise pose estimation. By formulating pose estimation as a gradient-based optimization problem that minimizes discrepancies between rendered depth maps from a pre-existing 3D Gaussian scene and observed depth images, GSplatLoc achieves translational errors within 0.01 cm and near-zero rotational errors on the Replica dataset - significantly outperforming existing methods. Evaluations on the Replica and TUM RGB-D datasets demonstrate the method's robustness in challenging indoor environments with complex camera motions. GSplatLoc sets a new benchmark for localization in dense mapping, with important implications for applications requiring accurate real-time localization, such as robotics and augmented reality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024das3r" data-title="DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction" data-authors="Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao" data-year="2024" data-tags='["Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024das3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024das3r.jpg" data-fallback="None" alt="Paper thumbnail for DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.19584.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kai422.github.io/DAS3R/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/kai422/das3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://kai422.github.io/DAS3R/assets/davis.gif" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a novel framework for scene decomposition and static background reconstruction from everyday videos. By integrating the trained motion masks and modeling the static scene as Gaussian splats with dynamics-aware optimization, our method achieves more accurate background reconstruction results than previous works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods, DAS3R is more robust in complex motion scenarios, capable of handling videos where dynamic objects occupy a significant portion of the scene, and does not require camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates enhanced performance and robustness with a margin of more than 2 dB in PSNR. The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cai2024dust" data-title="Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images" data-authors="Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu" data-year="2024" data-tags='["Inpainting", "Poses", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cai2024dust', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cai2024dust.jpg" data-fallback="None" alt="Paper thumbnail for Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu</p>
      <div class="paper-tags"><span class="paper-tag">Inpainting</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.19518.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Photo-realistic scene reconstruction from sparse-view, uncalibrated images is highly required in practice. Although some successes have been made, existing methods are either Sparse-View but require accurate camera parameters (i.e., intrinsic and extrinsic), or SfM-free but need densely captured images. To combine the advantages of both methods while addressing their respective weaknesses, we propose Dust to Tower (D2T), an accurate and efficient coarse-to-fine framework to optimize 3DGS and image poses simultaneously from sparse and uncalibrated images. Our key idea is to first construct a coarse model efficiently and subsequently refine it using warped and inpainted images at novel viewpoints. To do this, we first introduce a Coarse Construction Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA) module to refine the coarse depth maps by aligning their confident parts with estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting (WIGI) module is proposed to warp the training images to novel viewpoints by the refined depth maps, and inpainting is applied to fulfill the ``holes" in the warped images caused by view-direction changes, providing high-quality supervision to further optimize the 3D model and the camera poses. Extensive experiments and ablation studies demonstrate the validity of D2T and its design choices, achieving state-of-the-art performance in both tasks of novel view synthesis and pose estimation while keeping high efficiency. Codes will be publicly available.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yao2024reflective" data-title="Reflective Gaussian Splatting" data-authors="Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang" data-year="2024" data-tags='["Meshing", "Project", "Ray Tracing", "Relight"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yao2024reflective', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yao2024reflective.jpg" data-fallback="None" alt="Paper thumbnail for Reflective Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Reflective Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Relight</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.19282.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fudan-zvg.github.io/ref-gaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (\textbf{Ref-Gaussian}) framework characterized with two components: (I) {\em Physically based deferred rendering} that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) {\em Gaussian-grounded inter-reflection} that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qian2024weathergs" data-title="WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting" data-authors="Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula" data-year="2024" data-tags='["Code", "In the Wild"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qian2024weathergs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qian2024weathergs.jpg" data-fallback="None" alt="Paper thumbnail for WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">In the Wild</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.18862.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Jumponthemoon/WeatherGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lyu2024facelift" data-title="FaceLift: Single Image to 3D Head with View Generation and GS-LRM" data-authors="Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu" data-year="2024" data-tags='["Avatar", "Feed-Forward", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lyu2024facelift', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lyu2024facelift.jpg" data-fallback="None" alt="Paper thumbnail for FaceLift: Single Image to 3D Head with View Generation and GS-LRM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FaceLift: Single Image to 3D Head with View Generation and GS-LRM <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.17812.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.wlyu.me/FaceLift/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://huggingface.co/wlyu/FaceLift/resolve/main/videos/website_video.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shao2024gausim" data-title="GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator" data-authors="Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai" data-year="2024" data-tags='["Dynamic", "Physics", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shao2024gausim', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shao2024gausim.jpg" data-fallback="None" alt="Paper thumbnail for GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GauSim: Registering Elastic Objects into Digital World by Gaussian Simulator <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yidi Shao, Mu Huang, Chen Change Loy, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.17804.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.mmlab-ntu.com/project/gausim/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this work, we introduce GauSim, a novel neural network-based simulator designed to capture the dynamic behaviors of real-world elastic objects represented through Gaussian kernels. Unlike traditional methods that treat kernels as particles within particle-based simulations, we leverage continuum mechanics, modeling each kernel as a continuous piece of matter to account for realistic deformations without idealized assumptions. To improve computational efficiency and fidelity, we employ a hierarchical structure that organizes kernels into Center of Mass Systems (CMS) with explicit formulations, enabling a coarse-to-fine simulation approach. This structure significantly reduces computational overhead while preserving detailed dynamics. In addition, GauSim incorporates explicit physics constraints, such as mass and momentum conservation, ensuring interpretable results and robust, physically plausible simulations. To validate our approach, we present a new dataset, READY, containing multi-view videos of real-world elastic deformations. Experimental results demonstrate that GauSim achieves superior performance compared to existing physics-driven baselines, offering a practical and accurate solution for simulating complex dynamic behaviors. Code and model will be released.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jin2024activegs" data-title="ActiveGS: Active Scene Reconstruction using Gaussian Splatting" data-authors="Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija Popović" data-year="2024" data-tags='["Meshing", "Robotics", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jin2024activegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jin2024activegs.jpg" data-fallback="None" alt="Paper thumbnail for ActiveGS: Active Scene Reconstruction using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ActiveGS: Active Scene Reconstruction using Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija Popović</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.17769.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an on-board RGB-D camera. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. The core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the applicability of our active scene reconstruction framework in the real world using an unmanned aerial vehicle.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gao2024cosurfgscollaborative" data-title="CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction" data-authors="Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen Zhang, Tong He, Guofeng Zhang, Junwei Han" data-year="2024" data-tags='["Distributed", "Large-Scale", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gao2024cosurfgscollaborative', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gao2024cosurfgscollaborative.jpg" data-fallback="None" alt="Paper thumbnail for CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen Zhang, Tong He, Guofeng Zhang, Junwei Han</p>
      <div class="paper-tags"><span class="paper-tag">Distributed</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.17612.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gyy456.github.io/CoSurfGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene reconstruction. However, most existing GS-based surface reconstruction methods focus on 3D objects or limited scenes. Directly applying these methods to large-scale scene reconstruction will pose challenges such as high memory costs, excessive time consumption, and lack of geometric detail, which makes it difficult to implement in practical applications. To address these issues, we propose a multi-agent collaborative fast 3DGS surface reconstruction framework based on distributed learning for large-scale surface reconstruction. Specifically, we develop local model compression (LMC) and model aggregation schemes (MAS) to achieve high-quality surface representation of large scenes while reducing GPU memory consumption. Extensive experiments on Urban3d, MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast and scalable high-fidelity surface reconstruction and photorealistic rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gui2024balanced" data-title="Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling" data-authors="Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu" data-year="2024" data-tags='["Acceleration"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gui2024balanced', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gui2024balanced.jpg" data-fallback="None" alt="Paper thumbnail for Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.17378.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jambon2024interactive" data-title="Interactive Scene Authoring with Specialized Generative Primitives" data-authors="Clément Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young Min Kim" data-year="2024" data-tags='["Editing", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jambon2024interactive', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jambon2024interactive.jpg" data-fallback="None" alt="Paper thumbnail for Interactive Scene Authoring with Specialized Generative Primitives" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Interactive Scene Authoring with Specialized Generative Primitives <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Clément Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young Min Kim</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.16253.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Generating high-quality 3D digital assets often requires expert knowledge of complex design tools. We introduce Specialized Generative Primitives, a generative framework that allows non-expert users to author high-quality 3D scenes in a seamless, lightweight, and controllable manner. Each primitive is an efficient generative model that captures the distribution of a single exemplar from the real world. With our framework, users capture a video of an environment, which we turn into a high-quality and explicit appearance model thanks to 3D Gaussian Splatting. Users then select regions of interest guided by semantically-aware features. To create a generative primitive, we adapt Generative Cellular Automata to single-exemplar training and controllable generation. We decouple the generative task from the appearance model by operating on sparse voxels and we recover a high-quality output with a subsequent sparse patch consistency step. Each primitive can be trained within 10 minutes and used to author new scenes interactively in a fully compositional manner. We showcase interactive sessions where various primitives are extracted from real-world scenes and controlled to create 3D assets and scenes in a few minutes. We also demonstrate additional capabilities of our primitives: handling various 3D representations to control generation, transferring appearances, and editing geometries.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shen2024solidgs" data-title="SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction" data-authors="Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang, Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang" data-year="2024" data-tags='["Meshing", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shen2024solidgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shen2024solidgs.jpg" data-fallback="None" alt="Paper thumbnail for SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang, Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.15400.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mickshen7558.github.io/projects/SolidGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian splatting has achieved impressive improvements for both novel-view synthesis and surface reconstruction from multi-view images. However, current methods still struggle to reconstruct high-quality surfaces from only sparse view input images using Gaussian splatting. In this paper, we propose a novel method called SolidGS to address this problem. We observed that the reconstructed geometry can be severely inconsistent across multi-views, due to the property of Gaussian function in geometry rendering. This motivates us to consolidate all Gaussians by adopting a more solid kernel function, which effectively improves the surface reconstruction quality. With the additional help of geometrical regularization and monocular normal estimation, our method achieves superior performance on the sparse view surface reconstruction than all the Gaussian splatting methods and neural field methods on the widely used DTU, Tanks-and-Temples, and LLFF datasets.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xie2024envgs" data-title="EnvGS: Modeling View-Dependent Appearance with Environment Gaussian" data-authors="Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou" data-year="2024" data-tags='["Project", "Ray Tracing", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xie2024envgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xie2024envgs.jpg" data-fallback="None" alt="Paper thumbnail for EnvGS: Modeling View-Dependent Appearance with Environment Gaussian" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EnvGS: Modeling View-Dependent Appearance with Environment Gaussian <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.15215.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zju3dv.github.io/envgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://raw.githubusercontent.com/xbillowy/assets/refs/heads/main/envgs.github.io.assets/teaser.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing complex reflections in real-world scenes from 2D images is essential for achieving photorealistic novel view synthesis. Existing methods that utilize environment maps to model reflections from distant lighting often struggle with high-frequency reflection details and fail to account for near-field reflections. In this work, we introduce EnvGS, a novel approach that employs a set of Gaussian primitives as an explicit 3D representation for capturing reflections of environments. These environment Gaussian primitives are incorporated with base Gaussian primitives to model the appearance of the whole scene. To efficiently render these environment Gaussian primitives, we developed a ray-tracing-based renderer that leverages the GPU's RT core for fast rendering. This allows us to jointly optimize our model for high-quality reconstruction while maintaining real-time rendering speeds. Results from multiple real-world and synthetic datasets demonstrate that our method produces significantly more detailed reflections, achieving the best rendering quality in real-time novel view synthesis.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="saito2024squeezeme" data-title="SqueezeMe: Efficient Gaussian Avatars for VR" data-authors="Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon" data-year="2024" data-tags='["Avatar", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'saito2024squeezeme', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/saito2024squeezeme.jpg" data-fallback="None" alt="Paper thumbnail for SqueezeMe: Efficient Gaussian Avatars for VR" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SqueezeMe: Efficient Gaussian Avatars for VR <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.15171.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://forresti.github.io/squeezeme" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting has enabled real-time 3D human avatars with unprecedented levels of visual quality. While previous methods require a desktop GPU for real-time inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto a portable virtual reality headset with real-time drivable inference. We begin by training a previous work, Animatable Gaussians, on a high quality dataset captured with 512 cameras. The Gaussians are animated by controlling base set of Gaussians with linear blend skinning (LBS) motion and then further adjusting the Gaussians with a neural network decoder to correct their appearance. When deploying the model on a Meta Quest 3 VR headset, we find two major computational bottlenecks: the decoder and the rendering. To accelerate the decoder, we train the Gaussians in UV-space instead of pixel-space, and we distill the decoder to a single neural network layer. Further, we discover that neighborhoods of Gaussians can share a single corrective from the decoder, which provides an additional speedup. To accelerate the rendering, we develop a custom pipeline in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently at 72 FPS on a VR headset. 
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="peng2024gags" data-title="GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting" data-authors="Yuning Peng, Haiping Wang, Yuan Liu, Chenglu Wen, Zhen Dong, Bisheng Yang" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'peng2024gags', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/peng2024gags.jpg" data-fallback="None" alt="Paper thumbnail for GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuning Peng, Haiping Wang, Yuan Liu, Chenglu Wen, Zhen Dong, Bisheng Yang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.13654.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pz0826.github.io/GAGS-Webpage/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/WHU-USI3DV/GAGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D open-vocabulary scene understanding, which accurately perceives complex semantic properties of objects in space, has gained significant attention in recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP features into 3D Gaussian splatting, enabling open-vocabulary queries for renderings on arbitrary viewpoints. The main challenge of distilling 2D features for 3D fields lies in the multiview inconsistency of extracted 2D features, which provides unstable supervision for the 3D feature field. GAGS addresses this challenge with two novel strategies. First, GAGS associates the prompt point density of SAM with the camera distances, which significantly improves the multiview consistency of segmentation results. Second, GAGS further decodes a granularity factor to guide the distillation process and this granularity factor can be learned in a unsupervised manner to only select the multiview consistent 2D features in the distillation process. Experimental results on two datasets demonstrate significant performance and stability improvements of GAGS in visual grounding and semantic segmentation, with an inference speed 2$\times$ faster than baseline methods. The code and additional results are available at https://pz0826.github.io/GAGS-Webpage/ .
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lu2024turbogs" data-title="Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields" data-authors="Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar" data-year="2024" data-tags='["Acceleration", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lu2024turbogs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lu2024turbogs.jpg" data-fallback="None" alt="Paper thumbnail for Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.13547v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ivl.cs.brown.edu/research/turbo-gs" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2024gausstr" data-title="GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding" data-authors="Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang" data-year="2024" data-tags='["Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2024gausstr', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2024gausstr.jpg" data-fallback="None" alt="Paper thumbnail for GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.13193.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hustvl.github.io/GaussTR/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hustvl/GaussTR" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Semantic Occupancy Prediction is fundamental for spatial understanding as it provides a comprehensive semantic cognition of surrounding environments. However, prevalent approaches primarily rely on extensive labeled data and computationally intensive voxel-based modeling, restricting the scalability and generalizability of 3D representation learning. In this paper, we introduce GaussTR, a novel Gaussian Transformer that leverages alignment with foundation models to advance self-supervised 3D spatial understanding. GaussTR adopts a Transformer architecture to predict sparse sets of 3D Gaussians that represent scenes in a feed-forward manner. Through aligning rendered Gaussian features with diverse knowledge from pre-trained foundation models, GaussTR facilitates the learning of versatile 3D representations and enables open-vocabulary occupancy prediction without explicit annotations. Empirical evaluations on the Occ3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot performance, achieving 11.70 mIoU while reducing training duration by approximately 50%. These experimental results highlight the significant potential of GaussTR for scalable and holistic 3D spatial understanding, with promising implications for autonomous driving and embodied agents. Code is available at https://github.com/hustvl/GaussTR.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun2024realtime" data-title="Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures" data-authors="Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann" data-year="2024" data-tags='["Avatar", "Project", "Sparse", "Texturing", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun2024realtime', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun2024realtime.jpg" data-fallback="None" alt="Paper thumbnail for Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Texturing</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.13183v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vcai.mpi-inf.mpg.de/projects/DUT/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://vcai.mpi-inf.mpg.de/projects/DUT/videos/main_video.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="weiss2024gaussian" data-title="Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures" data-authors="Sebastian Weiss, Derek Bradley" data-year="2024" data-tags='["2DGS", "Texturing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'weiss2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/weiss2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sebastian Weiss, Derek Bradley</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Texturing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12734v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting has recently emerged as the go-to representation for reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian primitives has further improved multi-view consistency and surface reconstruction accuracy. In this work we highlight the similarity between 2D Gaussian Splatting (2DGS) and billboards from traditional computer graphics. Both use flat semi-transparent 2D geometry that is positioned, oriented and scaled in 3D space. However 2DGS uses a solid color per splat and an opacity modulated by a Gaussian distribution, where billboards are more expressive, modulating the color with a uv-parameterized texture. We propose to unify these concepts by presenting Gaussian Billboards, a modification of 2DGS to add spatially-varying color achieved using per-splat texture interpolation. The result is a mixture of the two representations, which benefits from both the robust scene optimization power of 2DGS and the expressiveness of texture mapping. We show that our method can improve the sharpness and quality of the scene representation in a wide range of qualitative and quantitative evaluations compared to the original 2DGS implementation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu20243dgut" data-title="3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting" data-authors="Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, Zan Gojcic" data-year="2024" data-tags='["Perspective-correct", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu20243dgut', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu20243dgut.jpg" data-fallback="None" alt="Paper thumbnail for 3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, Zan Gojcic</p>
      <div class="paper-tags"><span class="paper-tag">Perspective-correct</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12507.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://research.nvidia.com/labs/toronto-ai/3DGUT/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://research.nvidia.com/labs/toronto-ai/3DGUT/res/3DGUT_ready_compressed.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has shown great potential for efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing volumetric particles instead, however, this comes at the cost of significantly slower rendering speeds. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="murai2024mast3rslam" data-title="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors" data-authors="Riku Murai, Eric Dexheimer, Andrew J. Davison" data-year="2024" data-tags='["3ster-based", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'murai2024mast3rslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/murai2024mast3rslam.jpg" data-fallback="None" alt="Paper thumbnail for MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Riku Murai, Eric Dexheimer, Andrew J. Davison</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12392.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.youtube.com/watch?v=wozt71NBFTQ" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a real-time monocular dense SLAM system designed bottom-up from MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this strong prior, our system is robust on in-the-wild video sequences despite making no assumption on a fixed or parametric camera model beyond a unique camera centre. We introduce efficient methods for pointmap matching, camera tracking and local fusion, graph construction and loop closure, and second-order global optimisation. With known calibration, a simple modification to the system achieves state-of-the-art performance across various benchmarks. Altogether, we propose a plug-and-play monocular SLAM system capable of producing globally-consistent poses and dense geometry while operating at 15 FPS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024pansplat" data-title="PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting" data-authors="Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai" data-year="2024" data-tags='["360 degree", "Code", "Feed-Forward", "Project", "Video", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024pansplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024pansplat.jpg" data-fallback="None" alt="Paper thumbnail for PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai</p>
      <div class="paper-tags"><span class="paper-tag">360 degree</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12096v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://chengzhag.github.io/publication/pansplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/chengzhag/PanSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/R3qIzL77ZSc" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">With the advent of portable 360{\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 $\times$ 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\times$ 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets. Code will be available at \url{https://github.com/chengzhag/PanSplat}.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="taubner2024cap4d" data-title="CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models" data-authors="Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell" data-year="2024" data-tags='["Avatar", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'taubner2024cap4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/taubner2024cap4d.jpg" data-fallback="None" alt="Paper thumbnail for CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12093" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://felixtaubner.github.io/cap4d/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints − for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2024wonderland" data-title="Wonderland: Navigating 3D Scenes from a Single Image" data-authors="Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren" data-year="2024" data-tags='["Feed-Forward", "Project", "Sparse", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2024wonderland', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2024wonderland.jpg" data-fallback="None" alt="Paper thumbnail for Wonderland: Navigating 3D Scenes from a Single Image" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Wonderland: Navigating 3D Scenes from a Single Image <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</p>
      <div class="paper-tags"><span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.12091v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://snap-research.github.io/wonderland/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2024deformable" data-title="Deformable Radial Kernel Splatting" data-authors="Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi" data-year="2024" data-tags='["Optimization", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2024deformable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2024deformable.jpg" data-fallback="None" alt="Paper thumbnail for Deformable Radial Kernel Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Deformable Radial Kernel Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</p>
      <div class="paper-tags"><span class="paper-tag">Optimization</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.11752v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yihua7.github.io/DRK-web/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, Gaussian splatting has emerged as a robust technique for representing 3D scenes, enabling real-time rasterization and high-fidelity rendering. However, Gaussians' inherent radial symmetry and smoothness constraints limit their ability to represent complex shapes, often requiring thousands of primitives to approximate detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature. iven DRK's planar nature, we further develop accurate ray-primitive intersection computation for depth sorting and introduce efficient kernel culling strategies for improved rasterization efficiency. Extensive experiments demonstrate that DRK outperforms existing methods in both representation efficiency and rendering quality, achieving state-of-the-art performance while dramatically reducing primitive count.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024gaussianproperty" data-title="GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs" data-authors="Xinli Xu, Wenhang Ge, Dicong Qiu, ZhiFei Chen, Dongyu Yan, Zhuoyun Liu, Haoyu Zhao, Hanfeng Zhao, Shunsi Zhang, Junwei Liang, Ying-Cong Chen" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024gaussianproperty', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024gaussianproperty.jpg" data-fallback="None" alt="Paper thumbnail for GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xinli Xu, Wenhang Ge, Dicong Qiu, ZhiFei Chen, Dongyu Yan, Zhuoyun Liu, Haoyu Zhao, Hanfeng Zhao, Shunsi Zhang, Junwei Liang, Ying-Cong Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.11258.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussian-property.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/xxlbigbrother/Gaussian-Property" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on \href{https://Gaussian-Property.github.io}{this https URL}.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2024supergseg" data-title="SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians" data-authors="Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari" data-year="2024" data-tags='["Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2024supergseg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2024supergseg.jpg" data-fallback="None" alt="Paper thumbnail for SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari</p>
      <div class="paper-tags"><span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.10231.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://supergseg.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tang2024gaf" data-title="GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion" data-authors="Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Nießner" data-year="2024" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tang2024gaf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tang2024gaf.jpg" data-fallback="None" alt="Paper thumbnail for GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Nießner</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.10209" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://tangjiapeng.github.io/projects/GAF/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/embed/QuIYTljvhygE" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve details of facial identity and appearance. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms the previous state-of-the-art methods in novel view synthesis and novel expression animation. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="park2024splinegs" data-title="SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video" data-authors="Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim" data-year="2024" data-tags='["Dynamic", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'park2024splinegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/park2024splinegs.jpg" data-fallback="None" alt="Paper thumbnail for SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.09982v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kaist-viclab.github.io/splinegs-site/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/KAIST-VICLab/SplineGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=EZbSV7QCbE8" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024representing" data-title="Representing Long Volumetric Video with Temporal Gaussian Hierarchy" data-authors="Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou" data-year="2024" data-tags='["Acceleration", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024representing', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024representing.jpg" data-fallback="None" alt="Paper thumbnail for Representing Long Volumetric Video with Temporal Gaussian Hierarchy" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Representing Long Volumetric Video with Temporal Gaussian Hierarchy <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, Xiaowei Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.09608.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zju3dv.github.io/longvolcap/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=y7e0YRNNmXw&feature=youtu.be" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1~2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling minutes of volumetric video data while maintaining state-of-the-art rendering quality. Our project page is available at: https://zju3dv.github.io/longvolcap.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024feat2gs" data-title="Feat2GS: Probing Visual Foundation Models with Gaussian Splatting" data-authors="Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu" data-year="2024" data-tags='["Project", "Rendering", "Video", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024feat2gs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024feat2gs.jpg" data-fallback="None" alt="Paper thumbnail for Feat2GS: Probing Visual Foundation Models with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Feat2GS: Probing Visual Foundation Models with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.09606.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fanegg.github.io/Feat2GS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/4fT5lzcAJqo" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry ($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024simavatar" data-title="SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing" data-authors="Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal" data-year="2024" data-tags='["Avatar", "Diffusion", "Language Embedding", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024simavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024simavatar.jpg" data-fallback="None" alt="Paper thumbnail for SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.09545.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nvlabs.github.io/SimAvatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=qEwBY7LBW2Y" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024slam3r" data-title="SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos" data-authors="Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen" data-year="2024" data-tags='["3ster-based", "Code", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024slam3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024slam3r.jpg" data-fallback="None" alt="Paper thumbnail for SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.09401.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/PKU-VCL-3DV/SLAM3R" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides an end-to-end solution by seamlessly integrating local 3D reconstruction and global coordinate registration through feed-forward neural networks. Given an input video, the system first converts it into overlapping clips using a sliding window mechanism. Unlike traditional pose optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB images in each window and progressively aligns and deforms these local pointmaps to create a globally consistent scene reconstruction - all without explicitly solving any camera parameters. Experiments across datasets consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy and completeness while maintaining real-time performance at 20+ FPS. Code and weights at: https://github.com/PKU-VCL-3DV/SLAM3R.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gomel2024diffusionbased" data-title="Diffusion-Based Attention Warping for Consistent 3D Scene Editing" data-authors="Eyal Gomel, Lior Wolf" data-year="2024" data-tags='["Diffusion", "Project", "Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gomel2024diffusionbased', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gomel2024diffusionbased.jpg" data-fallback="None" alt="Paper thumbnail for Diffusion-Based Attention Warping for Consistent 3D Scene Editing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Diffusion-Based Attention Warping for Consistent 3D Scene Editing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Eyal Gomel, Lior Wolf</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.07984.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://attention-warp.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fidelity and spatial alignment in 3D space. Extensive evaluations demonstrate the effectiveness of our method in generating versatile edits of 3D scenes, significantly advancing the capabilities of scene manipulation compared to the existing methods. Project page: \url{https://attention-warp.github.io}
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="saunders2024gasp" data-title="GASP: Gaussian Avatars with Synthetic Priors" data-authors="Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrušaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay Namboodiri, Benjamin Lundell" data-year="2024" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'saunders2024gasp', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/saunders2024gasp.jpg" data-fallback="None" alt="Paper thumbnail for GASP: Gaussian Avatars with Synthetic Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GASP: Gaussian Avatars with Synthetic Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrušaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay Namboodiri, Benjamin Lundell</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.07739" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://microsoft.github.io/GASP/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=3oWB7-UJUYE" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or video and fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360-degree rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024faster" data-title="Faster and Better 3D Splatting via Group Training" data-authors="Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao" data-year="2024" data-tags='["Acceleration", "Densification", "Optimization"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024faster', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024faster.jpg" data-fallback="None" alt="Paper thumbnail for Faster and Better 3D Splatting via Group Training" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Faster and Better 3D Splatting via Group Training <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Optimization</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.07608.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis, demonstrating remarkable capability in high-fidelity scene reconstruction through its Gaussian primitive representations. However, the computational overhead induced by the massive number of primitives poses a significant bottleneck to training efficiency. To overcome this challenge, we propose Group Training, a simple yet effective strategy that organizes Gaussian primitives into manageable groups, optimizing training efficiency and improving rendering quality. This approach shows universal compatibility with existing 3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently achieving accelerated training while maintaining superior synthesis quality. Extensive experiments reveal that our straightforward Group Training strategy achieves up to 30% faster convergence and improved rendering quality across diverse scenarios.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024recap" data-title="ReCap: Better Gaussian Relighting with Cross-Environment Captures" data-authors="Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte" data-year="2024" data-tags='["Project", "Relight"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024recap', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024recap.jpg" data-fallback="None" alt="Paper thumbnail for ReCap: Better Gaussian Relighting with Cross-Environment Captures" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ReCap: Better Gaussian Relighting with Cross-Environment Captures <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.07534.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jingzhi.github.io/ReCap/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Accurate 3D objects relighting in diverse unseen environments is crucial for realistic virtual object placement. Due to the albedo-lighting ambiguity, existing methods often fall short in producing faithful relights. Without proper constraints, observed training views can be explained by numerous combinations of lighting and material attributes, lacking physical correspondence with the actual environment maps used for relighting. In this work, we present ReCap, treating cross-environment captures as multi-task target to provide the missing supervision that cuts through the entanglement. Specifically, ReCap jointly optimizes multiple lighting representations that share a common set of material attributes. This naturally harmonizes a coherent set of lighting representations around the mutual material attributes, exploiting commonalities and differences across varied object appearances. Such coherence enables physically sound lighting reconstruction and robust material estimation - both essential for accurate relighting. Together with a streamlined shading function and effective post-processing, ReCap outperforms the leading competitor by 3.4 dB in PSNR on an expanded relighting benchmark.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lyu2024resgs" data-title="ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery" data-authors="Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen" data-year="2024" data-tags='["Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lyu2024resgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lyu2024resgs.jpg" data-fallback="None" alt="Paper thumbnail for ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.07494.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view synthesis, achieving high fidelity and efficiency. However, it often struggles to capture rich details and complete geometry. Our analysis highlights a key limitation of 3D-GS caused by the fixed threshold in densification, which balances geometry coverage against detail recovery as the threshold varies. To address this, we introduce a novel densification method, residual split, which adds a downscaled Gaussian as a residual. Our approach is capable of adaptively retrieving details and complementing missing geometry while enabling progressive refinement. To further support this method, we propose a pipeline named ResGS. Specifically, we integrate a Gaussian image pyramid for progressive supervision and implement a selection scheme that prioritizes the densification of coarse Gaussians over time. Extensive experiments demonstrate that our method achieves SOTA rendering quality. Consistent performance improvements can be achieved by applying our residual split on various 3D-GS variants, underscoring its versatility and potential for broader application in 3D-GS-based applications.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tang2024mvdust3r" data-title="MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds" data-authors="Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan" data-year="2024" data-tags='["3ster-based", "Code", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tang2024mvdust3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tang2024mvdust3r.jpg" data-fallback="None" alt="Paper thumbnail for MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.06974.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mv-dust3rp.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/facebookresearch/mvdust3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/LBvnuKQ8Rso" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guédon2024matcha" data-title="MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views" data-authors="Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino" data-year="2024" data-tags='["Meshing", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guédon2024matcha', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guédon2024matcha.jpg" data-fallback="None" alt="Paper thumbnail for MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Antoine Guédon, Tomoki Ichikawa, Kohei Yamashita, Ko Nishino</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.06767.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://anttwo.github.io/matcha/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism. Our project page is the following: https://anttwo.github.io/matcha/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024deblur4dgs" data-title="Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video" data-authors="Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, Wangmeng Zuo" data-year="2024" data-tags='["Code", "Deblurring", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024deblur4dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024deblur4dgs.jpg" data-fallback="None" alt="Paper thumbnail for Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, Wangmeng Zuo</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Deblurring</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.06424.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://deblur4dgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ZcsrenlongZ/Deblur4DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=1I2N2IDneUQ" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent 4D reconstruction methods have yielded impressive results but rely on sharp videos as supervision. However, motion blur often occurs in videos due to camera shake and object movement, while existing methods render blurry results when using such videos for reconstructing 4D models. Although a few NeRF-based approaches attempted to address the problem, they struggled to produce high-quality results, due to the inaccuracy in estimating continuous dynamic representations within the exposure time. Encouraged by recent works in 3D motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest taking 3DGS as the scene representation manner, and propose the first 4D Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry monocular video, named Deblur4DGS. Specifically, we transform continuous dynamic representations estimation within an exposure time into the exposure time estimation. Moreover, we introduce exposure regularization to avoid trivial solutions, as well as multi-frame and multi-resolution consistency ones to alleviate artifacts. Furthermore, to better represent objects with large motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view synthesis, Deblur4DGS can be applied to improve blurry video from multiple perspectives, including deblurring, frame interpolation, and video stabilization. Extensive experiments on the above four tasks show that Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes are available at https://github.com/ZcsrenlongZ/Deblur4DGS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yan20244d" data-title="4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes" data-authors="Jinbo Yan, Rui Peng, Luyang Tang, Ronggang Wang" data-year="2024" data-tags='["Code", "Densification", "Dynamic", "Optimization", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yan20244d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yan20244d.jpg" data-fallback="None" alt="Paper thumbnail for 4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jinbo Yan, Rui Peng, Luyang Tang, Ronggang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Optimization</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.06299.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yjb6.github.io/SaRO-GS.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yjb6/SaRO-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing dynamic scenes from video sequences is a highly promising task in the multimedia domain. While previous methods have made progress, they often struggle with slow rendering and managing temporal complexities such as significant motion and object appearance/disappearance. In this paper, we propose SaRO-GS as a novel dynamic scene representation capable of achieving real-time rendering while effectively handling temporal complexities in dynamic scenes. To address the issue of slow rendering speed, we adopt a Gaussian primitive-based representation and optimize the Gaussians in 4D space, which facilitates real-time rendering with the assistance of 3D Gaussian Splatting. Additionally, to handle temporally complex dynamic scenes, we introduce a Scale-aware Residual Field. This field considers the size information of each Gaussian primitive while encoding its residual feature and aligns with the self-splitting behavior of Gaussian primitives. Furthermore, we propose an Adaptive Optimization Schedule, which assigns different optimization strategies to Gaussian primitives based on their distinct temporal properties, thereby expediting the reconstruction of dynamic regions. Through evaluations on monocular and multi-view datasets, our method has demonstrated state-of-the-art performance. Please see our project page at https://yjb6.github.io/SaRO-GS.github.io.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qiu2024advancing" data-title="Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects" data-authors="Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qiu2024advancing', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qiu2024advancing.jpg" data-fallback="None" alt="Paper thumbnail for Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.06257" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has attracted significant attention for its potential to revolutionize 3D representation, rendering, and interaction. Despite the rapid growth of 3DGS research, its direct application to Extended Reality (XR) remains underexplored. Although many studies recognize the potential of 3DGS for XR, few have explicitly focused on or demonstrated its effectiveness within XR environments. In this paper, we aim to synthesize innovations in 3DGS that show specific potential for advancing XR research and development. We conduct a comprehensive review of publicly available 3DGS papers, with a focus on those referencing XR-related concepts. Additionally, we perform an in-depth analysis of innovations explicitly relevant to XR and propose a taxonomy to highlight their significance. Building on these insights, we propose several prospective XR research areas where 3DGS can make promising contributions, yet remain rarely touched. By investigating the intersection of 3DGS and XR, this paper provides a roadmap to push the boundaries of XR using cutting-edge 3DGS techniques.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="javed2024temporally" data-title="Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes" data-authors="Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, Mathieu Salzmann" data-year="2024" data-tags='["Acceleration", "Compression", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'javed2024temporally', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/javed2024temporally.jpg" data-fallback="None" alt="Paper thumbnail for Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, Mathieu Salzmann</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.05700.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ahmad-jarrar.github.io/tc-3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in high-fidelity dynamic scene reconstruction have leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene representation. However, to make these methods viable for real-time applications such as AR/VR, gaming, and rendering on low-power devices, substantial reductions in memory usage and improvements in rendering efficiency are required. While many state-of-the-art methods prioritize lightweight implementations, they struggle in handling scenes with complex motions or long sequences. In this work, we introduce Temporally Compressed 3D Gaussian Splatting (TC3DGS), a novel technique designed specifically to effectively compress dynamic 3D Gaussian representations. TC3DGS selectively prunes Gaussians based on their temporal relevance and employs gradient-aware mixed-precision quantization to dynamically compress Gaussian parameters. It additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a post-processing step to further reduce storage by interpolating Gaussian trajectories across frames. Our experiments across multiple datasets demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or no degradation in visual quality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fan2024momentumgs" data-title="Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction" data-authors="Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang" data-year="2024" data-tags='["Code", "Large-Scale", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fan2024momentumgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fan2024momentumgs.jpg" data-fallback="None" alt="Paper thumbnail for Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.04887.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jixuan-fan.github.io/Momentum-GS_Page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Jixuan-Fan/Momentum-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://jixuan-fan.github.io/Momentum-GS_Page/static/videos/Momentum-GS_demo.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each block's weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: https://jixuan-fan.github.io/Momentum-GS_Page/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun2024sparse" data-title="Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering" data-authors="Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang" data-year="2024" data-tags='["Code", "Project", "Rendering", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun2024sparse', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun2024sparse.jpg" data-fallback="None" alt="Paper thumbnail for Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.04459.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://svraster.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NVlabs/svraster" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to render sparse voxels in the correct depth order along pixel rays by using dynamic Morton ordering. This avoids the well-known popping artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels to different levels of detail within scenes, faithfully reproducing scene details while achieving high rendering frame rates. Our method improves the previous neural-free voxel grid representation by over 4db PSNR and more than 10x rendering FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our neural-free sparse voxels are seamlessly compatible with grid-based 3D processing algorithms. We achieve promising mesh reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our sparse grid system.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2024feedforward" data-title="Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos" data-authors="Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang" data-year="2024" data-tags='["Dynamic", "Feed-Forward", "Monocular"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2024feedforward', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2024feedforward.jpg" data-fallback="None" alt="Paper thumbnail for Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Monocular</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.03526.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tan2024planarsplatting" data-title="PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes" data-authors="Bin Tan, Rui Yu, Yujun Shen, Nan Xue" data-year="2024" data-tags='["Acceleration", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tan2024planarsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tan2024planarsplatting.jpg" data-fallback="None" alt="Paper thumbnail for PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Bin Tan, Rui Yu, Yujun Shen, Nan Xue</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.03451.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://icetttb.github.io/PlanarSplatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction. Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy. Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method. We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future. The code of our CUDA implementation will be publicly available. Project page: https://icetttb.github.io/PlanarSplatting/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="schmidt2024nerf" data-title="NeRF and Gaussian Splatting SLAM in the Wild" data-authors="Fabian Schmidt, Markus Enzweiler, Abhinav Valada" data-year="2024" data-tags='["Code", "In the Wild", "Review", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'schmidt2024nerf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/schmidt2024nerf.jpg" data-fallback="None" alt="Paper thumbnail for NeRF and Gaussian Splatting SLAM in the Wild" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">NeRF and Gaussian Splatting SLAM in the Wild <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Fabian Schmidt, Markus Enzweiler, Abhinav Valada</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">In the Wild</span>
<span class="paper-tag">Review</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.03263.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/iis-esslingen/nerf-3dgs-benchmark" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Navigating outdoor environments with visual Simultaneous Localization and Mapping (SLAM) systems poses significant challenges due to dynamic scenes, lighting variations, and seasonal changes, requiring robust solutions. While traditional SLAM methods struggle with adaptability, deep learning-based approaches and emerging neural radiance fields as well as Gaussian Splatting-based SLAM methods, offer promising alternatives. However, these methods have primarily been evaluated in controlled indoor environments with stable conditions, leaving a gap in understanding their performance in unstructured and variable outdoor settings. This study addresses this gap by evaluating these methods in natural outdoor environments, focusing on camera tracking accuracy, robustness to environmental factors, and computational efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate that neural SLAM methods achieve superior robustness, particularly under challenging conditions such as low light, but at a high computational cost. At the same time, traditional methods perform the best across seasons but are highly sensitive to variations in lighting conditions. The code of the benchmark is publicly available at https://github.com/iis-esslingen/nerf-3dgs-benchmark.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="song2024hdgs" data-title="HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering" data-authors="Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis" data-year="2024" data-tags='["2DGS", "Antialiasing", "Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'song2024hdgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/song2024hdgs.jpg" data-fallback="None" alt="Paper thumbnail for HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.01823.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in neural rendering, particularly 2D Gaussian Splatting (2DGS), have shown promising results for jointly reconstructing fine appearance and geometry by leveraging 2D Gaussian surfels. However, current methods face significant challenges when rendering at arbitrary viewpoints, such as anti-aliasing for down-sampled rendering, and texture detail preservation for high-resolution rendering. We proposed a novel method to align the 2D surfels with texture maps and augment it with per-ray depth sorting and fisher-based pruning for rendering consistency and efficiency. With correct order, per-surfel texture maps significantly improve the capabilities to capture fine details. Additionally, to render high-fidelity details in varying viewpoints, we designed a frustum-based sampling method to mitigate the aliasing artifacts. Experimental results on benchmarks and our custom texture-rich dataset demonstrate that our method surpasses existing techniques, particularly in detail preservation and anti-aliasing.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="joanna2024occams" data-title="Occam's LGS: A Simple Approach for Language Gaussian Splatting" data-authors="Jiahuan (Joanna) Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel" data-year="2024" data-tags='["Acceleration", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'joanna2024occams', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/joanna2024occams.jpg" data-fallback="None" alt="Paper thumbnail for Occam's LGS: A Simple Approach for Language Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Occam's LGS: A Simple Approach for Language Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiahuan (Joanna) Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.01807" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://insait-institute.github.io/OccamLGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting is a widely adopted approach for 3D scene representation that offers efficient, high-quality 3D reconstruction and rendering. A major reason for the success of 3DGS is its simplicity of representing a scene with a set of Gaussians, which makes it easy to interpret and adapt. To enhance scene understanding beyond the visual representation, approaches have been developed that extend 3D Gaussian Splatting with semantic vision-language features, especially allowing for open-set tasks. In this setting, the language features of 3D Gaussian Splatting are often aggregated from multiple 2D views. Existing works address this aggregation problem using cumbersome techniques that lead to high computational cost and training time. In this work, we show that the sophisticated techniques for language-grounded 3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor to the task at hand and perform weighted multi-view feature aggregation using the weights derived from the standard rendering process, followed by a simple heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art results with a speed-up of two orders of magnitude. We showcase our results in two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach allows us to perform reasoning directly in the language features, without any compression whatsoever. Such modeling in turn offers easy scene manipulation, unlike the existing methods -- which we illustrate using an application of object insertion in the scene. Furthermore, we provide a thorough discussion regarding the significance of our contributions within the context of the current literature.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024dynsup" data-title="DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair" data-authors="Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li" data-year="2024" data-tags='["Dynamic", "Poses", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024dynsup', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024dynsup.jpg" data-fallback="None" alt="Paper thumbnail for DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.00851" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://colin-de.github.io/DynSUP/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in 3D Gaussian Splatting have shown promising results. Existing methods typically assume static scenes and/or multiple images with prior poses. Dynamics, sparse views, and unknown poses significantly increase the problem complexity due to insufficient geometric constraints. To overcome this challenge, we propose a method that can use only two images without prior poses to fit Gaussians in dynamic environments. To achieve this, we introduce two technical contributions. First, we propose an object-level two-view bundle adjustment. This strategy decomposes dynamic scenes into piece-wise rigid components, and jointly estimates the camera pose and motions of dynamic objects. Second, we design an SE(3) field-driven Gaussian training method. It enables fine-grained motion modeling through learnable per-Gaussian transformations. Our method leads to high-fidelity novel view synthesis of dynamic scenes while accurately preserving temporal consistency and object motion. Experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art approaches designed for the cases of static environments, multiple images, and/or known poses. Our project page is available at https://colin-de.github.io/DynSUP/.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hanson2024speedysplat" data-title="Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives" data-authors="Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein" data-year="2024" data-tags='["Acceleration", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hanson2024speedysplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hanson2024speedysplat.jpg" data-fallback="None" alt="Paper thumbnail for Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.00578.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS, achieving substantial improvements in rendering speed, model size, and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $6.71\times$ across scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets with $10.6\times$ fewer primitives than 3D-GS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="pryadilshchikov2024t3dgs" data-title="T-3DGS: Removing Transient Objects for 3D Scene Reconstruction" data-authors="Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev" data-year="2024" data-tags='["Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'pryadilshchikov2024t3dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/pryadilshchikov2024t3dgs.jpg" data-fallback="None" alt="Paper thumbnail for T-3DGS: Removing Transient Objects for 3D Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">T-3DGS: Removing Transient Objects for 3D Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Vadim Pryadilshchikov, Alexander Markin, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2412.00155" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://transient-3dgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Vadim200116/T-3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a novel framework to remove transient objects from input videos for 3D scene reconstruction using Gaussian Splatting. Our framework consists of the following steps. In the first step, we propose an unsupervised training strategy for a classification network to distinguish between transient objects and static scene parts based on their different training behavior inside the 3D Gaussian Splatting reconstruction. In the second step, we improve the boundary quality and stability of the detected transients by combining our results from the first step with an off-the-shelf segmentation method. We also propose a simple and effective strategy to track objects in the input video forward and backward in time. Our results show an improvement over the current state of the art in existing sparsely captured datasets and significant improvements in a newly proposed densely captured (video) dataset.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024gaussurf" data-title="GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction" data-authors="Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku Komura, Wenping Wang" data-year="2024" data-tags='["Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024gaussurf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024gaussurf.jpg" data-fallback="None" alt="Paper thumbnail for GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku Komura, Wenping Wang</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.19454" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jiepengwang.github.io/GausSurf/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has achieved impressive performance in novel view synthesis with real-time rendering capabilities. However, reconstructing high-quality surfaces with fine details using 3D Gaussians remains a challenging task. In this work, we introduce GausSurf, a novel approach to high-quality surface reconstruction by employing geometry guidance from multi-view consistency in texture-rich areas and normal priors in texture-less areas of a scene. We observe that a scene can be mainly divided into two primary regions: 1) texture-rich and 2) texture-less areas. To enforce multi-view consistency at texture-rich areas, we enhance the reconstruction quality by incorporating a traditional patch-match based Multi-View Stereo (MVS) approach to guide the geometry optimization in an iterative scheme. This scheme allows for mutual reinforcement between the optimization of Gaussians and patch-match refinement, which significantly improves the reconstruction results and accelerates the training process. Meanwhile, for the texture-less areas, we leverage normal priors from a pre-trained normal estimation model to guide optimization. Extensive experiments on the DTU and Tanks and Temples datasets demonstrate that our method surpasses state-of-the-art methods in terms of reconstruction quality and computation time.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024sadg" data-title="SADG: Segment Any Dynamic Gaussians Without Object Trackers" data-authors="Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers" data-year="2024" data-tags='["Dynamic", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024sadg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024sadg.jpg" data-fallback="None" alt="Paper thumbnail for SADG: Segment Any Dynamic Gaussians Without Object Trackers" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SADG: Segment Any Dynamic Gaussians Without Object Trackers <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yun-Jin Li, Mariia Gladkova, Yan Xia, Daniel Cremers</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.19290" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yunjinli.github.io/project-sadg/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://yunjinli.github.io/project-sadg/static/videos/split-cookie-2x4x.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Understanding dynamic 3D scenes is fundamental for various applications, including extended reality (XR) and autonomous driving. Effectively integrating semantic information into 3D reconstruction enables holistic representation that opens opportunities for immersive and interactive applications. We introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines dynamic Gaussian Splatting representation and semantic information without reliance on object IDs. In contrast to existing works, we do not rely on supervision based on object identities to enable consistent segmentation of dynamic 3D objects. To this end, we propose to learn semantically-aware features by leveraging masks generated from the Segment Anything Model (SAM) and utilizing our novel contrastive learning objective based on hard pixel mining. The learned Gaussian features can be effectively clustered without further post-processing. This enables fast computation for further object-level editing, such as object removal, composition, and style transfer by manipulating the Gaussians in the scene. We further extend several dynamic novel-view datasets with segmentation benchmarks to enable testing of learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks and demonstrate the superior performance of our approach in segmenting objects within dynamic scenes along with its effectiveness for further downstream editing tasks.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ren2024agsmesh" data-title="AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones" data-authors="Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, Esa Rahtu" data-year="2024" data-tags='["Code", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ren2024agsmesh', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ren2024agsmesh.jpg" data-fallback="None" alt="Paper thumbnail for AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors for Indoor Room Reconstruction Using Smartphones <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, Esa Rahtu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.19271" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xuqianren.github.io/ags_mesh_website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/XuqianRen/AGS_Mesh" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools. Our code is released in https://xuqianren.github.io/ags_mesh_website/.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wimmer2024gaussianstolife" data-title="Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes" data-authors="Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari" data-year="2024" data-tags='["Code", "Diffusion", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wimmer2024gaussianstolife', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wimmer2024gaussianstolife.jpg" data-fallback="None" alt="Paper thumbnail for Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.19233.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://wimmerth.github.io/gaussians2life.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/wimmerth/gaussians2life" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">State-of-the-art novel view synthesis methods achieve impressive results for multi-view captures of static 3D scenes. However, the reconstructed scenes still lack "liveliness," a key component for creating engaging 3D experiences. Recently, novel video diffusion models generate realistic videos with complex motion and enable animations of 2D images, however they cannot naively be used to animate 3D scenes as they lack multi-view consistency. To breathe life into the static world, we propose Gaussians2Life, a method for animating parts of high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is to leverage powerful video diffusion models as the generative component of our model and to combine these with a robust technique to lift 2D videos into meaningful 3D motion. We find that, in contrast to prior work, this enables realistic animations of complex, pre-existing 3D scenes and further enables the animation of a large variety of object classes, while related work is mostly focused on prior-based character animation, or single 3D objects. Our model enables the creation of consistent, immersive 3D experiences for arbitrary scenes.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024supergaussians" data-title="SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors" data-authors="Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing Xin, Taku Komura, Xin Li, Wenping Wang" data-year="2024" data-tags='["Code", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024supergaussians', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024supergaussians.jpg" data-fallback="None" alt="Paper thumbnail for SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SuperGaussians: Enhancing Gaussian Splatting Using Primitives with Spatially Varying Colors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing Xin, Taku Komura, Xin Li, Wenping Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.18966" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ruixu.me/html/SuperGaussians/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Xrvitd/SuperGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=K0liSjZGnIY&feature=youtu.be" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splattings demonstrate impressive results in multi-view reconstruction based on Gaussian explicit representations. However, the current Gaussian primitives only have a single view-dependent color and an opacity to represent the appearance and geometry of the scene, resulting in a non-compact representation. In this paper, we introduce a new method called SuperGaussians that utilizes spatially varying colors and opacity in a single Gaussian primitive to improve its representation ability. We have implemented bilinear interpolation, movable kernels, and even tiny neural networks as spatially varying functions. Quantitative and qualitative experimental results demonstrate that all three functions outperform the baseline, with the best movable kernels achieving superior novel view synthesis performance on multiple datasets, highlighting the strong potential of spatially varying functions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chao2024textured" data-title="Textured Gaussians for Enhanced 3D Scene Appearance Modeling" data-authors="Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim" data-year="2024" data-tags='["In the Wild", "Project", "Rendering", "Texturing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chao2024textured', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chao2024textured.jpg" data-fallback="None" alt="Paper thumbnail for Textured Gaussians for Enhanced 3D Scene Appearance Modeling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Textured Gaussians for Enhanced 3D Scene Appearance Modeling <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim</p>
      <div class="paper-tags"><span class="paper-tag">In the Wild</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Texturing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.18625.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://textured-gaussians.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024cat4d" data-title="CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models" data-authors="Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski" data-year="2024" data-tags='["Diffusion", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024cat4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024cat4d.jpg" data-fallback="None" alt="Paper thumbnail for CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.18613.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://cat-4d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: https://cat-4d.github.io/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kang2024selfsplat" data-title="SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting" data-authors="Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, Eunbyung Park" data-year="2024" data-tags='["Code", "Feed-Forward", "Poses", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kang2024selfsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kang2024selfsplat.jpg" data-fallback="None" alt="Paper thumbnail for SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, Eunbyung Park</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.17190.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gynjn.github.io/selfsplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Gynjn/selfsplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at https://gynjn.github.io/selfsplat/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="flynn2024quark" data-title="Quark: Real-time, High-resolution, and General Neural View Synthesis" data-authors="John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck" data-year="2024" data-tags='["Feed-Forward", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'flynn2024quark', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/flynn2024quark.jpg" data-fallback="None" alt="Paper thumbnail for Quark: Real-time, High-resolution, and General Neural View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Quark: Real-time, High-resolution, and General Neural View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck</p>
      <div class="paper-tags"><span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.16680.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://arxiv.org/abs/2411.16680" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/w9Oqhaqbsyo?si=ochKsH7fuxgM5BFg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a novel neural algorithm for performing high-quality, high-resolution, real-time novel view synthesis. From a sparse set of input RGB images or videos streams, our network both reconstructs the 3D scene and renders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our feed-forward network generalizes across a wide variety of datasets and scenes and produces state-of-the-art quality for a real-time method. Our quality approaches, and in some cases surpasses, the quality of some of the top offline methods. In order to achieve these results we use a novel combination of several key concepts, and tie them together into a cohesive and effective algorithm. We build on previous works that represent the scene using semi-transparent layers and use an iterative learned render-and-refine approach to improve those layers. Instead of flat layers, our method reconstructs layered depth maps (LDMs) that efficiently represent scenes with complex depth and occlusions. The iterative update steps are embedded in a multi-scale, UNet-style architecture to perform as much compute as possible at reduced resolution. Within each update step, to better aggregate the information from multiple input views, we use a specialized Transformer-based network component. This allows the majority of the per-input image processing to be performed in the input image space, as opposed to layer space, further increasing efficiency. Finally, due to the real-time nature of our reconstruction and rendering, we dynamically create and discard the internal 3D geometry for each frame, generating the LDM for each view. Taken together, this produces a novel and effective algorithm for view synthesis. Through extensive evaluation, we demonstrate that we achieve state-of-the-art quality at real-time rates. Project page: https://quark-3d.github.io/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hess2024splatad" data-title="SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving" data-authors="Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson" data-year="2024" data-tags='["Autonomous Driving", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hess2024splatad', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hess2024splatad.jpg" data-fallback="None" alt="Paper thumbnail for SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.16816" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://research.zenseact.com/publications/splatad/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024quadratic" data-title="Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction" data-authors="Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen" data-year="2024" data-tags='["Code", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024quadratic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024quadratic.jpg" data-fallback="None" alt="Paper thumbnail for Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Quadratic Gaussian Splatting for Efficient and Detailed Surface Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.16392.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://quadraticgs.github.io/QGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/QuadraticGS/QGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://quadraticgs.github.io/QGS/assets/QGS_web.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its superior rendering quality and speed over Neural Radiance Fields (NeRF). To address 3DGS's limitations in surface representation, 2D Gaussian Splatting (2DGS) introduced disks as scene primitives to model and reconstruct geometries from multi-view images, offering view-consistent geometry. However, the disk's first-order linear approximation often leads to over-smoothed results. We propose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks with quadric surfaces, enhancing geometric fitting, whose code will be open-sourced. QGS defines Gaussian distributions in non-Euclidean space, allowing primitives to capture more complex textures. As a second-order surface approximation, QGS also renders spatial curvature to guide the normal consistency term, to effectively reduce over-smoothing. Moreover, QGS is a generalized version of 2DGS that achieves more accurate and detailed reconstructions, as verified by experiments on DTU and TNT, demonstrating its effectiveness in surpassing current state-of-the-art methods in geometry reconstruction. Our code willbe released as open source.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yugay2024magicslam" data-title="MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM" data-authors="Vladimir Yugay, Theo Gevers, Martin R. Oswald" data-year="2025" data-tags='["Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yugay2024magicslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yugay2024magicslam.jpg" data-fallback="None" alt="Paper thumbnail for MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM <span class="paper-year">(2025)</span></h2>
      <p class="paper-authors">Vladimir Yugay, Theo Gevers, Martin R. Oswald</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.16785.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vladimiryugay.github.io/magic_slam/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=e0WjMK72LEg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Simultaneous localization and mapping (SLAM) systems with novel view synthesis capabilities are widely used in computer vision, with applications in augmented reality, robotics, and autonomous driving. However, existing approaches are limited to single-agent operation. Recent work has addressed this problem using a distributed neural scene representation. Unfortunately, existing methods are slow, cannot accurately render real-world data, are restricted to two agents, and have limited tracking accuracy. In contrast, we propose a rigidly deformable 3D Gaussian-based scene representation that dramatically speeds up the system. However, improving tracking accuracy and reconstructing a globally consistent map from multiple agents remains challenging due to trajectory drift and discrepancies across agents' observations. Therefore, we propose new tracking and map-merging mechanisms and integrate loop closure in the Gaussian-based SLAM pipeline. We evaluate MAGiC-SLAM on synthetic and real-world datasets and find it more accurate and faster than the state of the art.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024fate" data-title="FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video" data-authors="Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu" data-year="2024" data-tags='["Avatar", "Code", "Dynamic", "Editing", "Monocular", "Project", "Texturing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024fate', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024fate.jpg" data-fallback="None" alt="Paper thumbnail for FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiawei Zhang, Zijian Wu, Zhiyang Liang, Yicheng Gong, Dongfang Hu, Yao Yao, Xun Cao, Hao Zhu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Texturing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.15604.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zjwfufu.github.io/FATE-page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zjwfufu/FateAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing high-fidelity, animatable 3D head avatars from effortlessly captured monocular videos is a pivotal yet formidable challenge. Although significant progress has been made in rendering performance and manipulation capabilities, notable challenges remain, including incomplete reconstruction and inefficient Gaussian representation. To address these challenges, we introduce FATE, a novel method for reconstructing an editable full-head avatar from a single monocular video. FATE integrates a sampling-based densification strategy to ensure optimal positional distribution of points, improving rendering efficiency. A neural baking technique is introduced to convert discrete Gaussian representations into continuous attribute maps, facilitating intuitive appearance editing. Furthermore, we propose a universal completion framework to recover non-frontal appearance, culminating in a 360$^\circ$-renderable 3D head avatar. FATE outperforms previous approaches in both qualitative and quantitative evaluations, achieving state-of-the-art performance. To the best of our knowledge, FATE is the first animatable and 360$^\circ$ full-head monocular reconstruction method for a 3D head avatar. The code will be publicly released upon publication.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="blark2024splatsdf" data-title="SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion" data-authors="Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen" data-year="2024" data-tags='["Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'blark2024splatsdf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/blark2024splatsdf.jpg" data-fallback="None" alt="Paper thumbnail for SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.15468.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://blarklee.github.io/splatsdf/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">A signed distance function (SDF) is a useful representation for continuous-space geometry and many related operations, including rendering, collision checking, and mesh generation. Hence, reconstructing SDF from image observations accurately and efficiently is a fundamental problem. Recently, neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering, have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF enables continuous-space SDF reconstruction with better geometric and photometric accuracy. However, the accuracy and convergence speed of scene-level SDF reconstruction require further improvements for many applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit representation with excellent rendering quality and speed, several works have focused on improving SDF-NeRF by introducing consistency losses on depth and surface normals between 3DGS and SDF-NeRF. However, loss-level connections alone lead to incremental improvements. We propose a novel neural implicit SDF called “SplatSDF” to fuse 3DGS and SDF-NeRF at an architecture level with significant boosts to geometric and photometric accuracy and convergence speed. Our SplatSDF relies on 3DGS as input only during training, and keeps the same complexity and efficiency as the original SDF-NeRF during inference. Our method outperforms state-of-the-art SDF-NeRF models on geometric and photometric evaluation by the time of submission.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chou2024generating" data-title="Generating 3D-Consistent Videos from Unposed Internet Photos" data-authors="Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, Noah Snavely" data-year="2024" data-tags='["Feed-Forward", "In the Wild", "Poses", "Project", "Transformer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chou2024generating', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chou2024generating.jpg" data-fallback="None" alt="Paper thumbnail for Generating 3D-Consistent Videos from Unposed Internet Photos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Generating 3D-Consistent Videos from Unposed Internet Photos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, Noah Snavely</p>
      <div class="paper-tags"><span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">In the Wild</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Transformer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.13549.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://genechou.com/kfcw/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We address the problem of generating videos from unposed internet photos. A handful of input images serve as keyframes, and our model interpolates between them to simulate a path moving between the cameras. Given random images, a model's ability to capture underlying geometry, recognize scene identity, and relate frames in terms of camera position and orientation reflects a fundamental understanding of 3D structure and scene layout. However, existing video models such as Luma Dream Machine fail at this task. We design a self-supervised method that takes advantage of the consistency of videos and variability of multiview internet photos to train a scalable, 3D-aware video model without any 3D annotations such as camera parameters. We validate that our method outperforms all baselines in terms of geometric and appearance consistency. We also show our model benefits applications that enable camera control, such as 3D Gaussian Splatting. Our results suggest that we can scale up scene-level 3D learning using only 2D data such as videos and multiview internet photos.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="joseph2024gradientweighted" data-title="Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting" data-authors="Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar" data-year="2024" data-tags='["Code", "Editing", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'joseph2024gradientweighted', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/joseph2024gradientweighted.jpg" data-fallback="None" alt="Paper thumbnail for Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.15193.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jojijoseph.github.io/3dgs-backprojection/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JojiJoseph/3dgs-gradient-backprojection" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce a training-free method for feature field rendering in Gaussian splatting. Our approach back-projects 2D features into pre-trained 3D Gaussians, using a weighted sum based on each Gaussian's influence in the final rendering. While most training-based feature field rendering methods excel at 2D segmentation but perform poorly at 3D segmentation without post-processing, our method achieves high-quality results in both 2D and 3D segmentation. Experimental results demonstrate that our approach is fast, scalable, and offers performance comparable to training-based methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fang2024minisplatting2" data-title="Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification" data-authors="Guangchi Fang, Bing Wang" data-year="2024" data-tags='["Acceleration", "Code", "Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fang2024minisplatting2', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fang2024minisplatting2.jpg" data-fallback="None" alt="Paper thumbnail for Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guangchi Fang, Bing Wang</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.12788.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/fatPeter/mini-splatting2" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this study, we explore the essential challenge of fast scene optimization for Gaussian Splatting. Through a thorough analysis of the geometry modeling process, we reveal that dense point clouds can be effectively reconstructed early in optimization through Gaussian representations. This insight leads to our approach of aggressive Gaussian densification, which provides a more efficient alternative to conventional progressive densification methods. By significantly increasing the number of critical Gaussians, we enhance the model capacity to capture dense scene geometry at the early stage of optimization. This strategy is seamlessly integrated into the Mini-Splatting densification and simplification framework, enabling rapid convergence without compromising quality. Additionally, we introduce visibility culling within Gaussian Splatting, leveraging per-view Gaussian importance as precomputed visibility to accelerate the optimization process. Our Mini-Splatting2 achieves a balanced trade-off among optimization time, the number of Gaussians, and rendering quality, establishing a strong baseline for future Gaussian-Splatting-based works. Our work sets the stage for more efficient, high-quality 3D scene modeling in real-world applications, and the code will be made available no matter acceptance.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2024gpsgaussian" data-title="GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views" data-authors="Boyao Zhou, Shunyuan Zheng, Hanzhang Tu, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu" data-year="2024" data-tags='["Acceleration", "Dynamic", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2024gpsgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2024gpsgaussian.jpg" data-fallback="None" alt="Paper thumbnail for GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Boyao Zhou, Shunyuan Zheng, Hanzhang Tu, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.11363.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yaourtb.github.io/GPS-Gaussian+" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Differentiable rendering techniques have recently shown promising results for free-viewpoint video synthesis of characters. However, such methods, either Gaussian Splatting or neural implicit rendering, typically necessitate per-subject optimization which does not meet the requirement of real-time rendering in an interactive application. We propose a generalizable Gaussian Splatting approach for high-resolution image rendering under a sparse-view camera setting. To this end, we introduce Gaussian parameter maps defined on the source views and directly regress Gaussian properties for instant novel view synthesis without any fine-tuning or optimization. We train our Gaussian parameter regression module on human-only data or human-scene data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable with both depth and rendering supervision or with only rendering supervision. We further introduce a regularization term and an epipolar attention mechanism to preserve geometry consistency between two source views, especially when neglecting depth supervision. Experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kong2024dgsslam" data-title="DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment" data-authors="Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim" data-year="2024" data-tags='["SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kong2024dgsslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kong2024dgsslam.jpg" data-fallback="None" alt="Paper thumbnail for DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.10722" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://youtu.be/Mq3qZTTcN3E?feature=shared" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic SLAM framework built on the foundation of Gaussian Splatting. While recent advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene representation, most approaches assume a static environment, making them vulnerable to photometric and geometric inconsistencies caused by dynamic objects. To address these challenges, we integrate Gaussian Splatting SLAM with a robust filtering process to handle dynamic objects throughout the entire pipeline, including Gaussian insertion and keyframe selection. Within this framework, to further improve the accuracy of dynamic object removal, we introduce a robust mask generation method that enforces photometric consistency across keyframes, reducing noise from inaccurate segmentation and artifacts such as shadows. Additionally, we propose the loop-aware window selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops between the current and past frames, facilitating joint optimization of the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art performance in both camera tracking and novel view synthesis on various dynamic SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tang2024spars3r" data-title="SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction" data-authors="Yutao Tang, Yuxiang Guo, Deming Li, Cheng Peng" data-year="2024" data-tags='["3ster-based", "Poses", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tang2024spars3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tang2024spars3r.jpg" data-fallback="None" alt="Paper thumbnail for SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yutao Tang, Yuxiang Guo, Deming Li, Cheng Peng</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.12592.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve photorealistic rendering; however, such capability is limited in sparse-view scenarios due to sparse initialization and over-fitting floaters. Recent progress in depth estimation and alignment can provide dense point cloud with few views; however, the resulting pose accuracy is suboptimal. In this work, we present SPARS3R, which combines the advantages of accurate pose estimation from Structure-from-Motion and dense point cloud from depth estimation. To this end, SPARS3R first performs a Global Fusion Alignment process that maps a prior dense point cloud to a sparse point cloud from Structure-from-Motion based on triangulated correspondences. RANSAC is applied during this process to distinguish inliers and outliers. SPARS3R then performs a second, Semantic Outlier Alignment step, which extracts semantically coherent regions around the outliers and performs local alignment in these regions. Along with several improvements in the evaluation process, we demonstrate that SPARS3R can achieve photorealistic rendering with sparse images and significantly outperforms existing approaches.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="svitov2024billboard" data-title="BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis" data-authors="David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue" data-year="2024" data-tags='["Code", "Optimization", "Project", "Texturing", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'svitov2024billboard', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/svitov2024billboard.jpg" data-fallback="None" alt="Paper thumbnail for BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Optimization</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Texturing</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.08508.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://david-svitov.github.io/BBSplat_project_page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/david-svitov/BBSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/uRM7WFo5vVg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present billboard Splatting (BBSplat) - a novel approach for 3D scene representation based on textured geometric primitives. BBSplat represents the scene as a set of optimizable textured planar primitives with learnable RGB textures and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our method's qualitative and quantitative improvements over 3D and 2D Gaussians are most noticeable when fewer primitives are used, when BBSplat achieves over 1200 FPS. Our novel regularization term encourages textures to have a sparser structure, unlocking an efficient compression that leads to a reduction in storage space of the model. Our experiments show the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU, and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics compared to the state-of-the-art, especially for the case when fewer primitives are used, which, on the other hand, leads to up to 2 times inference speed improvement for the same rendering quality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024mbaslam" data-title="MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation" data-authors="Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu" data-year="2024" data-tags='["Project", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024mbaslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024mbaslam.jpg" data-fallback="None" alt="Paper thumbnail for MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.08279" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://wangpeng000.github.io/MBA-SLAM/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras' local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024gaussianspa" data-title="GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting" data-authors="Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin" data-year="2024" data-tags='["Compression", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024gaussianspa', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024gaussianspa.jpg" data-fallback="None" alt="Paper thumbnail for GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.06019.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussianspa.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient "optimizing-sparsifying" solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://gaussianspa.github.io/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lu20243dgscd" data-title="3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement" data-authors="Ziqi Lu, Jianbo Ye, John Leonard" data-year="2024" data-tags='["Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lu20243dgscd', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lu20243dgscd.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ziqi Lu, Jianbo Ye, John Leonard</p>
      <div class="paper-tags"><span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.03706" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS's novel view rendering and EfficientSAM's zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models -- An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024citygaussianv2" data-title="CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes" data-authors="Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang" data-year="2024" data-tags='["Code", "Large-Scale", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024citygaussianv2', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024citygaussianv2.jpg" data-fallback="None" alt="Paper thumbnail for CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2411.00771" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dekuliutesla.github.io/CityGaussianV2/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/DekuLiuTesla/CityGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10x compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zubair2024neural" data-title="Neural Fields in Robotics: A Survey" data-authors="Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Zsolt Kira, Rares Ambrus, Johnathan Trembley" data-year="2024" data-tags='["Review", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zubair2024neural', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zubair2024neural.jpg" data-fallback="None" alt="Paper thumbnail for Neural Fields in Robotics: A Survey" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Neural Fields in Robotics: A Survey <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Zsolt Kira, Rares Ambrus, Johnathan Trembley</p>
      <div class="paper-tags"><span class="paper-tag">Review</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.20220" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2024diffgs" data-title="DiffGS: Functional Gaussian Splatting Diffusion" data-authors="Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2024diffgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2024diffgs.jpg" data-fallback="None" alt="Paper thumbnail for DiffGS: Functional Gaussian Splatting Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DiffGS: Functional Gaussian Splatting Diffusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.19657.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://junshengzhou.github.io/DiffGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/weiqi-zhang/DiffGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fan2024large" data-title="Large Spatial Model: End-to-end Unposed Images to Semantic 3D" data-authors="Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang" data-year="2024" data-tags='["Code", "Feed-Forward", "Poses", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fan2024large', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fan2024large.jpg" data-fallback="None" alt="Paper thumbnail for Large Spatial Model: End-to-end Unposed Images to Semantic 3D" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Large Spatial Model: End-to-end Unposed Images to Semantic 3D <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.18956.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://largespatialmodel.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NVlabs/LSM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://largespatialmodel.github.io/static/videos/LSM_demo1.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing and understanding 3D structures from a limited number of images is a well-established problem in computer vision. Traditional methods usually break this task into multiple subtasks, each requiring complex transformations between different data representations. For instance, dense reconstruction through Structure-from-Motion (SfM) involves converting images into key points, optimizing camera parameters, and estimating structures. Afterward, accurate sparse reconstructions are required for further dense modeling, which is subsequently fed into task-specific neural networks. This multi-step process results in considerable processing time and increased engineering complexity. In this work, we present the Large Spatial Model (LSM), which processes unposed RGB images directly into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward operation, and it can generate versatile label maps by interacting with language at novel viewpoints. Leveraging a Transformer-based architecture, LSM integrates global geometry through pixel-aligned point maps. To enhance spatial attribute regression, we incorporate local context aggregation with multi-scale fusion, improving the accuracy of fine local details. To tackle the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder then parameterizes a set of semantic anisotropic Gaussians, facilitating supervised end-to-end learning. Extensive experiments across various tasks show that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hou2024sortfree" data-title="Sort-free Gaussian Splatting via Weighted Sum Rendering" data-authors="Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said" data-year="2024" data-tags='["Acceleration", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hou2024sortfree', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hou2024sortfree.jpg" data-fallback="None" alt="Paper thumbnail for Sort-free Gaussian Splatting via Weighted Sum Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Sort-free Gaussian Splatting via Weighted Sum Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.18931.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering performance is constrained by its dependence on costly non-commutative alpha-blending operations. These operations mandate complex view dependent sorting operations that introduce computational overhead, especially on the resource-constrained platforms such as mobile phones. In this paper, we propose Weighted Sum Rendering, which approximates alpha blending with weighted sums, thereby removing the need for sorting. This simplifies implementation, delivers superior performance, and eliminates the "popping" artifacts caused by sorting. Experimental results show that optimizing a generalized Gaussian splatting formulation to the new differentiable rendering yields competitive image quality. The method was implemented and tested in a mobile device GPU, achieving on average $1.23\times$ faster rendering.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2024fully" data-title="Fully Explicit Dynamic Gaussian Splatting" data-authors="Junoh Lee, Changyeon Won, HyunJun Jung, Inhwan Bae, Hae-Gon Jeon" data-year="2024" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2024fully', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2024fully.jpg" data-fallback="None" alt="Paper thumbnail for Fully Explicit Dynamic Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Fully Explicit Dynamic Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junoh Lee, Changyeon Won, HyunJun Jung, Inhwan Bae, Hae-Gon Jeon</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.15629" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://leejunoh.com/Ex4DGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/juno181/Ex4DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design a \Edited{Explicit 4D Gaussian Splatting(Ex4DGS)}. Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hahlbohm2024efficient" data-title="Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency" data-authors="Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor" data-year="2024" data-tags='["Perspective-correct", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hahlbohm2024efficient', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hahlbohm2024efficient.jpg" data-fallback="None" alt="Paper thumbnail for Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor</p>
      <div class="paper-tags"><span class="paper-tag">Perspective-correct</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.08129.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fhahlbohm.github.io/htgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://fhahlbohm.github.io/htgs/assets/htgs_twitter_teaser.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both for inverse rendering as well as real-time exploration of scenes. In these applications, coherence across camera frames and multiple views is crucial, be it for robust convergence of a scene reconstruction or for artifact-free fly-throughs. Recent work started mitigating artifacts that break multi-view coherence, including popping artifacts due to inconsistent transparency sorting and perspective-correct outlines of (2D) splats. At the same time, real-time requirements forced such implementations to accept compromises in how transparency of large assemblies of 3D Gaussians is resolved, in turn breaking coherence in other ways. In our work, we aim at achieving maximum coherence, by rendering fully perspective-correct 3D Gaussians while using a high-quality approximation of accurate blending, hybrid transparency, on a per-pixel level, in order to retain real-time frame rates. Our fast and perspectively accurate approach for evaluation of 3D Gaussians does not require matrix inversions, thereby ensuring numerical stability and eliminating the need for special handling of degenerate splats, and the hybrid transparency formulation for blending maintains similar quality as fully resolved per-pixel transparencies at a fraction of the rendering costs. We further show that each of these two components can be independently integrated into Gaussian splatting systems. In combination, they achieve up to 2$\times$ higher frame rates, 2$\times$ faster optimization, and equal or better image quality with fewer rendering artifacts compared to traditional 3DGS on common benchmarks.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chu2024generalizable" data-title="Generalizable and Animatable Gaussian Head Avatar" data-authors="Xuangeng Chu, Tatsuya Harada" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chu2024generalizable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chu2024generalizable.jpg" data-fallback="None" alt="Paper thumbnail for Generalizable and Animatable Gaussian Head Avatar" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Generalizable and Animatable Gaussian Head Avatar <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xuangeng Chu, Tatsuya Harada</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.07971" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xg-chu.site/project_gagavatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/xg-chu/GAGAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/9244ZgOl4Xk" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture identity and facial details. Additionally, we leverage global image features and the 3D morphable model to construct 3D Gaussians for controlling expressions. After training, our model can reconstruct unseen identities without specific optimizations and perform reenactment rendering at real-time speeds. Experiments show that our method exhibits superior performance compared to previous methods in terms of reconstruction quality and expression accuracy. We believe our method can establish new benchmarks for future research and advance applications of digital avatars.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2024motiongs" data-title="MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting" data-authors="Ruijie Zhu*, Yanzhe Liang*, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang" data-year="2024" data-tags='["Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2024motiongs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2024motiongs.jpg" data-fallback="None" alt="Paper thumbnail for MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ruijie Zhu*, Yanzhe Liang*, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.07707" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ruijiezhu94.github.io/MotionGS_page" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/25DgViuuKFI" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tang2024hisplat" data-title="HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction" data-authors="Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang" data-year="2024" data-tags='["Code", "Feed-Forward", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tang2024hisplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tang2024hisplat.jpg" data-fallback="None" alt="Paper thumbnail for HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.06245.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://open3dvlab.github.io/HiSplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Open3DVLab/HiSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024splatraj" data-title="SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting" data-authors="Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi" data-year="2024" data-tags='["Language Embedding", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024splatraj', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024splatraj.jpg" data-fallback="None" alt="Paper thumbnail for SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi</p>
      <div class="paper-tags"><span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.06014" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://youtu.be/PUXNBfpeZkg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024monst3r" data-title="MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion" data-authors="Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang" data-year="2024" data-tags='["3ster-based", "Code", "Dynamic", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024monst3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024monst3r.jpg" data-fallback="None" alt="Paper thumbnail for MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.03825.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://monst3r-project.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Junyi42/monst3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://monst3r-project.github.io/files/teaser_vid_v2_lowres.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024gigs" data-title="GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering" data-authors="Hongze Chen, Zehong Lin, Jun Zhang" data-year="2024" data-tags='["Code", "Project", "Ray Tracing", "Relight"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024gigs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024gigs.jpg" data-fallback="None" alt="Paper thumbnail for GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hongze Chen, Zehong Lin, Jun Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Relight</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.02619.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://stopaimme.github.io/GI-GS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/stopaimme/GI-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GI-GS, a novel inverse rendering framework that leverages 3D Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel view synthesis and relighting. In inverse rendering, accurately modeling the shading processes of objects is essential for achieving high-fidelity results. Therefore, it is critical to incorporate global illumination to account for indirect lighting that reaches an object after multiple bounces across the scene. Previous 3DGS-based methods have attempted to model indirect lighting by characterizing indirect illumination as learnable lighting volumes or additional attributes of each Gaussian, while using baked occlusion to represent shadow effects. These methods, however, fail to accurately model the complex physical interactions between light and objects, making it impossible to construct realistic indirect illumination during relighting. To address this limitation, we propose to calculate indirect lighting using efficient path tracing with deferred shading. In our framework, we first render a G-buffer to capture the detailed geometry and material properties of the scene. Then, we perform physically-based rendering (PBR) only for direct lighting. With the G-buffer and previous rendering results, the indirect lighting can be calculated through a lightweight path tracing. Our method effectively models indirect lighting under any given lighting conditions, thereby achieving better novel view synthesis and relighting. Quantitative and qualitative results show that our GI-GS outperforms existing baselines in both rendering quality and efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xie2024supergs" data-title="SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field and Gradient-guided Splitting" data-authors="Shiyun Xie, Zhiru Wang, Yinghao Zhu, Chengwei Pan" data-year="2024" data-tags='["Densification", "Feed-Forward", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xie2024supergs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xie2024supergs.jpg" data-fallback="None" alt="Paper thumbnail for SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field and Gradient-guided Splitting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field and Gradient-guided Splitting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shiyun Xie, Zhiru Wang, Yinghao Zhu, Chengwei Pan</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.02571v1.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis with its real-time rendering capabilities and superior quality. However, it faces challenges for high-resolution novel view synthesis (HRNVS) due to the coarse nature of primitives derived from low-resolution input views. To address this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion of 3DGS designed with a two-stage coarse-to-fine training framework, utilizing pretrained low-resolution scene representation as an initialization for super-resolution optimization. Moreover, we introduce Multi-resolution Feature Gaussian Splatting (MFGS) to incorporates a latent feature field for flexible feature sampling and Gradient-guided Selective Splitting (GSS) for effective Gaussian upsampling. By integrating these strategies within the coarse-to-fine framework ensure both high fidelity and memory efficiency. Extensive experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods on challenging real-world datasets using only low-resolution inputs.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="mai2024ever" data-title="EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis" data-authors="Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan T. Barron, Yinda Zhang" data-year="2024" data-tags='["Code", "Project", "Ray Tracing", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'mai2024ever', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/mai2024ever.jpg" data-fallback="None" alt="Paper thumbnail for EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan T. Barron, Yinda Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.01804.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://half-potato.gitlab.io/posts/ever/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/half-potato/ever_training" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/dqLi2-v38LE" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time differentiable emission-only volume rendering. Unlike recent rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive based representation allows for exact volume rendering, rather than alpha compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does not suffer from popping artifacts and view dependent density, but still achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our approach is built upon ray tracing it enables effects such as defocus blur and camera distortion (e.g. such as from fisheye cameras), which are difficult to achieve by rasterization. We show that our method is more accurate with fewer blending issues than 3DGS and follow-up work on view-consistent rendering, especially on the challenging large-scale scenes from the Zip-NeRF dataset where it achieves sharpest results among real-time techniques.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cao20243dgsdet" data-title="3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection" data-authors="Yang Cao, Yuanliang Jv, Dan Xu" data-year="2024" data-tags='["Object Detection"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cao20243dgsdet', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cao20243dgsdet.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yang Cao, Yuanliang Jv, Dan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Object Detection</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2410.01647" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3D object detection through view-synthesis representation. However, NeRF faces inherent limitations: (i) It has limited representational capacity for 3DOD due to its implicit nature, and (ii) it suffers from slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations with faster rendering capabilities. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs – 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs – 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background (see Fig. 1). To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from the proposed Boundary Guidance and Box-Focused Sampling, our final method, 3DGS-DET, achieves significant improvements (+5.6 on mAP@0.25, +3.7 on mAP@0.5) over our basic pipeline version, without introducing any additional learnable parameters. Furthermore, 3DGS-DET significantly outperforms the state-of-the-art NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset. Codes and models are publicly available at: https://github.com/yangcaoai/3DGS-DET.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="duisterhof2024mast3rsfm" data-title="MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion" data-authors="Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, Jerome Revaud" data-year="2024" data-tags='["Code"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'duisterhof2024mast3rsfm', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/duisterhof2024mast3rsfm.jpg" data-fallback="None" alt="Paper thumbnail for MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, Jerome Revaud</p>
      <div class="paper-tags"><span class="paper-tag">Code</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.19152.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/naver/mast3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Structure-from-Motion (SfM), a task aiming at jointly recovering camera poses and 3D geometry of a scene given a set of images, remains a hard problem with still many open challenges despite decades of significant progress. The traditional solution for SfM consists of a complex pipeline of minimal solvers which tends to propagate errors and fails when images do not sufficiently overlap, have too little motion, etc. Recent methods have attempted to revisit this paradigm, but we empirically show that they fall short of fixing these core issues. In this paper, we propose instead to build upon a recently released foundation model for 3D vision that can robustly produce local 3D reconstructions and accurate matches. We introduce a low-memory approach to accurately align these local reconstructions in a global coordinate system. We further show that such foundation models can serve as efficient image retrievers without any overhead, reducing the overall complexity from quadratic to linear. Overall, our novel SfM pipeline is simple, scalable, fast and truly unconstrained, i.e. it can handle any collection of images, ordered or not. Extensive experiments on multiple benchmarks show that our method provides steady performance across diverse settings, especially outperforming existing methods in small- and medium-scale settings.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2024languageembedded" data-title="Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot" data-authors="Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Robotics", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2024languageembedded', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2024languageembedded.jpg" data-fallback="None" alt="Paper thumbnail for Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Language-Embedded Gaussian Splats (LEGS): Incrementally Building Room-Scale Representations with a Mobile Robot <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.18108" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://berkeleyautomation.github.io/LEGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/BerkeleyAutomation/L3GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Building semantic 3D maps is valuable for searching for objects of interest in offices, warehouses, stores, and homes. We present a mapping system that incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D scene representation that encodes both appearance and semantics in a unified representation. LEGS is trained online as a robot traverses its environment to enable localization of open-vocabulary object queries. We evaluate LEGS on 4 room-scale scenes where we query for objects in the scene to assess how LEGS can capture semantic meaning. We compare LEGS to LERF and find that while both systems have comparable object query success rates, LEGS trains over 3.5x faster than LERF. Results suggest that a multi-camera setup and incremental bundle adjustment can boost visual reconstruction quality in constrained robot trajectories, and suggest LEGS can localize open-vocabulary and long-tail object queries with up to 66% accuracy.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024v3" data-title="V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians" data-authors="Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, Lan Xu" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024v3', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024v3.jpg" data-fallback="None" alt="Paper thumbnail for V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, Lan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.13648" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://authoritywang.github.io/v3/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/AuthorityWang/VideoGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/Z5La9AporRU?si=iJ-m_mvUSxQN4Bwm" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V^3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at this https URL.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chelani2024edgegaussians" data-title="EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting" data-authors="Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl" data-year="2024" data-tags='["Code", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chelani2024edgegaussians', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chelani2024edgegaussians.jpg" data-fallback="None" alt="Paper thumbnail for EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.12886.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/kunalchelani/EdgeGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">With their meaningful geometry and their omnipresence in the 3D world, edges are extremely useful primitives in computer vision. 3D edges comprise of lines and curves, and methods to reconstruct them use either multi-view images or point clouds as input. State-of-the-art image-based methods first learn a 3D edge point cloud then fit 3D edges to it. The edge point cloud is obtained by learning a 3D neural implicit edge field from which the 3D edge points are sampled on a specific level set (0 or 1). However, such methods present two important drawbacks: i) it is not realistic to sample points on exact level sets due to float imprecision and training inaccuracies. Instead, they are sampled within a range of levels so the points do not lie accurately on the 3D edges and require further processing. ii) Such implicit representations are computationally expensive and require long training times. In this paper, we address these two limitations and propose a 3D edge mapping that is simpler, more efficient, and preserves accuracy. Our method learns explicitly the 3D edge points and their edge direction hence bypassing the need for point sampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge direction as the principal axis of the Gaussian. Such a representation has the advantage of being not only geometrically meaningful but also compatible with the efficient training optimization defined in Gaussian Splatting. Results show that the proposed method produces edges as accurate and complete as the state-of-the-art while being an order of magnitude faster.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="joseph2024gradientdriven" data-title="Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks" data-authors="Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar" data-year="2024" data-tags='["Code", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'joseph2024gradientdriven', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/joseph2024gradientdriven.jpg" data-fallback="None" alt="Paper thumbnail for Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.11681" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jojijoseph.github.io/3dgs-segmentation/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JojiJoseph/3dgs-gradient-segmentation" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we introduce a novel voting-based method that extends 2D segmentation models to 3D Gaussian splats. Our approach leverages masked gradients, where gradients are filtered by input 2D masks, and these gradients are used as votes to achieve accurate segmentation. As a byproduct, we found that inference-time gradients can also be used to prune Gaussians, resulting in up to 21% compression. Additionally, we explore few-shot affordance transfer, allowing annotations from 2D images to be effectively transferred onto 3D Gaussian splats. The robust yet straightforward mathematical formulation underlying this approach makes it a highly effective tool for numerous downstream applications, such as augmented reality (AR), object editing, and robotics.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="mihajlovic2024splatfields" data-title="SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction" data-authors="Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, Edmond Boyer" data-year="2024" data-tags='["Code", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'mihajlovic2024splatfields', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/mihajlovic2024splatfields.jpg" data-fallback="None" alt="Paper thumbnail for SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, Edmond Boyer</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.11211.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://markomih.github.io/SplatFields/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/markomih/SplatFields" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="meng2024beings" data-title="BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting" data-authors="Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang" data-year="2024" data-tags='["Autonomous Driving", "Code", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'meng2024beings', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/meng2024beings.jpg" data-fallback="None" alt="Paper thumbnail for BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.10216" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.mwg.ink/BEINGS-web" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/guaMass/BEINGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot’s sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024pshuman" data-title="PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion" data-authors="Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo" data-year="2024" data-tags='["Avatar", "Code", "Diffusion", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024pshuman', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024pshuman.jpg" data-fallback="None" alt="Paper thumbnail for PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.10141.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://penghtyx.github.io/PSHuman/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/pengHTYX/PSHuman" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Detailed and photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, full-body reconstruction from a monocular RGB image remains challenging due to the ill-posed nature of the problem and sophisticated clothing topology with self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling detailed and identity-preserved novel-view generation without any geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models like SMPL-X, which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multi-view normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experimental results and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate PSHumans superiority in geometry details, texture fidelity, and generalization capability.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2024dualgs" data-title="DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos" data-authors="Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu" data-year="2024" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2024dualgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2024dualgs.jpg" data-fallback="None" alt="Paper thumbnail for DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DualGS: Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.08353" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nowheretrix.github.io/DualGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/HiFi-Human/DualGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=vwDE8xr78Bg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liao2024fisheyegs" data-title="Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras" data-authors="Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang" data-year="2024" data-tags='["Code", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liao2024fisheyegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liao2024fisheyegs.jpg" data-fallback="None" alt="Paper thumbnail for Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.04751.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/zmliao/Fisheye-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="seidenschwarz2024dynomo" data-title="DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction" data-authors="Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taixé" data-year="2024" data-tags='["Dynamic", "Monocular"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'seidenschwarz2024dynomo', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/seidenschwarz2024dynomo.jpg" data-fallback="None" alt="Paper thumbnail for DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taixé</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.02104.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing scenes and tracking motion are two sides of the same coin. Tracking points allow for geometric reconstruction [14], while geometric reconstruction of (dynamic) scenes allows for 3D tracking of points over time [24, 39]. The latter was recently also exploited for 2D point tracking to overcome occlusion ambiguities by lifting tracking directly into 3D [38]. However, above approaches either require offline processing or multi-view camera setups both unrealistic for real-world applications like robot navigation or mixed reality. We target the challenge of online 2D and 3D point tracking from unposed monocular camera input introducing Dynamic Online Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to reconstruct dynamic scenes in an online fashion. Our approach extends 3D Gaussians to capture new content and object motions while estimating camera movements from a single RGB frame. DynOMo stands out by enabling emergence of point trajectories through robust image feature reconstruction and a novel similarity-enhanced regularization term, without requiring any correspondence-level supervision. It sets the first baseline for online point tracking with monocular unposed cameras, achieving performance on par with existing methods. We aim to inspire the community to advance online point tracking and reconstruction, expanding the applicability to diverse real-world scenarios.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zoomers2024progs" data-title="PRoGS: Progressive Rendering of Gaussian Splats" data-authors="Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels" data-year="2024" data-tags='["Compression", "LoD", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zoomers2024progs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zoomers2024progs.jpg" data-fallback="None" alt="Paper thumbnail for PRoGS: Progressive Rendering of Gaussian Splats" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PRoGS: Progressive Rendering of Gaussian Splats <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">LoD</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2409.01761.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Over the past year, 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However, it can require a substantial amount of storage since each splat's individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint, they still necessitate retrieving the entire scene before any part of it can be rendered. In this work, we introduce a novel approach for progressively rendering such scenes, aiming to display visible content that closely approximates the final scene as early as possible without loading the entire scene into memory. This approach benefits both on-device rendering applications limited by memory constraints and streaming applications where minimal bandwidth usage is preferred. To achieve this, we approximate the contribution of each Gaussian to the final scene and construct an order of prioritization on their inclusion in the rendering process. Additionally, we demonstrate that our approach can be combined with existing compression methods to progressively render (and stream) 3DGS scenes, optimizing bandwidth usage by focusing on the most important splats within a scene. Overall, our work establishes a foundation for making remotely hosted 3DGS content more quickly accessible to end-users in over-the-top consumption scenarios, with our results showing significant improvements in quality across all metrics compared to existing methods.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024omnire" data-title="OmniRe: Omni Urban Scene Reconstruction" data-authors="Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang" data-year="2024" data-tags='["Autonomous Driving", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024omnire', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024omnire.jpg" data-fallback="None" alt="Paper thumbnail for OmniRe: Omni Urban Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">OmniRe: Omni Urban Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.16760" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ziyc.github.io/omnire/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ziyc/drivestudio" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce OmniRe, a holistic approach for efficiently reconstructing high-fidelity dynamic urban scenes from on-device logs. Recent methods for modeling driving sequences using neural radiance fields or Gaussian Splatting have demonstrated the potential of reconstructing challenging dynamic scenes, but often overlook pedestrians and other non-vehicle dynamic actors, hindering a complete pipeline for dynamic urban scene reconstruction. To that end, we propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that allows for accurate, full-length reconstruction of diverse dynamic objects in a driving log. OmniRe builds dynamic neural scene graphs based on Gaussian representations and constructs multiple local canonical spaces that model various dynamic actors, including vehicles, pedestrians, and cyclists, among many others. This capability is unmatched by existing methods. OmniRe allows us to holistically reconstruct different objects present in the scene, subsequently enabling the simulation of reconstructed scenarios with all actors participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We believe our work fills a critical gap in driving reconstruction.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang20243d" data-title="3D Reconstruction with Spatial Memory" data-authors="Hengyi Wang, Lourdes Agapito" data-year="2024" data-tags='["3ster-based", "Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Reconstruction with Spatial Memory" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Reconstruction with Spatial Memory <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hengyi Wang, Lourdes Agapito</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.16061.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hengyiwang.github.io/projects/spanner" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/HengyiWang/spann3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://hengyiwang.github.io/projects/spanner/videos/spanner_intro.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress pointmaps from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and can process ordered image collections in real time. Project page: \url{https://hengyiwang.github.io/projects/spanner}
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shi2024lapisgs" data-title="LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming" data-authors="Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi," data-year="2024" data-tags='["Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shi2024lapisgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shi2024lapisgs.jpg" data-fallback="None" alt="Paper thumbnail for LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive Streaming <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi,</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.14823" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yuang-ian.github.io/lapisgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The rise of Extended Reality (XR) requires efficient streaming of 3D online worlds, challenging current 3DGS representations to adapt to bandwidth-constrained environments. We propose LapisGS, a layered 3DGS that supports adaptive streaming and progressive rendering. Our method constructs a layered structure for cumulative representation, incorporates dynamic opacity optimization to maintain visual fidelity, and utilizes occupancy maps to efficiently manage Gaussian splats. This proposed model offers a progressive representation supporting a continuous rendering quality adapted for bandwidth-aware streaming. Extensive experiments validate the effectiveness of our approach in balancing visual fidelity with the compactness of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in LPIPS, and 318.41% reduction in model size, and shows its potential for bandwidth-adapted 3D streaming and rendering applications.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="smart2024splatt3r" data-title="Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs" data-authors="Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu" data-year="2024" data-tags='["3ster-based", "Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'smart2024splatt3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/smart2024splatt3r.jpg" data-fallback="None" alt="Paper thumbnail for Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.13912.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://splatt3r.active.vision/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/btsmart/splatt3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang202425" data-title="TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers" data-authors="Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang" data-year="2024" data-tags='["Code", "Feed-Forward", "Project", "Sparse", "Transformer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang202425', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang202425.jpg" data-fallback="None" alt="Paper thumbnail for TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Transformer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.13770" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xingyoujun.github.io/transplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/xingyoujun/transplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Compared with previous 3D reconstruction methods like Nerf, recent Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even in the sparse-view setting. However, the promising reconstruction performance of existing G-3DGS methods relies heavily on accurate multi-view feature matching, which is quite challenging. Especially for the scenes that have many non-overlapping areas between various views and contain numerous similar regions, the matching performance of existing methods is poor and the reconstruction precision is limited. To address this problem, we develop a strategy that utilizes a predicted depth confidence map to guide accurate local feature matching. In addition, we propose to utilize the knowledge of existing monocular depth estimation models as prior to boost the depth estimation precision in non-overlapping areas between views. Combining the proposed strategies, we present a novel G-3DGS method named TranSplat, which obtains the best performance on both the RealEstate10K and ACID benchmarks while maintaining competitive speed and presenting strong cross-dataset generalization ability.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dihlmann2024subsurface" data-title="Subsurface Scattering for 3D Gaussian Splatting" data-authors="Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch" data-year="2024" data-tags='["Project", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dihlmann2024subsurface', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dihlmann2024subsurface.jpg" data-fallback="None" alt="Paper thumbnail for Subsurface Scattering for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Subsurface Scattering for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.12282.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://sss.jdihlmann.com/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting and novel view synthesis at interactive rates. We show successful application on synthetic data and introduce a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes. Project page https://sss.jdihlmann.com/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024gsloc" data-title="GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting" data-authors="Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud" data-year="2024" data-tags='["Poses", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024gsloc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024gsloc.jpg" data-fallback="None" alt="Paper thumbnail for GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud</p>
      <div class="paper-tags"><span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.11085" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gsloc.active.vision/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement framework, GSLoc. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model renders high-quality synthetic images and depth maps to facilitate the establishment of 2D-3D correspondences. GSLoc obviates the need for training feature extractors or descriptors by operating directly on RGB images, utilizing the 3D foundation model, MASt3R, for precise 2D matching. To improve the robustness of our model in challenging outdoor environments, we incorporate an exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables efficient one-shot pose refinement given a single RGB query and a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based optimization methods in both accuracy and runtime across indoor and outdoor visual localization benchmarks, achieving new state-of-the-art accuracy on two indoor datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2024loopsplat" data-title="LoopSplat: Loop Closure by Registering 3D Gaussian Splats" data-authors="Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro Armeni" data-year="2024" data-tags='["Code", "Project", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2024loopsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2024loopsplat.jpg" data-fallback="None" alt="Paper thumbnail for LoopSplat: Loop Closure by Registering 3D Gaussian Splats" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LoopSplat: Loop Closure by Registering 3D Gaussian Splats <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro Armeni</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.10154" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://loopsplat.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/GradientSpaces/LoopSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose LoopSplat, which takes RGB-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD, ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense RGB-D SLAM.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="feng2024flashgs" data-title="FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering" data-authors="Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, Bo Dai" data-year="2024" data-tags='["Acceleration", "Code"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'feng2024flashgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/feng2024flashgs.jpg" data-fallback="None" alt="Paper thumbnail for FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.07967.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/InternLandMark/FlashGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This work introduces FlashGS, an open-source CUDA Python library, designed to facilitate the efficient differentiable rasterization of 3D Gaussian Splatting through algorithmic and kernel-level optimizations. FlashGS is developed based on the observations from a comprehensive analysis of the rendering process to enhance computational efficiency and bring the technique to wide adoption. The paper includes a suite of optimization strategies, encompassing redundancy elimination, efficient pipelining, refined control and scheduling mechanisms, and memory access optimizations, all of which are meticulously integrated to amplify the performance of the rasterization process. An extensive evaluation of FlashGS' performance has been conducted across a diverse spectrum of synthetic and real-world large-scale scenes, encompassing a variety of image resolutions. The empirical findings demonstrate that FlashGS consistently achieves an average 4x acceleration over mobile consumer GPUs, coupled with reduced memory consumption. These results underscore the superior performance and resource optimization capabilities of FlashGS, positioning it as a formidable tool in the domain of 3D rendering.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2024rethinking" data-title="Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space" data-authors="Hyunjee Lee*, Youngsik Yun*, Jeongmin Bae, Seoha Kim, Youngjung Uh" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2024rethinking', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2024rethinking.jpg" data-fallback="None" alt="Paper thumbnail for Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hyunjee Lee*, Youngsik Yun*, Jeongmin Bae, Seoha Kim, Youngjung Uh</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.07416" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hyunji12.github.io/Open3DRF/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hyunji12/Open3DRF" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Understanding the 3D semantics of a scene is a fundamental problem for various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view synthesis, previous methods for understanding their semantics have been limited to incomplete 3D understanding: their segmentation results are 2D masks and their supervision is anchored at 2D pixels. This paper revisits the problem set to pursue a better 3D understanding of a scene modeled by NeRFs and 3DGS as follows. 1) We directly supervise the 3D points to train the language embedding field. It achieves state-of-the-art accuracy without relying on multi-scale language embeddings. 2) We transfer the pre-trained language field to 3DGS, achieving the first real-time rendering speed without sacrificing training time or accuracy. 3) We introduce a 3D querying and evaluation protocol for assessing the reconstructed geometry and semantics together. Code, checkpoints, and annotations will be available online.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zheng2024headgap" data-title="HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors" data-authors="Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu" data-year="2024" data-tags='["Avatar", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zheng2024headgap', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zheng2024headgap.jpg" data-fallback="None" alt="Paper thumbnail for HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.06019.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://headgap.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present a novel 3D head avatar creation approach capable of generalizing from few-shot in-the-wild data with high-fidelity and animatable robustness. Given the underconstrained nature of this problem, incorporating prior knowledge is essential. Therefore, we propose a framework comprising prior learning and avatar creation phases. The prior learning phase leverages 3D head priors derived from a large-scale multi-view dynamic dataset, and the avatar creation phase applies these priors for few-shot personalization. Our approach effectively captures these priors by utilizing a Gaussian Splatting-based auto-decoder network with part-based dynamic modeling. Our method employs identity-shared encoding with personalized latent codes for individual identities to learn the attributes of Gaussian primitives. During the avatar creation phase, we achieve fast head avatar personalization by leveraging inversion and fine-tuning strategies. Extensive experiments demonstrate that our model effectively exploits head priors and successfully generalizes them to few-shot personalization, achieving photo-realistic rendering quality, multi-view consistency, and stable animation.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chahe2024query3d" data-title="Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language Embedded 3D Gaussian" data-authors="Amirhosein Chahe, Lifeng Zhou" data-year="2024" data-tags='["Code", "Language Embedding", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chahe2024query3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chahe2024query3d.jpg" data-fallback="None" alt="Paper thumbnail for Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language Embedded 3D Gaussian" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language Embedded 3D Gaussian <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Amirhosein Chahe, Lifeng Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.03516.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Zhourobotics/Query-3DGS-LLM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper introduces a novel method for open-vocabulary 3D scene querying in autonomous driving by combining Language Embedded 3D Gaussians with Large Language Models (LLMs). We propose utilizing LLMs to generate both contextually canonical phrases and helping positive words for enhanced segmentation and scene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to create a high-quality text dataset, which we then use to fine-tune smaller, more efficient LLMs for on-device deployment. Our comprehensive evaluation on the WayveScenes101 dataset demonstrates that LLM-guided segmentation significantly outperforms traditional approaches based on predefined canonical phrases. Notably, our fine-tuned smaller models achieve performance comparable to larger expert models while maintaining faster inference times. Through ablation studies, we discover that the effectiveness of helping positive words correlates with model scale, with larger models better equipped to leverage additional semantic information. This work represents a significant advancement towards more efficient, context-aware autonomous driving systems, effectively bridging 3D scene representation with high-level semantic querying while maintaining practical deployment considerations.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kaleta2024lumigauss" data-title="LumiGauss: Relightable Gaussian Splatting in the Wild" data-authors="Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski" data-year="2024" data-tags='["Code", "In the Wild", "Project", "Relight"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kaleta2024lumigauss', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kaleta2024lumigauss.jpg" data-fallback="None" alt="Paper thumbnail for LumiGauss: Relightable Gaussian Splatting in the Wild" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LumiGauss: Relightable Gaussian Splatting in the Wild <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">In the Wild</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2408.04474.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lumigauss.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/joaxkal/lumigauss" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue, often at the expense of output fidelity, which questions the practicality of such methods. We introduce LumiGauss - a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows, common in outdoor scenes, by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer. We validate our method on the NeRF-OSR dataset, demonstrating superior performance over baseline methods. Moreover, LumiGauss can synthesize realistic images for unseen environment maps. Our code: https://github.com/joaxkal/lumigauss.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wildersmith2024radiance" data-title="Radiance Fields for Robotic Teleoperation" data-authors="Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter" data-year="2024" data-tags='["Code", "Misc", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wildersmith2024radiance', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wildersmith2024radiance.jpg" data-fallback="None" alt="Paper thumbnail for Radiance Fields for Robotic Teleoperation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Radiance Fields for Robotic Teleoperation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2407.20194" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://leggedrobotics.github.io/rffr.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/leggedrobotics/radiance_field_ros" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bao20243d" data-title="3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities" data-authors="Yanqi Bao, Tianyu Ding, Jing Huo, Yaoli Liu, Yuxin Li, Wenbin Li, Yang Gao, Jiebo Luo" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bao20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bao20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yanqi Bao, Tianyu Ding, Jing Huo, Yaoli Liu, Yuxin Li, Wenbin Li, Yang Gao, Jiebo Luo</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2407.17418.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the potential to become a mainstream method for 3D representations. It can effectively transform multi-view images into explicit 3D Gaussian through efficient training, and achieve real-time rendering of novel views. This survey aims to analyze existing 3DGS-related works from multiple intersecting perspectives, including related tasks, technologies, challenges, and opportunities. The primary objective is to provide newcomers with a rapid understanding of the field and to assist researchers in methodically organizing existing technologies and challenges. Specifically, we delve into the optimization, application, and extension of 3DGS, categorizing them based on their focuses or motivations. Additionally, we summarize and classify nine types of technical modules and corresponding improvements identified in existing works. Based on these analyses, we further examine the common challenges and technologies across various tasks, proposing potential research opportunities.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="moenne-loccoz20243d" data-title="3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes" data-authors="Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic" data-year="2024" data-tags='["Project", "Ray Tracing", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'moenne-loccoz20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/moenne-loccoz20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2407.07090.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussiantracer.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://gaussiantracer.github.io/res/3dgrt_supplementary_video.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ji2024segment" data-title="Segment Any 4D Gaussians" data-authors="Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang" data-year="2024" data-tags='["Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ji2024segment', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ji2024segment.jpg" data-fallback="None" alt="Paper thumbnail for Segment Any 4D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Segment Any 4D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2407.04504" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jsxzs.github.io/sa4d/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Modeling, understanding, and reconstructing the real world are crucial in XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable success in modeling and understanding 3D scenes. Similarly, various 4D representations have demonstrated the ability to capture the dynamics of the 4D world. However, there is a dearth of research focusing on segmentation within 4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D), one of the first frameworks to segment anything in the 4D digital world based on 4D Gaussians. In SA4D, an efficient temporal identity feature field is introduced to handle Gaussian drifting, with the potential to learn precise identity features from noisy and sparse input. Additionally, a 4D segmentation refinement process is proposed to remove artifacts. Our SA4D achieves precise, high-quality segmentation within seconds in 4D Gaussians and shows the ability to remove, recolor, compose, and render high-quality anything masks. More demos are available at: https://jsxzs.github.io/sa4d/.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2024gscore" data-title="GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting" data-authors="Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, Jaewoong Sim" data-year="2024" data-tags='["Acceleration", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2024gscore', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2024gscore.jpg" data-fallback="None" alt="Paper thumbnail for GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSCore: Efficient Radiance Field Rendering via Architectural Support for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, Jaewoong Sim</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://jaewoong.org/pubs/asplos24-gscore.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://youtu.be/TByYGw837IU?si=7zBe0yqpsJUoVbIV" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents GSCore, a hardware acceleration unit that efficiently executes the rendering pipeline of 3D Gaussian Splatting with algorithmic optimizations. GSCore builds on the observations from an in-depth analysis of Gaussian-based radiance field rendering to enhance computational efficiency and bring the technique to wide adoption. In doing so, we present several optimization techniques, Gaussian shape-aware intersection test, hierarchical sorting, and subtile skipping, all of which are synergistically integrated with GSCore. We implement the hardware design of GSCore, synthesize it using a commercial 28nm technology, and evaluate the performance across a range of synthetic and real-world scenes with varying image resolutions. Our evaluation results show that GSCore achieves a 15.86× speedup on average over the mobile consumer GPU with a substantially smaller area and lower energy consumption.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kim2024optimizing" data-title="Optimizing Dynamic NeRF and 3DGS with No Video Synchronization" data-authors="Seoha Kim*, Jeongmin Bae*, Youngsik Yun, HyunSeung Son, Hahyun Lee, Gun Bang, Youngjung Uh" data-year="2024" data-tags='["Dynamic"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kim2024optimizing', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kim2024optimizing.jpg" data-fallback="None" alt="Paper thumbnail for Optimizing Dynamic NeRF and 3DGS with No Video Synchronization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Optimizing Dynamic NeRF and 3DGS with No Video Synchronization <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Seoha Kim*, Jeongmin Bae*, Youngsik Yun, HyunSeung Son, Hahyun Lee, Gun Bang, Youngjung Uh</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span></div>
      <div class="paper-links"><a href="https://openreview.net/pdf?id=RQutkn4V9I" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in 4D scene reconstruction using dynamic NeRF and 3DGS have demonstrated the ability to represent dynamic scenes from multi-view videos. However, they fail to reconstruct the dynamic scenes and struggle to fit even the training views in unsynchronized settings. It happens because they employ a single latent embedding for a frame, while the multi-view images at the same frame were actually captured at different moments. To address this limitation, we introduce time offsets for individual unsynchronized videos and jointly optimize the offsets with the field. By design, our method is applicable for various baselines, even regardless of the types of radiance fields. We conduct experiments on the common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to verify the performance of our method.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024egogaussian" data-title="EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting" data-authors="Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang" data-year="2024" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024egogaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024egogaussian.jpg" data-fallback="None" alt="Paper thumbnail for EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.19811" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zdwww.github.io/egogs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zdwww/EgoGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/nsZrmM7CJB0?si=IJnfWH_Vf_UW2JoF" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Human activities are inherently complex, often involving numerous object interactions. To better understand these activities, it is crucial to model their interactions with the environment captured through dynamic changes. The recent availability of affordable head-mounted cameras and egocentric data offers a more accessible and efficient means to understand human-object interactions in 3D environments. However, most existing methods for human activity modeling neglect the dynamic interactions with objects, resulting in only static representations. The few existing solutions often require inputs from multiple sources, including multi-camera setups, depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the first method capable of simultaneously reconstructing 3D scenes and dynamically tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely discrete nature of Gaussian Splatting and segment dynamic interactions from the background, with both having explicit representations. Our approach employs a clip-level online learning pipeline that leverages the dynamic nature of human activities, allowing us to reconstruct the temporal evolution of the scene in chronological order and track rigid object motion. EgoGaussian shows significant improvements in terms of both dynamic object and background reconstruction quality compared to the state-of-the-art. We also qualitatively demonstrate the high quality of the reconstructed models.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="stearns2024dynamic" data-title="Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos" data-authors="Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas" data-year="2024" data-tags='["Code", "Dynamic", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'stearns2024dynamic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/stearns2024dynamic.jpg" data-fallback="None" alt="Paper thumbnail for Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.18717.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/coltonstearns/dynamic-gaussian-marbles" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian splatting has become a popular representation for novel-view synthesis, exhibiting clear strengths in efficiency, photometric quality, and compositional edibility. Following its success, many works have extended Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while also tracking scene geometry far better than alternative representations. Yet, these methods assume dense multi-view videos as supervision. In this work, we are interested in extending the capability of Gaussian scene representations to casually captured monocular videos. We show that existing 4D Gaussian methods dramatically fail in this setup because the monocular setting is underconstrained. Building off this finding, we propose a method we call Dynamic Gaussian Marbles, which consist of three core modifications that target the difficulties of the monocular setting. First, we use isotropic Gaussian "marbles'', reducing the degrees of freedom of each Gaussian. Second, we employ a hierarchical divide and-conquer learning strategy to efficiently guide the optimization towards solutions with globally coherent motion. Finally, we add image-level and geometry-level priors into the optimization, including a tracking loss that takes advantage of recent progress in point tracking. By constraining the optimization, Dynamic Gaussian Marbles learns Gaussian trajectories that enable novel-view rendering and accurately capture the 3D motion of the scene elements. We evaluate on the Nvidia Dynamic Scenes dataset and the DyCheck iPhone dataset, and show that Gaussian Marbles significantly outperforms other Gaussian baselines in quality, and is on-par with non-Gaussian representations, all while maintaining the efficiency, compositionality, editability, and tracking benefits of Gaussians. Our project page can be found here https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhao2024on" data-title="On Scaling Up 3D Gaussian Splatting Training" data-authors="Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie" data-year="2024" data-tags='["Code", "Distributed", "Large-Scale", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhao2024on', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhao2024on.jpg" data-fallback="None" alt="Paper thumbnail for On Scaling Up 3D Gaussian Splatting Training" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">On Scaling Up 3D Gaussian Splatting Training <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Distributed</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.18533" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://daohanlu.github.io/scaling-up-3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/nyu-systems/Grendel-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/WaYfY3GTs6U" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024gsoctree" data-title="GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting" data-authors="Jiaze Li, Zhengyu Wen, Luo Zhang, Jiangbei Hu, Fei Hou, Zhebin Zhang, Ying He" data-year="2024" data-tags='["Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024gsoctree', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024gsoctree.jpg" data-fallback="None" alt="Paper thumbnail for GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D Reconstruction Under Strong Lighting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiaze Li, Zhengyu Wen, Luo Zhang, Jiangbei Hu, Fei Hou, Zhebin Zhang, Ying He</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.18199.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The 3D Gaussian Splatting technique has significantly advanced the construction of radiance fields from multi-view images, enabling real-time rendering. While point-based rasterization effectively reduces computational demands for rendering, it often struggles to accurately reconstruct the geometry of the target object, especially under strong lighting. To address this challenge, we introduce a novel approach that combines octree-based implicit surface representations with Gaussian splatting. Our method consists of four stages. Initially, it reconstructs a signed distance field (SDF) and a radiance field through volume rendering, encoding them in a low-resolution octree. The initial SDF represents the coarse geometry of the target object. Subsequently, it introduces 3D Gaussians as additional degrees of freedom, which are guided by the SDF. In the third stage, the optimized Gaussians further improve the accuracy of the SDF, allowing it to recover finer geometric details compared to the initial SDF obtained in the first stage. Finally, it adopts the refined SDF to further optimize the 3D Gaussians via splatting, eliminating those that contribute little to visual appearance. Experimental results show that our method, which leverages the distribution of 3D Gaussians with SDFs, reconstructs more accurate geometry, particularly in images with specular highlights caused by strong lighting.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="papantonakis2024reducing" data-title="Reducing the Memory Footprint of 3D Gaussian Splatting" data-authors="Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, George Drettakis" data-year="2024" data-tags='["Code", "Compression", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'papantonakis2024reducing', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/papantonakis2024reducing.jpg" data-fallback="None" alt="Paper thumbnail for Reducing the Memory Footprint of 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Reducing the Memory Footprint of 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, George Drettakis</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.17074.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://repo-sam.inria.fr/fungraph/reduced_3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://repo-sam.inria.fr/fungraph/reduced_3dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=EnKE-d7eMds&t=48s" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting provides excellent visual quality for novel view synthesis, with fast training and realtime rendering; unfortunately, the memory requirements of this method for storing and transmission are unreasonably high. We first analyze the reasons for this, identifying three main areas where storage can be reduced: the number of 3D Gaussian primitives used to represent a scene, the number of coefficients for the spherical harmonics used to represent directional radiance, and the precision required to store Gaussian primitive attributes. We present a solution to each of these issues. First, we propose an efficient, resolutionaware primitive pruning approach, reducing the primitive count by half. Second, we introduce an adaptive adjustment method to choose the number of coefficients used to represent directional radiance for each Gaussian primitive, and finally a codebook-based quantization method, together with a half-float representation for further memory reduction. Taken together, these three components result in a ×27 reduction in overall size on disk on the standard datasets we tested, along with a x1.7 speedup in rendering speed. We demonstrate our method on standard datasets and show how our solution results in significantly reduced download times when using the method on a mobile device (see Fig. 1).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="mallick2024taming" data-title="Taming 3DGS: High-Quality Radiance Fields with Limited Resources" data-authors="Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Francisco Vicente Carrasco, Markus Steinberger, Fernando De La Torre" data-year="2024" data-tags='["Acceleration", "Code", "Densification", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'mallick2024taming', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/mallick2024taming.jpg" data-fallback="None" alt="Paper thumbnail for Taming 3DGS: High-Quality Radiance Fields with Limited Resources" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Taming 3DGS: High-Quality Radiance Fields with Limited Resources <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Saswat Subhajyoti Mallick, Rahul Goel, Bernhard Kerbl, Francisco Vicente Carrasco, Markus Steinberger, Fernando De La Torre</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.15643.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://humansensinglab.github.io/taming-3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/humansensinglab/taming-3dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/iiUXqYmezbY" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has transformed novel-view synthesis with its fast, interpretable, and high-fidelity rendering. However, its resource requirements limit its usability. Especially on constrained devices, training performance degrades quickly and often cannot complete due to excessive memory consumption of the model. The method converges with an indefinite number of Gaussians -- many of them redundant -- making rendering unnecessarily slow and preventing its usage in downstream tasks that expect fixed-size inputs. To address these issues, we tackle the challenges of training and rendering 3DGS models on a budget. We use a guided, purely constructive densification process that steers densification toward Gaussians that raise the reconstruction quality. Model size continuously increases in a controlled manner towards an exact budget, using score-based densification of Gaussians with training-time priors that measure their contribution. We further address training speed obstacles: following a careful analysis of 3DGS' original pipeline, we derive faster, numerically equivalent solutions for gradient computation and attribute updates, including an alternative parallelization for efficient backpropagation. We also propose quality-preserving approximations where suitable to reduce training time even further. Taken together, these enhancements yield a robust, scalable solution with reduced training times, lower compute and memory requirements, and high quality. Our evaluation shows that in a budgeted setting, we obtain competitive quality metrics with 3DGS while achieving a 4--5x reduction in both model size and training time. With more generous budgets, our measured quality surpasses theirs. These advances open the door for novel-view synthesis in constrained environments, e.g., mobile devices.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun2024splatter" data-title="Splatter a Video: Video Gaussian Representation for Versatile Processing" data-authors="Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi" data-year="2024" data-tags='["Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun2024splatter', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun2024splatter.jpg" data-fallback="None" alt="Paper thumbnail for Splatter a Video: Video Gaussian Representation for Versatile Processing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Splatter a Video: Video Gaussian Representation for Versatile Processing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.13870.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://sunyangtian.github.io/spatter_a_video_web/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Video representation is a long-standing problem that is crucial for various down-stream tasks, such as tracking,depth prediction,segmentation,view synthesis,and editing. However, current methods either struggle to model complex motions due to the absence of 3D structure or rely on implicit 3D representations that are ill-suited for manipulation tasks. To address these challenges, we introduce a novel explicit 3D representation-video Gaussian representation -- that embeds a video into 3D Gaussians. Our proposed representation models video appearance in a 3D canonical space using explicit Gaussians as proxies and associates each Gaussian with 3D motions for video motion. This approach offers a more intrinsic and explicit representation than layered atlas or volumetric pixel matrices. To obtain such a representation, we distill 2D priors, such as optical flow and depth, from foundation models to regularize learning in this ill-posed setting. Extensive applications demonstrate the versatility of our new video representation. It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation. Project page: https://sunyangtian.github.io/spatter_a_video_web/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="pan2023humansplat" data-title="HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors" data-authors="Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, Yebin Liu" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'pan2023humansplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/pan2023humansplat.jpg" data-fallback="None" alt="Paper thumbnail for HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.12459" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://humansplat.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/humansplat/humansplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we present HumanSplat that predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner. In particular, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is further designed to achieve high-fidelity texture modeling and better constrain the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis. Project page: https://humansplat.github.io/.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kerbl2024a" data-title="A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets" data-authors="Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer,  Alexandre Lanvin, George Drettakis" data-year="2024" data-tags='["Code", "Large-Scale", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kerbl2024a', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kerbl2024a.jpg" data-fallback="None" alt="Paper thumbnail for A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer,  Alexandre Lanvin, George Drettakis</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.12080.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/graphdeco-inria/hierarchical-3d-gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/content/videos/small_city.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hyung2024effective" data-title="Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting" data-authors="Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, Jin-Hwa Kim" data-year="2024" data-tags='["Densification", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hyung2024effective', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hyung2024effective.jpg" data-fallback="None" alt="Paper thumbnail for Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junha Hyung, Susung Hong, Sungwon Hwang, Jaeseong Lee, Jaegul Choo, Jin-Hwa Kim</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.11672" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://junhahyung.github.io/erankgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D reconstruction from multi-view images is one of the fundamental challenges in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising technique capable of real-time rendering with high-quality 3D reconstruction. This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying. Despite its potential, 3DGS encounters challenges, including needle-like artifacts, suboptimal geometries, and inaccurate normals, due to the Gaussians converging into anisotropic Gaussians with one dominant variance. We propose using effective rank analysis to examine the shape statistics of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like shapes with the effective rank 1. To address this, we introduce effective rank as a regularization, which constrains the structure of the Gaussians. Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts. The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="t20243dgszip" data-title="3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods" data-authors="Milena T. Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter Eisert, Wieland Morgenstern" data-year="2024" data-tags='["Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 't20243dgszip', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/t20243dgszip.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Milena T. Bagdasarian, Paul Knoll, Florian Barthel, Anna Hilsmann, Peter Eisert, Wieland Morgenstern</p>
      <div class="paper-tags"><span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2407.09510" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://w-m.github.io/3dgs-compression-survey" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a work-in-progress survey on 3D Gaussian Splatting compression methods, focusing on their statistical performance across various benchmarks. This survey aims to facilitate comparability by summarizing key statistics of different compression approaches in a tabulated format. The datasets evaluated include TanksAndTemples, MipNeRF360, DeepBlending, and SyntheticNeRF. For each method, we report the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the resultant size in megabytes (MB), as provided by the respective authors. This is an ongoing, open project, and we invite contributions from the research community as GitHub issues or pull requests. Please visit https://w-m.github.io/3dgs-compression-survey/ for more information and a sortable version of the table.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hanson2024pup" data-title="PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting" data-authors="Alex Hanson, Allen Tu, Vasu Singla, Mayuka Jayawardhana, Matthias Zwicker, Tom Goldstein" data-year="2024" data-tags='["Code", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hanson2024pup', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hanson2024pup.jpg" data-fallback="None" alt="Paper thumbnail for PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Alex Hanson, Allen Tu, Vasu Singla, Mayuka Jayawardhana, Matthias Zwicker, Tom Goldstein</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.10219.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pup3dgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/j-alex-hanson/gaussian-splatting-pup" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="leroy2024grounding" data-title="Grounding Image Matching in 3D with MASt3R" data-authors="Vincent Leroy, Yohann Cabon, Jérôme Revaud" data-year="2024" data-tags='["Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'leroy2024grounding', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/leroy2024grounding.jpg" data-fallback="None" alt="Paper thumbnail for Grounding Image Matching in 3D with MASt3R" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Grounding Image Matching in 3D with MASt3R <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Vincent Leroy, Yohann Cabon, Jérôme Revaud</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.09756.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/naver/mast3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision. Yet despite matching being fundamentally a 3D problem, intrinsically linked to camera pose and scene geometry, it is typically treated as a 2D problem. This makes sense as the goal of matching is to establish correspondences between 2D pixel fields, but also seems like a potentially hazardous choice. In this work, we take a different stance and propose to cast matching as a 3D task with DUSt3R, a recent and powerful 3D reconstruction framework based on Transformers. Based on pointmaps regression, this method displayed impressive robustness in matching views with extreme viewpoint changes, yet with limited accuracy. We aim here to improve the matching capabilities of such an approach while preserving its robustness. We thus propose to augment the DUSt3R network with a new head that outputs dense local features, trained with an additional matching loss. We further address the issue of quadratic complexity of dense matching, which becomes prohibitively slow for downstream applications if not carefully treated. We introduce a fast reciprocal matching scheme that not only accelerates matching by orders of magnitude, but also comes with theoretical guarantees and, lastly, yields improved results. Extensive experiments show that our approach, coined MASt3R, significantly outperforms the state of the art on multiple matching tasks. In particular, it beats the best published methods by 30% (absolute improvement) in VCRE AUC on the extremely challenging Map-free localization dataset.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kirschstein2024gghead" data-title="GGHead: Fast and Generalizable 3D Gaussian Heads" data-authors="Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner" data-year="2024" data-tags='["Code", "GAN", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kirschstein2024gghead', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kirschstein2024gghead.jpg" data-fallback="None" alt="Paper thumbnail for GGHead: Fast and Generalizable 3D Gaussian Heads" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GGHead: Fast and Generalizable 3D Gaussian Heads <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">GAN</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.09377.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://tobias-kirschstein.github.io/gghead/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/tobias-kirschstein/gghead" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/M5vq3DoZ7RI" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time. Project Website: https://tobias-kirschstein.github.io/gghead
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jaganathan2024iceg" data-title="ICE-G: Image Conditional Editing of 3D Gaussian Splats" data-authors="Vishnu Jaganathan, Hannah Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira" data-year="2024" data-tags='["Editing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jaganathan2024iceg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jaganathan2024iceg.jpg" data-fallback="None" alt="Paper thumbnail for ICE-G: Image Conditional Editing of 3D Gaussian Splats" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ICE-G: Image Conditional Editing of 3D Gaussian Splats <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Vishnu Jaganathan, Hannah Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.08488" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ice-gaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/dDsCwRXixp8?si=415s7-dEpM7-FPMq" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine grained control of editing.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fan2024trim" data-title="Trim 3D Gaussian Splatting for Accurate Geometry Representation" data-authors="Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang" data-year="2024" data-tags='["2DGS", "Code", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fan2024trim', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fan2024trim.jpg" data-fallback="None" alt="Paper thumbnail for Trim 3D Gaussian Splatting for Accurate Geometry Representation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Trim 3D Gaussian Splatting for Accurate Geometry Representation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.07499" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://trimgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/YuxueYang1204/TrimGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu20244real" data-title="4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models" data-authors="Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee" data-year="2024" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu20244real', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu20244real.jpg" data-fallback="None" alt="Paper thumbnail for 4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.07472.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://snap-research.github.io/4Real/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets. As a result, the generated scenes are often object-centric and lack photorealism. To address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets. Our method begins by generating a reference video using the video generation model. We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video. To handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections. We then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video. The pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jin2024lighting" data-title="Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis" data-authors="Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, Chongyi Li" data-year="2024" data-tags='["Code", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jin2024lighting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jin2024lighting.jpg" data-fallback="None" alt="Paper thumbnail for Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, Chongyi Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.06216.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://srameo.github.io/projects/le3d/intro.htmla" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Srameo/LE3D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://srameo.github.io/projects/le3d/assets/demo_video_interactive_viewer_compressed.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes. While, they suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in distant views; 2) the limited representation capacity of spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM, and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for downstream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous volumetric rendering based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can be found in https://github.com/Srameo/LE3D .
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hyun2024adversarial" data-title="Adversarial Generation of Hierarchical Gaussians for 3d Generative Model" data-authors="Sangeek Hyun, Jae-Pil Heo" data-year="2024" data-tags='["Code", "GAN", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hyun2024adversarial', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hyun2024adversarial.jpg" data-fallback="None" alt="Paper thumbnail for Adversarial Generation of Hierarchical Gaussians for 3d Generative Model" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Adversarial Generation of Hierarchical Gaussians for 3d Generative Model <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sangeek Hyun, Jae-Pil Heo</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">GAN</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.02968" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://hse1032.github.io/gsgan" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hse1032/GSGAN" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a naïve generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024gs2mesh" data-title=" RaDe-GS: Rasterizing Depth in Gaussian Splatting " data-authors="Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan" data-year="2024" data-tags='["Code", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024gs2mesh', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024gs2mesh.jpg" data-fallback="None" alt="Paper thumbnail for  RaDe-GS: Rasterizing Depth in Gaussian Splatting " class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title"> RaDe-GS: Rasterizing Depth in Gaussian Splatting  <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.01467" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://baowenz.github.io/radegs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/BaowenZ/RaDe-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting (GS) has proven to be highly effective in novel view synthesis, achieving high-quality and real-time rendering. However, its potential for reconstructing detailed 3D shapes has not been fully explored. Existing methods often suffer from limited shape accuracy due to the discrete and unstructured nature of Gaussian splats, which complicates the shape extraction. While recent techniques like 2D GS have attempted to improve shape reconstruction, they often reformulate the Gaussian primitives in ways that reduce both rendering quality and computational efficiency. To address these problems, our work introduces a rasterized approach to render the depth maps and surface normal maps of general 3D Gaussian splats. Our method not only significantly enhances shape reconstruction accuracy but also maintains the computational efficiency intrinsic to Gaussian Splatting. Our approach achieves a Chamfer distance error comparable to NeuraLangelo[Li et al. 2023] on the DTU dataset and similar training and rendering time as traditional Gaussian Splatting on the Tanks & Temples dataset. Our method is a significant advancement in Gaussian Splatting and can be directly integrated into existing Gaussian Splatting-based methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024modgs" data-title="MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos" data-authors="Qingming Liu*, Yuan Liu*, Jiepeng Wang, Xianqiang Lv,Peng Wang, Wenping Wang, Junhui Hou†," data-year="2024" data-tags='["Dynamic", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024modgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024modgs.jpg" data-fallback="None" alt="Paper thumbnail for MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qingming Liu*, Yuan Liu*, Jiepeng Wang, Xianqiang Lv,Peng Wang, Wenping Wang, Junhui Hou†,</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.00434" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://modgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we propose MoDGS, a new pipeline to render novel-view images in dynamic scenes using only casually captured monocular videos. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement of input cameras to construct multiview consistency but fail to reconstruct dynamic scenes on casually captured input videos whose cameras are static or move slowly. To address this challenging task, MoDGS adopts recent single-view depth estimation methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization method is proposed to learn a reasonable deformation field and a new robust depth loss is proposed to guide the learning of dynamic scene geometry. Comprehensive experiments demonstrate that MoDGS is able to render high-quality novel view images of dynamic scenes from just a casually captured monocular video, which outperforms baseline methods by a significant margin.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024contextgs" data-title="ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model" data-authors="Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, Bihan Wen" data-year="2024" data-tags='["Code", "Compression"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024contextgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024contextgs.jpg" data-fallback="None" alt="Paper thumbnail for ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, Bihan Wen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.20721.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/wyf0912/ContextGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for novel view synthesis, offering fast rendering speeds and high fidelity. However, the large number of Gaussians and their associated attributes require effective compression techniques. Existing methods primarily compress neural Gaussians individually and independently, i.e., coding all the neural Gaussians at the same time, with little design for their interactions and spatial dependence. Inspired by the effectiveness of the context model in image compression, we propose the first autoregressive model at the anchor level for 3DGS compression in this work. We divide anchors into different levels and the anchors that are not coded yet can be predicted based on the already coded ones in all the coarser levels, leading to more accurate modeling and higher coding efficiency. To further improve the efficiency of entropy coding, e.g., to code the coarsest level with no already coded anchors, we propose to introduce a low-dimensional quantized feature as the hyperprior for each anchor, which can be effectively compressed. Our work pioneers the context model in the anchor level for 3DGS representation, yielding an impressive size reduction of over 100 times compared to vanilla 3DGS and 15 times compared to the most recent state-of-the-art work Scaffold-GS, while achieving comparable or even higher rendering quality.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="labe2024dgd" data-title="DGD: Dynamic 3D Gaussians Distillation" data-authors="Isaac Labe*, Noam Issachar*, Itai Lang, Sagie Benaim" data-year="2024" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'labe2024dgd', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/labe2024dgd.jpg" data-fallback="None" alt="Paper thumbnail for DGD: Dynamic 3D Gaussians Distillation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DGD: Dynamic 3D Gaussians Distillation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Isaac Labe*, Noam Issachar*, Itai Lang, Sagie Benaim</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.19321" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://isaaclabe.github.io/DGD-Website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Isaaclabe/DGD-Dynamic-3D-Gaussians-Distillation" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=GzX2GJn9OKs" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jurca2024rtgs2" data-title="RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields" data-authors="Mihnea-Bogdan Jurca, Remco Royen, Ion Giosan, Adrian Munteanu" data-year="2024" data-tags='["Code", "Point Cloud", "Project", "Segmentation", "Transformer", "Virtual Reality"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jurca2024rtgs2', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jurca2024rtgs2.jpg" data-fallback="None" alt="Paper thumbnail for RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mihnea-Bogdan Jurca, Remco Royen, Ion Giosan, Adrian Munteanu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Point Cloud</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Transformer</span>
<span class="paper-tag">Virtual Reality</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.18033.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mbjurca.github.io/rt-gs2/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/mbjurca/RT_GS2" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting has revolutionized the world of novel view synthesis by achieving high rendering performance in real-time. Recently, studies have focused on enriching these 3D representations with semantic information for downstream tasks. In this paper, we introduce RT-GS2, the first generalizable semantic segmentation method employing Gaussian Splatting. While existing Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2 demonstrates the ability to generalize to unseen scenes. Our method adopts a new approach by first extracting view-independent 3D Gaussian features in a self-supervised manner, followed by a novel View-Dependent / View-Independent (VDVI) feature fusion to enhance semantic consistency over different views. Extensive experimentation on three different datasets showcases RT-GS2's superiority over the state-of-the-art methods in semantic segmentation quality, exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our method achieves real-time performance of 27.03 FPS, marking an astonishing 901 times speedup compared to existing approaches. This work represents a significant advancement in the field by introducing, to the best of our knowledge, the first real-time generalizable semantic segmentation method for 3D Gaussian representations of radiance fields.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024vidu4d" data-title="Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels" data-authors="Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, Jun Zhu" data-year="2024" data-tags='["2DGS", "Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024vidu4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024vidu4d.jpg" data-fallback="None" alt="Paper thumbnail for Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, Jun Zhu</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.16822.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vidu4d-dgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yikaiw/vidu4d" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Video generative models are receiving particular attention given their ability to generate realistic and imaginative frames. Besides, these models are also observed to exhibit strong 3D consistency, significantly enhancing their potential to act as world simulators. In this work, we present Vidu4D, a novel reconstruction model that excels in accurately reconstructing 4D (i.e., sequential 3D) representations from single generated videos, addressing challenges associated with non-rigidity and frame distortion. This capability is pivotal for creating high-fidelity virtual contents that maintain both spatial and temporal coherence. At the core of Vidu4D is our proposed Dynamic Gaussian Surfels (DGS) technique. DGS optimizes time-varying warping functions to transform Gaussian surfels (surface elements) from a static state to a dynamically warped state. This transformation enables a precise depiction of motion and deformation over time. To preserve the structural integrity of surface-aligned Gaussian surfels, we design the warped-state geometric regularization based on continuous warping fields for estimating normals. Additionally, we learn refinements on rotation and scaling parameters of Gaussian surfels, which greatly alleviates texture flickering during the warping process and enhances the capture of fine-grained appearance details. Vidu4D also contains a novel initialization state that provides a proper start for the warping fields in DGS. Equipping Vidu4D with an existing video generative model, the overall framework demonstrates high-fidelity text-to-4D generation in both appearance and geometry.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="paul2024spsup2sup360" data-title="Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors" data-authors="Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen" data-year="2024" data-tags='["Code", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'paul2024spsup2sup360', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/paul2024spsup2sup360.jpg" data-fallback="None" alt="Paper thumbnail for Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Sp<sup>2</sup>360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.16517" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/mvp18/sp2-360" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We aim to tackle sparse-view reconstruction of a 360 3D scene using priors from latent diffusion models (LDM). The sparse-view setting is ill-posed and underconstrained, especially for scenes where the camera rotates 360 degrees around a point, as no visual information is available beyond some frontal views focused on the central object(s) of interest. In this work, we show that pretrained 2D diffusion models can strongly improve the reconstruction of a scene with low-cost fine-tuning. Specifically, we present SparseSplat360 (Sp2360), a method that employs a cascade of in-painting and artifact removal models to fill in missing details and clean novel views. Due to superior training and rendering speeds, we use an explicit scene representation in the form of 3D Gaussians over NeRF-based implicit representations. We propose an iterative update strategy to fuse generated pseudo novel views with existing 3D Gaussians fitted to the initial sparse inputs. As a result, we obtain a multi-view consistent scene representation with details coherent with the observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows that our proposed 2D to 3D distillation algorithm considerably improves the performance of a regularized version of 3DGS adapted to a sparse-view setting and outperforms existing sparse-view reconstruction methods in 360 scene reconstruction. Qualitatively, our method generates entire 360 scenes from as few as 9 input views, with a high degree of foreground and background detail.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="condor2024dsyg" data-title="Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media" data-authors="Jorge Condor, Sebastien Speierer, Lukas Bode, Aljaz Bozic, Simon Green, Piotr Didyk, Adrian Jarabo" data-year="2024" data-tags='["Physics", "Ray Tracing", "Relight", "Rendering", "Project", "Code", "360 degree", "Antialiasing", "Perspective-correct"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'condor2024dsyg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/condor2024dsyg.jpg" data-fallback="None" alt="Paper thumbnail for Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Don't Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jorge Condor, Sebastien Speierer, Lukas Bode, Aljaz Bozic, Simon Green, Piotr Didyk, Adrian Jarabo</p>
      <div class="paper-tags"><span class="paper-tag">Physics</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">360 degree</span>
<span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Perspective-correct</span></div>
      <div class="paper-links"><a href="https://arcanous98.github.io/assets/data/papers/Gaussian_tracing_meta_TOG-compressed.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://arcanous98.github.io/projectPages/gaussianVolumes.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/facebookresearch/volumetric_primitives" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Banking on the popularity of rasterized 3D Gaussian Splatting methods, we formalize the ray-tracing of volumes composed of kernel mixture models (Gaussian or otherwise). Our physically-based, path-traced formulation allows us to render and optimize both scattering and emissive volumes, as well as radiance fields, in an extremely efficient and compact manner. We also introduce the Epanechnikov kernel as an efficient alternative for the Gaussian kernel in radiance field rendering, and showcase the advantages of a ray-traced framework, while maintaining real-time performance. </div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024dogs" data-title="DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus" data-authors="Yu Chen, Gim Hee Lee" data-year="2024" data-tags='["Code", "Large-Scale", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024dogs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024dogs.jpg" data-fallback="None" alt="Paper thumbnail for DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yu Chen, Gim Hee Lee</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.13943.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://aibluefisher.github.io/DOGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/AIBluefisher/DOGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into K blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DOGS maintains one global 3DGS model on the master node and K local 3DGS models on the slave nodes. The K local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our code is publicly available at https://github.com/AIBluefisher/DOGS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024garmentdreamer" data-title="GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details" data-authors="Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, Chenfanfu Jiang" data-year="2024" data-tags='["Avatar", "Code", "Dynamic", "Project", "Rendering", "Texturing", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024garmentdreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024garmentdreamer.jpg" data-fallback="None" alt="Paper thumbnail for GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and Texture Details <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, Chenfanfu Jiang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Texturing</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.12420.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xuan-li.github.io/GarmentDreamerDemo/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/boqian-li/GarmentDreamer" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://xuan-li.github.io/GarmentDreamerDemo/dance.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024gaussian" data-title="Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping" data-authors="Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli" data-year="2024" data-tags='["Avatar", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.12069.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussian-head-shoulders.netlify.app/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dalal2024gaussian" data-title="Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review" data-authors="Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dalal2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dalal2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.03417" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chu2024dreamscene4d" data-title="DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos" data-authors="Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki" data-year="2024" data-tags='["Code", "Dynamic", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chu2024dreamscene4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chu2024dreamscene4d.jpg" data-fallback="None" alt="Paper thumbnail for DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Wen-Hsuan Chu, Lei Ke, Katerina Fragkiadaki</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2405.02280" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dreamscene4d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/dreamscene4d/dreamscene4d" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a "decompose-then-recompose" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="peng2024rtgslam" data-title="RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting" data-authors="Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou" data-year="2024" data-tags='["Code", "Project", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'peng2024rtgslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/peng2024rtgslam.jpg" data-fallback="None" alt="Paper thumbnail for RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.19706" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gapszju.github.io/RTG-SLAM/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/MisEty/RTG-SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ye20243d" data-title="3D Gaussian Splatting with Deferred Reflection" data-authors="Keyang Ye, Qiming Hou, Kun Zhou" data-year="2024" data-tags='["Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ye20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ye20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Gaussian Splatting with Deferred Reflection" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Gaussian Splatting with Deferred Reflection <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Keyang Ye, Qiming Hou, Kun Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.18454.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gapszju.github.io/3DGS-DR/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/gapszju/3DGS-DR" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/3SsQZNXQBs8" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ni2024phyrecon" data-title="PhyRecon: Physically Plausible Neural Scene Reconstruction" data-authors="Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, Siyuan Huang" data-year="2024" data-tags='["Code", "Dynamic", "Meshing", "Physics", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ni2024phyrecon', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ni2024phyrecon.jpg" data-fallback="None" alt="Paper thumbnail for PhyRecon: Physically Plausible Neural Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PhyRecon: Physically Plausible Neural Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, Siyuan Huang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.16666.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://phyrecon.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/PhyRecon/PhyRecon" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=ZXe3bGJiL_k" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We address the issue of physical implausibility in multi-view neural reconstruction. While implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, limiting their utility in domains requiring rigorous physical accuracy. This lack of plausibility stems from the absence of physics modeling in existing methods and their inability to recover intricate geometrical structures. In this paper, we introduce PHYRECON, the first approach to leverage both differentiable rendering and differentiable physics simulation to learn implicit surface representations. PHYRECON features a novel differentiable particle-based physical simulator built on neural implicit representations. Central to this design is an efficient transformation between SDF-based implicit representations and explicit surface points via our proposed Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Additionally, PHYRECON models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors. The physical uncertainty further facilitates physics-guided pixel sampling to enhance the learning of slender structures. By integrating these techniques, our model supports differentiable joint modeling of appearance, geometry, and physics. Extensive experiments demonstrate that PHYRECON significantly improves the reconstruction quality. Our results also exhibit superior physical stability in physical simulators, with at least a 40% improvement across all datasets, paving the way for future physics-based applications.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2024guess" data-title="Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses" data-authors="Inhee Lee, Byungjun Kim, Hanbyul Joo" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2024guess', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2024guess.jpg" data-fallback="None" alt="Paper thumbnail for Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Inhee Lee, Byungjun Kim, Hanbyul Joo</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.14410" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://snuvclab.github.io/gtu/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/snuvclab/gtu/" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/l9c_rd4hmFI" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="c2024contrastive" data-title="Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation" data-authors="Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue" data-year="2024" data-tags='["Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'c2024contrastive', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/c2024contrastive.jpg" data-fallback="None" alt="Paper thumbnail for Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue</p>
      <div class="paper-tags"><span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.12784" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before α blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and α blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by +8% over the state of the art. Code and trained models will be released upon acceptance.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024infusion" data-title="InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior" data-authors="Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, Yang Cao" data-year="2024" data-tags='["Code", "Editing", "Inpainting", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024infusion', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024infusion.jpg" data-fallback="None" alt="Paper thumbnail for InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, Yang Cao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Inpainting</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.11613" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://johanan528.github.io/Infusion/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ali-vilab/infusion" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussians have recently emerged as an efficient representation for novel view synthesis. This work studies its editability with a particular focus on the inpainting task, which aims to supplement an incomplete set of 3D Gaussians with additional points for visually harmonious rendering. Compared to 2D inpainting, the crux of inpainting 3D Gaussians is to figure out the rendering-relevant properties of the introduced points, whose optimization largely benefits from their initial 3D positions. To this end, we propose to guide the point initialization with an image-conditioned depth completion model, which learns to directly restore the depth map based on the observed image. Such a design allows our model to fill in depth values at an aligned scale with the original depth, and also to harness strong generalizability from largescale diffusion prior. Thanks to the more accurate depth completion, our approach, dubbed InFusion, surpasses existing alternatives with sufficiently better fidelity and efficiency under various complex scenarios. We further demonstrate the effectiveness of InFusion with several practical applications, such as inpainting with user-specific texture or with novel object insertion.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="oh2024deblurgs" data-title="DeblurGS: Gaussian Splatting for Camera Motion Blur" data-authors="Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee" data-year="2024" data-tags='["Deblurring", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'oh2024deblurgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/oh2024deblurgs.jpg" data-fallback="None" alt="Paper thumbnail for DeblurGS: Gaussian Splatting for Camera Motion Blur" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DeblurGS: Gaussian Splatting for Camera Motion Blur <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</p>
      <div class="paper-tags"><span class="paper-tag">Deblurring</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.11358" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to realworld applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="barthel2024gaussian" data-title="Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks" data-authors="Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'barthel2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/barthel2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting Decoder for 3D‑aware Generative Adversarial Networks <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.10625" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://florian-barthel.github.io/gaussian_decoder/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/fraunhoferhhi/gaussian_gan_decoder" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://florian-barthel.github.io/gaussian_decoder/videos/latent.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">NeRF-based 3D-aware Generative Adversarial Networks like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses several challenges for most 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware Generative Adversarial Networks with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cui2024letsgo" data-title="LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives" data-authors="Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu" data-year="2024" data-tags='["Code", "Large-Scale", "Lidar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cui2024letsgo', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cui2024letsgo.jpg" data-fallback="None" alt="Paper thumbnail for LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiadi Cui, Junming Cao, Fuqiang Zhao, Zhipeng He, Yifan Chen, Yuhui Zhong, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Lidar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.09748.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zhaofuq.github.io/LetsGo/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zhaofuq/LOD-3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/fs42UBKvGRw?si=v1D0kPj1-QEzpMSR" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Large garages are ubiquitous yet intricate scenes that present unique challenges due to their monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction often fail in these environments due to poor correspondence construction. To address these challenges, we introduce LetsGo, a LiDAR-assisted Gaussian splatting framework for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate data acquisition. Using this Polar device, we present the GarageWorld dataset, consisting of eight expansive garage scenes with diverse geometric structures, which will be made publicly available for further research. Our approach demonstrates that LiDAR point clouds collected by the Polar device significantly enhance a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We introduce a novel depth regularizer that effectively eliminates floating artifacts in rendered images. Additionally, we propose a multi-resolution 3D Gaussian representation designed for Level-of-Detail (LOD) rendering. This includes adapted scaling factors for individual levels and a random-resolution-level training scheme to optimize the Gaussians across different resolutions. This representation enables efficient rendering of large-scale garage scenes on lightweight devices via a web-based renderer. Experimental results on our GarageWorld dataset, as well as on ScanNet++ and KITTI-360, demonstrate the superiority of our method in terms of rendering quality and resource efficiency.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kheradmand20243d" data-title="3D Gaussian Splatting as Markov Chain Monte Carlo" data-authors="Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi" data-year="2024" data-tags='["Code", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kheradmand20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kheradmand20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Gaussian Splatting as Markov Chain Monte Carlo" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Gaussian Splatting as Markov Chain Monte Carlo <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.09591.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ubc-vision.github.io/3dgs-mcmc/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ubc-vision/3dgs-mcmc" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which can lead to poor-quality renderings, and reliance on a good initialization. In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene-in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply introducing noise. We then rewrite the densification and pruning strategies in 3D Gaussian Splatting as simply a deterministic state transition of MCMC samples, removing these heuristics from the framework. To do so, we revise the 'cloning' of Gaussians into a relocalization scheme that approximately preserves sample probability. To encourage efficient use of Gaussians, we introduce a regularizer that promotes the removal of unused Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024loopgaussian" data-title="LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field" data-authors="Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He" data-year="2024" data-tags='["Code", "Physics", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024loopgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024loopgaussian.jpg" data-fallback="None" alt="Paper thumbnail for LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.08966" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pokerlishao.github.io/LoopGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Pokerlishao/LoopGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ye2024occgaussian" data-title="OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering" data-authors="Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu" data-year="2024" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ye2024occgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ye2024occgaussian.jpg" data-fallback="None" alt="Paper thumbnail for OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.07991" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://wenj.github.io/GoMAvatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/wenj/GoMAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wen2024gomavatar" data-title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh" data-authors="Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang" data-year="2024" data-tags='["Avatar", "Code", "Monocular", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wen2024gomavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wen2024gomavatar.jpg" data-fallback="None" alt="Paper thumbnail for GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.07991" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://wenj.github.io/GoMAvatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/wenj/GoMAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lyu2024gaga" data-title="Gaga: Group Any Gaussians via 3D-aware Memory Bank" data-authors="Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang" data-year="2024" data-tags='["Code", "Editing", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lyu2024gaga', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lyu2024gaga.jpg" data-fallback="None" alt="Paper thumbnail for Gaga: Group Any Gaussians via 3D-aware Memory Bank" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaga: Group Any Gaussians via 3D-aware Memory Bank <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.07977.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.gaga.gallery/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/weijielyu/Gaga" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=rqs5BuVFOok" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot segmentation models. Contrasted to prior 3D scene segmentation approaches that heavily rely on video object tracking, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot segmentation models, significantly enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as scene understanding and manipulation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shriram2024realmdreamer" data-title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion" data-authors="Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi" data-year="2024" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shriram2024realmdreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shriram2024realmdreamer.jpg" data-fallback="None" alt="Paper thumbnail for RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.07199" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://realmdreamer.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lang2024gaussianlic" data-title="Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting" data-authors="Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv" data-year="2024" data-tags='["Project", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lang2024gaussianlic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lang2024gaussianlic.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06926" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xingxingzuo.github.io/gaussian_lic/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend. Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper. We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting. Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability. Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2024dreamscene360" data-title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting" data-authors="Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2024dreamscene360', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2024dreamscene360.jpg" data-fallback="None" alt="Paper thumbnail for DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06903" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dreamscene360.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ShijieZhou-UCLA/DreamScene360" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/embed/6rMIQfe7b24?si=cm7cZ-T9r5na7YFD" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360∘ scene generation pipeline that facilitates the creation of comprehensive 360∘ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary "flat" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360∘ perspective, providing an enhanced immersive experience over existing techniques.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kruse2024splatpose" data-title="SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection" data-authors="Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn" data-year="2024" data-tags='["Code", "Misc", "Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kruse2024splatpose', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kruse2024splatpose.jpg" data-fallback="None" alt="Paper thumbnail for SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06832" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/m-kruse98/SplatPose" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2024zeroshot" data-title="Zero-shot Point Cloud Completion Via 2D Priors" data-authors="Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee" data-year="2024" data-tags='["Diffusion", "Point Cloud"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2024zeroshot', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2024zeroshot.jpg" data-fallback="None" alt="Paper thumbnail for Zero-shot Point Cloud Completion Via 2D Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Zero-shot Point Cloud Completion Via 2D Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Point Cloud</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06814" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dai2024spikenvs" data-title="SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera" data-authors="Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang" data-year="2024" data-tags='["Deblurring", "Misc"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dai2024spikenvs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dai2024spikenvs.jpg" data-fallback="None" alt="Paper thumbnail for SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang</p>
      <div class="paper-tags"><span class="paper-tag">Deblurring</span>
<span class="paper-tag">Misc</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06710" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">One of the most critical factors in achieving sharp Novel View Synthesis (NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) is the quality of the training images. However, Conventional RGB cameras are susceptible to motion blur. In contrast, neuromorphic cameras like event and spike cameras inherently capture more comprehensive temporal information, which can provide a sharp representation of the scene as additional training data. Recent methods have explored the integration of event cameras to improve the quality of NVS. The event-RGB approaches have some limitations, such as high training costs and the inability to work effectively in the background. Instead, our study introduces a new method that uses the spike camera to overcome these limitations. By considering texture reconstruction from spike streams as ground truth, we design the Texture from Spike (TfS) loss. Since the spike camera relies on temporal integration instead of temporal differentiation used by event cameras, our proposed TfS loss maintains manageable training costs. It handles foreground objects with backgrounds simultaneously. We also provide a real-world dataset captured with our spike-RGB camera system to facilitate future research endeavors. We conduct extensive experiments using synthetic and real-world datasets to demonstrate that our design can enhance novel view synthesis across NeRF and 3DGS.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024endtoend" data-title="End-to-End Rate-Distortion Optimized 3D Gaussian Representation" data-authors="Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, Zhibo Chen" data-year="2024" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024endtoend', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024endtoend.jpg" data-fallback="None" alt="Paper thumbnail for End-to-End Rate-Distortion Optimized 3D Gaussian Representation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">End-to-End Rate-Distortion Optimized 3D Gaussian Representation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, Zhibo Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2406.01597.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://rdogaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/USTC-IMCL/RDO-Gaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40×, and surpasses existing methods in rate-distortion performance.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lu20243d" data-title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis" data-authors="Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai" data-year="2024" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lu20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lu20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06270" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://npucvr.github.io/GaGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zhichengLuxx/GaGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bonilla2024gaussian" data-title="Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction" data-authors="Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano" data-year="2024" data-tags='["Code", "Medicine", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bonilla2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bonilla2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Medicine</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06128" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://papers.miccai.org/miccai-2024/349-Paper2298.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/smbonilla/GaussianPancakes" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting (3D GS) combined with a Recurrent Neural Network-based Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="rota2024revising" data-title="Revising Densification in Gaussian Splatting" data-authors="Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder" data-year="2024" data-tags='["Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'rota2024revising', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/rota2024revising.jpg" data-fallback="None" alt="Paper thumbnail for Revising Densification in Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Revising Densification in Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06109" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and pruning, however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of benchmark scenes, without sacrificing the method's efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2024hash3d" data-title="Hash3D: Training-free Acceleration for 3D Generation" data-authors="Xingyi Yang, Xinchao Wang" data-year="2024" data-tags='["Acceleration", "Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2024hash3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2024hash3d.jpg" data-fallback="None" alt="Paper thumbnail for Hash3D: Training-free Acceleration for 3D Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Hash3D: Training-free Acceleration for 3D Generation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xingyi Yang, Xinchao Wang</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.06091" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://adamdad.github.io/hash3D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Adamdad/hash3D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024stylizedgs" data-title="StylizedGS: Controllable Stylization for 3D Gaussian Splatting" data-authors="Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao" data-year="2024" data-tags='["Rendering", "Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024stylizedgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024stylizedgs.jpg" data-fallback="None" alt="Paper thumbnail for StylizedGS: Controllable Stylization for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">StylizedGS: Controllable Stylization for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao</p>
      <div class="paper-tags"><span class="paper-tag">Rendering</span>
<span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.05220" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">With the rapid development of XR, 3D generation and editing are becoming more and more important, among which, stylization is an important tool of 3D appearance editing. It can achieve consistent 3D artistic stylization given a single reference style image and thus is a user-friendly editing way. However, recent NeRF-based 3D stylization methods face efficiency issues that affect the actual user experience and the implicit nature limits its ability to transfer the geometric pattern styles. Additionally, the ability for artists to exert flexible control over stylized scenes is considered highly desirable, fostering an environment conducive to creative exploration. In this paper, we introduce StylizedGS, a 3D neural style transfer framework with adaptable control over perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The 3DGS brings the benefits of high efficiency. We propose a GS filter to eliminate floaters in the reconstruction which affects the stylization effects before stylization. Then the nearest neighbor-based style loss is introduced to achieve stylization by fine-tuning the geometry and color parameters of 3DGS, while a depth preservation loss with other regularizations is proposed to prevent the tampering of geometry content. Moreover, facilitated by specially designed losses, StylizedGS enables users to control color, stylized scale and regions during the stylization to possess customized capabilities. Our method can attain high-quality stylization results characterized by faithful brushstrokes and geometric consistency with flexible controls. Extensive experiments across various scenes and styles demonstrate the effectiveness and efficiency of our method concerning both stylization quality and inference FPS.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024dualcamera" data-title="Dual-Camera Smooth Zoom on Mobile Phones" data-authors="Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo" data-year="2024" data-tags='["Code", "Misc", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024dualcamera', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024dualcamera.jpg" data-fallback="None" alt="Paper thumbnail for Dual-Camera Smooth Zoom on Mobile Phones" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Dual-Camera Smooth Zoom on Mobile Phones <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.04908" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dualcamerasmoothzoom.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ZcsrenlongZ/ZoomGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">When zooming between dual cameras on a mobile, noticeable jumps in geometric content and image color occur in the preview, inevitably affecting the user's zoom experience. In this work, we introduce a new task, ie, dual-camera smooth zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI) technique is a potential solution but struggles with ground-truth collection. To address the issue, we suggest a data factory solution where continuous virtual cameras are assembled to generate DCSZ data by rendering reconstructed 3D models of the scene. In particular, we propose a novel dual-camera smooth zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is introduced to construct a specific 3D model for each virtual camera. With the proposed data factory, we construct a synthetic dataset for DCSZ, and we utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom images without ground-truth for evaluation. Extensive experiments are conducted with multiple FI methods. The results show that the fine-tuned FI models achieve a significant performance improvement over the original ones on DCSZ task.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="darmon2024robust" data-title="Robust Gaussian Splatting" data-authors="François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder" data-year="2024" data-tags='["Deblurring", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'darmon2024robust', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/darmon2024robust.jpg" data-fallback="None" alt="Paper thumbnail for Robust Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Robust Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</p>
      <div class="paper-tags"><span class="paper-tag">Deblurring</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.04211" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024mmgaussian" data-title="MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes" data-authors="Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang" data-year="2024" data-tags='["Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024mmgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024mmgaussian.jpg" data-fallback="None" alt="Paper thumbnail for MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.04026" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024sc4d" data-title="SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer" data-authors="Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024sc4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024sc4d.jpg" data-fallback="None" alt="Paper thumbnail for SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.03736" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://sc4d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JarrentWu1031/SC4D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/SkpTEuX4B5c?si=yvrF_iRHnMQR9TD0" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in 2D/3D generative models enable the generation of dynamic 3D objects from a single-view video. Existing approaches utilize score distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D Gaussians. However, these methods struggle to strike a balance among reference view alignment, spatio-temporal consistency, and motion fidelity under single-view conditions due to the implicit nature of NeRF or the intricate dense Gaussian motion prediction. To address these issues, this paper proposes an efficient, sparse-controlled video-to-4D framework named SC4D, that decouples motion and appearance to achieve superior video-to-4D generation. Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity of the learned motion and shape. Comprehensive experimental results demonstrate that our method surpasses existing methods in both quality and efficiency. In addition, facilitated by the disentangled modeling of motion and appearance of SC4D, we devise a novel application that seamlessly transfers the learned motion onto a diverse array of 4D entities according to textual descriptions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bae2024pergaussian" data-title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting" data-authors="Jeongmin Bae*, Seoha Kim*, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh" data-year="2024" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bae2024pergaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bae2024pergaussian.jpg" data-fallback="None" alt="Paper thumbnail for Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jeongmin Bae*, Seoha Kim*, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.03613" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jeongminb.github.io/e-d3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JeongminB/E-D3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames for representing a dynamic scene. However, previous works fail to accurately reconstruct complex dynamic scenes. We attribute the failure to the design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce a local smoothness regularization for per-Gaussian embedding to improve the details in dynamic regions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024dreamscene" data-title="DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling" data-authors="Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik Hang Lee, Pengyuan Zhou" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024dreamscene', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024dreamscene.jpg" data-fallback="None" alt="Paper thumbnail for DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik Hang Lee, Pengyuan Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.03575.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dreamscene-project.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/DreamScene-Project/DreamScene" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors, increasingly capturing the attention of both academic and industry circles. Despite significant progress, current methods still struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework that leverages Formation Pattern Sampling (FPS) for core structuring, augmented with a strategic camera sampling and supported by holistic object-environment integration to overcome these hurdles. FPS, guided by the formation patterns of 3D objects, employs multi-timesteps sampling to quickly form semantically rich, high-quality representations, uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. The camera sampling strategy incorporates a progressive three-stage approach, specifically designed for both indoor and outdoor settings, to effectively ensure scene-wide 3D consistency. DreamScene enhances scene editing flexibility by combining objects and environments, enabling targeted adjustments. Extensive experiments showcase DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="meng2024omnigs" data-title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images" data-authors="Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma" data-year="2024" data-tags='["360 degree", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'meng2024omnigs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/meng2024omnigs.jpg" data-fallback="None" alt="Paper thumbnail for OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</p>
      <div class="paper-tags"><span class="paper-tag">360 degree</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.03202" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://liquorleaf.github.io/research/OmniGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="nikolakakis2024gaspct" data-title="GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis" data-authors="Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu" data-year="2024" data-tags='["Medicine"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'nikolakakis2024gaspct', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/nikolakakis2024gaspct.jpg" data-fallback="None" alt="Paper thumbnail for GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu</p>
      <div class="paper-tags"><span class="paper-tag">Medicine</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.03126" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhao2024tclcgs" data-title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes" data-authors="Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren" data-year="2024" data-tags='["Autonomous Driving", "Lidar", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhao2024tclcgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhao2024tclcgs.jpg" data-fallback="None" alt="Paper thumbnail for TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Lidar</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.02410.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.youtube.com/watch?v=CEo6mZ6FGg0" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian's properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method's state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wolf2024gs2mesh" data-title="GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views" data-authors="Yaniv Wolf, Amit Bracha, Ron Kimmel" data-year="2024" data-tags='["2DGS", "Code", "Meshing", "Project", "Stereo", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wolf2024gs2mesh', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wolf2024gs2mesh.jpg" data-fallback="None" alt="Paper thumbnail for GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yaniv Wolf, Amit Bracha, Ron Kimmel</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Stereo</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.01810" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gs2mesh.github.io//" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yanivw12/gs2mesh" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/cjtmLDD8YZk" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for accurately representing scenes. However, despite its superior novel view synthesis capabilities, extracting the geometry of the scene directly from the Gaussian properties remains a challenge, as those are optimized based on a photometric loss. While some concurrent models have tried adding geometric constraints during the Gaussian optimization process, they still produce noisy, unrealistic surfaces. We propose a novel approach for bridging the gap between the noisy 3DGS representation and the smooth 3D mesh representation, by injecting real-world knowledge into the depth extraction process. Instead of extracting the geometry of the scene directly from the Gaussian properties, we instead extract the geometry through a pre-trained stereo-matching model. We render stereo-aligned pairs of images corresponding to the original training poses, feed the pairs into a stereo model to get a depth profile, and finally fuse all of the profiles together to get a single mesh. The resulting reconstruction is smoother, more accurate and shows more intricate details compared to other methods for surface reconstruction from Gaussian Splatting, while only requiring a small overhead on top of the fairly short 3DGS optimization process. We performed extensive testing of the proposed method on in-the-wild scenes, obtained using a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the method on the Tanks and Temples and DTU benchmarks, achieving state-of-the-art results.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qiu2024feature" data-title="Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing" data-authors="Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang" data-year="2024" data-tags='["Code", "Editing", "Language Embedding", "Physics", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qiu2024feature', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qiu2024feature.jpg" data-fallback="None" alt="Paper thumbnail for Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.01223" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://feature-splatting.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/vuer-ai/feature_splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://feature-splatting.github.io/resources/teaser_overview.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="meng2024mirror3dgs" data-title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting" data-authors="Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma" data-year="2024" data-tags='["Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'meng2024mirror3dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/meng2024mirror3dgs.jpg" data-fallback="None" alt="Paper thumbnail for Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.01168.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mirror-gaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method's ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024citygaussian" data-title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians" data-authors="Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, Zhaoxiang Zhang" data-year="2024" data-tags='["Code", "Large-Scale", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024citygaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024citygaussian.jpg" data-fallback="None" alt="Paper thumbnail for CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Naiyan Wang, Junran Peng, Zhaoxiang Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.01133" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dekuliutesla.github.io/citygs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/DekuLiuTesla/CityGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shao2024haha" data-title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior" data-authors="Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang" data-year="2024" data-tags='["Avatar", "Code"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shao2024haha', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shao2024haha.jpg" data-fallback="None" alt="Paper thumbnail for HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.01053" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/david-svitov/HAHA" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="c2024mm3dgs" data-title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements" data-authors="Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu" data-year="2024" data-tags='["Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'c2024mm3dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/c2024mm3dgs.jpg" data-fallback="None" alt="Paper thumbnail for MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.00923" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vita-group.github.io/MM3DGS-SLAM/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/MM3DGS-SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=drf6UxehChE&feature=youtu.be" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="comi20243dgsr" data-title="3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting" data-authors="Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison" data-year="2024" data-tags='["Meshing", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'comi20243dgsr', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/comi20243dgsr.jpg" data-fallback="None" alt="Paper thumbnail for 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.00409.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fan2024instantsplat" data-title="InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds" data-authors="Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang" data-year="2024" data-tags='["Code", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fan2024instantsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fan2024instantsplat.jpg" data-fallback="None" alt="Paper thumbnail for InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.20309.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://instantsplat.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NVlabs/InstantSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/live/JdfrG89iPOA?si=JhoiMxrjVIh91Ws1" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While novel view synthesis (NVS) has made substantial progress in 3D computer vision, it typically requires an initial estimation of camera intrinsics and extrinsics from dense viewpoints. This pre-processing is usually conducted via a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and unreliable, particularly in sparse-view scenarios with insufficient matched features for accurate reconstruction. In this work, we integrate the strengths of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved issues in NVS under unconstrained settings, which encompasses pose-free and sparse view challenges. Our framework, InstantSplat, unifies dense stereo priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview & pose-free images in less than 1 minute. Specifically, InstantSplat comprises a Coarse Geometric Initialization (CGI) module that swiftly establishes a preliminary scene structure and camera parameters across all training views, utilizing globally-aligned 3D point maps derived from a pre-trained dense stereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO) module, which jointly optimizes the 3D Gaussian attributes and the initialized poses with pose regularization. Experiments conducted on the large-scale outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error (ATE) by 80%. These establish InstantSplat as a viable solution for scenarios involving posefree and sparse-view conditions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="comi2024snapit" data-title="Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces" data-authors="Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison" data-year="2024" data-tags='["Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'comi2024snapit', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/comi2024snapit.jpg" data-fallback="None" alt="Paper thumbnail for Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.20275" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://maxyang27896.github.io/publication/gaussian_splat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object's geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024hgsmapping" data-title="HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes" data-authors="Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding" data-year="2024" data-tags='["Large-Scale"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024hgsmapping', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024hgsmapping.jpg" data-fallback="None" alt="Paper thumbnail for HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation in Urban Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.20159.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2024sgd" data-title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior" data-authors="Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun" data-year="2024" data-tags='["Diffusion", "Lidar", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2024sgd', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2024sgd.jpg" data-fallback="None" alt="Paper thumbnail for SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Lidar</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.20079.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024hogaussian" data-title="HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes" data-authors="Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, Liangjun Zhang" data-year="2024" data-tags='["Densification", "Misc"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024hogaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024hogaussian.jpg" data-fallback="None" alt="Paper thumbnail for HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, Liangjun Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span>
<span class="paper-tag">Misc</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.20032.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural rendering, enabling real-time production of high-quality renderings. However, the previous 3DGS-based methods have limitations in urban scenes due to reliance on initial Structure-from-Motion(SfM) points and difficulties in rendering distant, sky and low-texture areas. To overcome these challenges, we propose a hybrid optimization method named HO-Gaussian, which combines a grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency on SfM point initialization, allowing for rendering of urban scenes, and incorporates the Point Densitification to enhance rendering quality in problematic regions during training. Furthermore, we introduce Gaussian Direction Encoding as an alternative for spherical harmonics in the rendering pipeline, which enables view-dependent color representation. To account for multi-camera systems, we introduce neural warping to enhance object consistency across different cameras. Experimental results on widely used autonomous driving datasets demonstrate that HO-Gaussian achieves photo-realistic rendering in real-time on multi-camera urban datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024gaussiancube" data-title="GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling" data-authors="Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024gaussiancube', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024gaussiancube.jpg" data-fallback="None" alt="Paper thumbnail for GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianCube: A Structured and Explicit Radiance Representation for 3D Generative Modeling <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.19655.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussiancube.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/GaussianCube/GaussianCube" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/3uo4Oud4cxI" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce a radiance representation that is both structured and fully explicit and thus greatly facilitates 3D generative modeling. Existing radiance representations either require an implicit feature decoder, which significantly degrades the modeling power of the representation, or are spatially unstructured, making them difficult to integrate with mainstream 3D diffusion methods. We derive GaussianCube by first using a novel densification-constrained Gaussian fitting algorithm, which yields high-accuracy fitting using a fixed number of free Gaussians, and then rearranging these Gaussians into a predefined voxel grid via Optimal Transport. Since GaussianCube is a structured grid representation, it allows us to use standard 3D U-Net as our backbone in diffusion modeling without elaborate designs. More importantly, the high-accuracy fitting of the Gaussians allows us to achieve a high-quality representation with orders of magnitude fewer parameters than previous structured representations for comparable quality, ranging from one to two orders of magnitude. The compactness of GaussianCube greatly eases the difficulty of 3D generative modeling. Extensive experiments conducted on unconditional and class-conditioned object generation, digital avatar creation, and text-to-3D synthesis all show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a highly accurate and versatile radiance representation for 3D generative modeling.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="song2024sags" data-title="SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing" data-authors="Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao" data-year="2024" data-tags='["Antialiasing", "Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'song2024sags', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/song2024sags.jpg" data-fallback="None" alt="Paper thumbnail for SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</p>
      <div class="paper-tags"><span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.19615" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kevinsong729.github.io/project-pages/SA-GS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zsy1987/SA-GS/" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024togs" data-title="TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering" data-authors="Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu" data-year="2024" data-tags='["Code", "Medicine"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024togs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024togs.jpg" data-fallback="None" alt="Paper thumbnail for TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Medicine</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.19586" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/hustvl/TOGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="paliwal2024coherentgs" data-title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians" data-authors="Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari" data-year="2024" data-tags='["Code", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'paliwal2024coherentgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/paliwal2024coherentgs.jpg" data-fallback="None" alt="Paper thumbnail for CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.19495" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/avinashpaliwal/CoherentGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=qiWdD3tOHKM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shen2024gamba" data-title="Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction" data-authors="Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang" data-year="2024" data-tags='["Code", "Feed-Forward", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shen2024gamba', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shen2024gamba.jpg" data-fallback="None" alt="Paper thumbnail for Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.18795" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://florinshen.github.io/gamba-project/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/SkyworkAI/Gamba" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization design, and training methodologies. We assessed Gamba against existing optimization-based and feed-forward 3D generation approaches using the real-world scanned OmniObject3D dataset. Here, Gamba demonstrates competitive generation capabilities, both qualitatively and quantitatively, while achieving remarkable speed, approximately 0.6 second on a single NVIDIA A100 GPU.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shao2024splatface" data-title="SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface" data-authors="Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang" data-year="2024" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shao2024splatface', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shao2024splatface.jpg" data-fallback="None" alt="Paper thumbnail for SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.18784" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present SplatFace, a novel Gaussian splatting framework designed for 3D human face reconstruction without reliance on accurate pre-determined geometry. Our method is designed to simultaneously deliver both high-quality novel view rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D Morphable Model (3DMM) to provide a surface geometric structure, making it possible to reconstruct faces with a limited set of input images. We introduce a joint optimization strategy that refines both the Gaussians and the morphable surface through a synergistic non-rigid alignment process. A novel distance metric, splat-to-surface, is proposed to improve alignment by considering both the Gaussian position and covariance. The surface information is also utilized to incorporate a world-space densification process, resulting in superior reconstruction quality. Our experimental analysis demonstrates that the proposed method is competitive with both other Gaussian splatting techniques in novel view synthesis and other 3D reconstruction methods in producing 3D face meshes with high geometric precision.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="savant2024modeling" data-title="Modeling uncertainty for Gaussian Splatting" data-authors="Luca Savant, Diego Valsesia, Enrico Magli" data-year="2024" data-tags='["Uncertainty"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'savant2024modeling', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/savant2024modeling.jpg" data-fallback="None" alt="Paper thumbnail for Modeling uncertainty for Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Modeling uncertainty for Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Luca Savant, Diego Valsesia, Enrico Magli</p>
      <div class="paper-tags"><span class="paper-tag">Uncertainty</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.18476" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gu2024egolifter" data-title="EgoLifter: Open-world 3D Segmentation for Egocentric Perception" data-authors="Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney" data-year="2024" data-tags='["Code", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gu2024egolifter', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gu2024egolifter.jpg" data-fallback="None" alt="Paper thumbnail for EgoLifter: Open-world 3D Segmentation for Egocentric Perception" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EgoLifter: Open-world 3D Segmentation for Egocentric Perception <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.18118.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://egolifter.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/facebookresearch/egolifter" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=dWuZyeiOXyM&feature=youtu.be" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ren2024octreegs" data-title="Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians" data-authors="Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai" data-year="2024" data-tags='["Code", "Large-Scale", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ren2024octreegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ren2024octreegs.jpg" data-fallback="None" alt="Paper thumbnail for Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.17898" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://city-super.github.io/octree-gs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/city-super/Octree-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang20242d" data-title="2D Gaussian Splatting for Geometrically Accurate Radiance Fields" data-authors="Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao" data-year="2024" data-tags='["2DGS", "Code", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang20242d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang20242d.jpg" data-fallback="None" alt="Paper thumbnail for 2D Gaussian Splatting for Geometrically Accurate Radiance Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">2D Gaussian Splatting for Geometrically Accurate Radiance Fields <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.17888" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://surfsplatting.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hbb1/2d-gaussian-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=oaHCtB6yiKU" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="turkulainen2024dnsplatter" data-title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing" data-authors="Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala" data-year="2024" data-tags='["Code", "Meshing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'turkulainen2024dnsplatter', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/turkulainen2024dnsplatter.jpg" data-fallback="None" alt="Paper thumbnail for DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.17822" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://maturk.github.io/dn-splatter/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/maturk/dn-splatter" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lin2024dreampolisher" data-title="DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion" data-authors="Yuanze Lin, Ronald Clark, Philip Torr" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lin2024dreampolisher', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lin2024dreampolisher.jpg" data-fallback="None" alt="Paper thumbnail for DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuanze Lin, Ronald Clark, Philip Torr</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.17237" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yuanze-lin.me/DreamPolisher_page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yuanze-lin/DreamPolisher" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/YJkFMIV2OyQ" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024comp4d" data-title="Comp4D: LLM-Guided Compositional 4D Scene Generation" data-authors="Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024comp4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024comp4d.jpg" data-fallback="None" alt="Paper thumbnail for Comp4D: LLM-Guided Compositional 4D Scene Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Comp4D: LLM-Guided Compositional 4D Scene Generation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.16993.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vita-group.github.io/Comp4D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/Comp4D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/9q8SV1Xf_Xw" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content. However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation. To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation. Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately. Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories. It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths. To refine the scene, our method employs a compositional score distillation technique guided by the pre-defined trajectories, utilizing pre-trained diffusion models across text-to-image, text-to-video, and text-to-3D domains. Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2024gsdf" data-title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction" data-authors="Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai" data-year="2024" data-tags='["Code", "Meshing", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2024gsdf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2024gsdf.jpg" data-fallback="None" alt="Paper thumbnail for GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.16964" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://city-super.github.io/GSDF/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/city-super/GSDF" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wewer2024latentsplat" data-title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction" data-authors="Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen" data-year="2024" data-tags='["Feed-Forward", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wewer2024latentsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wewer2024latentsplat.jpg" data-fallback="None" alt="Paper thumbnail for latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</p>
      <div class="paper-tags"><span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.16292.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Chrixtar/latentsplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hu2024cgslam" data-title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field" data-authors="Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui" data-year="2024" data-tags='["Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hu2024cgslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hu2024cgslam.jpg" data-fallback="None" alt="Paper thumbnail for CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.16095" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zju3dv.github.io/cg-slam/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://raw.githubusercontent.com/hjr37/open_access_assets/main/cg-slam/video/cg-slam-show.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024gaussian" data-title="Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections" data-authors="Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian Wang" data-year="2024" data-tags='["Code", "In the Wild", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">In the Wild</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.15704" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://eastbeanzhang.github.io/GS-W/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/EastbeanZhang/Gaussian-Wild" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=BNIX-OmIzgo" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis from unconstrained in-the-wild images remains a meaningful but challenging task. The photometric variation and transient occluders in those unconstrained images make it difficult to reconstruct the original scene accurately. Previous approaches tackle the problem by introducing a global appearance feature in Neural Radiance Fields (NeRF). However, in the real world, the unique appearance of each tiny point in a scene is determined by its independent intrinsic material attributes and the varying environmental impacts it receives. Inspired by this fact, we propose Gaussian in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the scene and introduces separated intrinsic and dynamic appearance feature for each point, capturing the unchanged scene appearance along with dynamic variation like illumination and weather. Additionally, an adaptive sampling strategy is presented to allow each Gaussian point to focus on the local and detailed information more effectively. We also reduce the impact of transient occluders using a 2D visibility map. More experiments have demonstrated better reconstruction quality and details of GS-W compared to previous methods, with a 1000× increase in rendering speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guo2024semantic" data-title="Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting" data-authors="Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guo2024semantic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guo2024semantic.jpg" data-fallback="None" alt="Paper thumbnail for Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.15624" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://semantic-gaussians.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/sharinka0715/semantic-gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, withwide-ranging applications in embodied agents and augmented reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs) to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design a versatile projection approachthat maps various 2Dsemantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, withoutthe additional training required by NeRFs. We further build a 3D semantic network that directly predictsthe semantic component from raw 3D Gaussians for fast inference. We explore several applications ofSemantic Gaussians: semantic segmentation on ScanNet-20, where our approach attains a 4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene understanding counterparts; object part segmentation,sceneediting, and spatial-temporal segmentation with better qualitative results over 2D and 3D baselines,highlighting its versatility and effectiveness on supporting diverse downstream tasks.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024pixelgs" data-title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting" data-authors="Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao" data-year="2024" data-tags='["Code", "Densification", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024pixelgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024pixelgs.jpg" data-fallback="None" alt="Paper thumbnail for Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.15530.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pixelgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zhengzhang01/Pixel-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024endogslam" data-title="EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting" data-authors="Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen" data-year="2024" data-tags='["Code", "Medicine", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024endogslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024endogslam.jpg" data-fallback="None" alt="Paper thumbnail for EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Medicine</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.15124.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://endogslam.loping151.com/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/endogslam/EndoGSLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://loping151.github.io/endogslam/static/video/3_1.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zeng2024stag4d" data-title="STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians" data-authors="Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zeng2024stag4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zeng2024stag4d.jpg" data-fallback="None" alt="Paper thumbnail for STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14939" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nju-3dv.github.io/projects/STAG4D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zeng-yifei/STAG4D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=YJkFMIV2OyQ" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024mvsplat" data-title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images" data-authors="Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai" data-year="2024" data-tags='["Code", "Feed-Forward", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024mvsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024mvsplat.jpg" data-fallback="None" alt="Paper thumbnail for MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14627" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://donydchen.github.io/mvsplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/donydchen/mvsplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitives' opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses 10× fewer parameters and infers more than 2× faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024grm" data-title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation" data-authors="Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein" data-year="2024" data-tags='["Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024grm', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024grm.jpg" data-fallback="None" alt="Paper thumbnail for GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14621.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://justimyhxu.github.io/projects/grm/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guédon2024gaussian" data-title="Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering" data-authors="Antoine Guédon, Vincent Lepetit" data-year="2024" data-tags='["Code", "Dynamic", "Editing", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guédon2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guédon2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Antoine Guédon, Vincent Lepetit</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14554" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://anttwo.github.io/frosting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Anttwo/Frosting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/h7LeWq8sG78" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024hac" data-title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression" data-authors="Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai" data-year="2024" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024hac', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024hac.jpg" data-fallback="None" alt="Paper thumbnail for HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14530.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yihangchen-ee.github.io/project_hac/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/YihangChen-ee/HAC" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kim2024synctweedies" data-title="SyncTweedies: A General Generative Framework Based on Synchronized Diffusions" data-authors="Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kim2024synctweedies', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kim2024synctweedies.jpg" data-fallback="None" alt="Paper thumbnail for SyncTweedies: A General Generative Framework Based on Synchronized Diffusions" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SyncTweedies: A General Generative Framework Based on Synchronized Diffusions <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14370" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://synctweedies.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/KAIST-Visual-AI-Group/SyncTweedies" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie's formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fang2024minisplatting" data-title="Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians" data-authors="Guangchi Fang, Bing Wang" data-year="2024" data-tags='["Code", "Densification", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fang2024minisplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fang2024minisplatting.jpg" data-fallback="None" alt="Paper thumbnail for Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guangchi Fang, Bing Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.14166.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/fatPeter/mini-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="niemeyer2024radsplat" data-title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS" data-authors="Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari" data-year="2024" data-tags='["Densification", "Misc", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'niemeyer2024radsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/niemeyer2024radsplat.jpg" data-fallback="None" alt="Paper thumbnail for RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.13806.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://m-niemeyer.github.io/radsplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="seiskari2024gaussian" data-title="Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion" data-authors="Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin" data-year="2024" data-tags='["Code", "Deblurring", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'seiskari2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/seiskari2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Deblurring</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.13327.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://spectacularai.github.io/3dgs-deblur/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/SpectacularAI/3dgs-deblur" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=GwhNerXMLd4&feature=youtu.be" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">High-quality scene reconstruction and novel view synthesis based on Gaussian Splatting (3DGS) typically require steady, high-quality photographs, often impractical to capture with handheld cameras. We present a method that adapts to camera motion and allows high-quality scene reconstruction with handheld video data suffering from motion blur and rolling shutter distortion. Our approach is based on detailed modelling of the physical image formation process and utilizes velocities estimated using visual-inertial odometry (VIO). Camera poses are considered non-static during the exposure time of a single image frame and camera poses are further optimized in the reconstruction process. We formulate a differentiable rendering pipeline that leverages screen space approximation to efficiently incorporate rolling-shutter and motion blur effects into the 3DGS framework. Our results with both synthetic and real data demonstrate superior performance in mitigating camera motion over existing methods, thereby advancing 3DGS in naturalistic settings.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="he2024gvgen" data-title="GVGEN: Text-to-3D Generation with Volumetric Representation" data-authors="Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'he2024gvgen', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/he2024gvgen.jpg" data-fallback="None" alt="Paper thumbnail for GVGEN: Text-to-3D Generation with Volumetric Representation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GVGEN: Text-to-3D Generation with Volumetric Representation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.12957" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gvgen.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/GVGEN/GVGEN" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed (∼7 seconds), effectively striking a balance between quality and efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2023hugs" data-title="HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting" data-authors="Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao" data-year="2023" data-tags='["Autonomous Driving", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2023hugs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2023hugs.jpg" data-fallback="None" alt="Paper thumbnail for HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.12722.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xdimlab.github.io/hugs_website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hyzhou404/HUGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ha2024rgbd" data-title="RGBD GS-ICP SLAM" data-authors="Seongbo Ha, Jiung Yeon, Hyeonwoo Yu" data-year="2024" data-tags='["Code", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ha2024rgbd', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ha2024rgbd.jpg" data-fallback="None" alt="Paper thumbnail for RGBD GS-ICP SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RGBD GS-ICP SLAM <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Seongbo Ha, Jiung Yeon, Hyeonwoo Yu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.12550.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=e-bHh_uMMxE" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun2024highfidelity" data-title="High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization" data-authors="Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson" data-year="2024" data-tags='["SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun2024highfidelity', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun2024highfidelity.jpg" data-fallback="None" alt="Paper thumbnail for High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.12535.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gao2024gaussianflow" data-title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation" data-authors="Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann" data-year="2024" data-tags='["Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gao2024gaussianflow', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gao2024gaussianflow.jpg" data-fallback="None" alt="Paper thumbnail for GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.12365" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zerg-overmind.github.io/GaussianFlow.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=0qRcjTw7-YU" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method's effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024reinforcement" data-title="Reinforcement Learning with Generalizable Gaussian Splatting" data-authors="Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Yecheng Shao, Renjing Xu" data-year="2024" data-tags='["Misc", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024reinforcement', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024reinforcement.jpg" data-fallback="None" alt="Paper thumbnail for Reinforcement Learning with Generalizable Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Reinforcement Learning with Generalizable Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Yecheng Shao, Renjing Xu</p>
      <div class="paper-tags"><span class="paper-tag">Misc</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2404.07950" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2024viewconsistent" data-title="View-Consistent 3D Editing with Gaussian Splatting" data-authors="Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang" data-year="2024" data-tags='["Editing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2024viewconsistent', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2024viewconsistent.jpg" data-fallback="None" alt="Paper thumbnail for View-Consistent 3D Editing with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">View-Consistent 3D Editing with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11868.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vcedit.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing, offering efficient, high-fidelity rendering and enabling precise local manipulations. Currently, diffusion-based 2D editing models are harnessed to modify multi-view rendered images, which then guide the editing of 3DGS models. However, this approach faces a critical issue of multi-view inconsistency, where the guidance images exhibit significant discrepancies across views, leading to mode collapse and visual artifacts of 3DGS. To this end, we introduce View-consistent Editing (VcEdit), a novel framework that seamlessly incorporates 3DGS into image editing processes, ensuring multi-view consistency in edited guidance images and effectively mitigating mode collapse issues. VcEdit employs two innovative consistency modules: the Cross-attention Consistency Module and the Editing Consistency Module, both designed to reduce inconsistencies in edited images. By incorporating these consistency modules into an iterative pattern, VcEdit proficiently resolves the issue of multi-view inconsistency, facilitating high-quality 3DGS editing across a diverse range of scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhao2024badgaussians" data-title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting" data-authors="Lingzhe Zhao, Peng Wang, Peidong Liu" data-year="2024" data-tags='["Code", "Deblurring", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhao2024badgaussians', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhao2024badgaussians.jpg" data-fallback="None" alt="Paper thumbnail for BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lingzhe Zhao, Peng Wang, Peidong Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Deblurring</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11831.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lingzhezhao.github.io/BAD-Gaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/WU-CVGL/BAD-Gaussians/" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds into 3D Gaussians. In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time. In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ji2024nedsslam" data-title="NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting" data-authors="Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie" data-year="2024" data-tags='["SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ji2024nedsslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ji2024nedsslam.jpg" data-fallback="None" alt="Paper thumbnail for NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11679.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time. In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption. Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View Pruning method to eliminate outlier GS points, thereby effectively enhancing the quality of scene representations. Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lei2024gaussnav" data-title="GaussNav: Gaussian Splatting for Visual Navigation" data-authors="Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li" data-year="2024" data-tags='["Autonomous Driving", "Code", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lei2024gaussnav', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lei2024gaussnav.jpg" data-fallback="None" alt="Paper thumbnail for GaussNav: Gaussian Splatting for Visual Navigation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussNav: Gaussian Splatting for Visual Navigation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11625.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xiaohanlei.github.io/projects/GaussNav/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/XiaohanLei/GaussNav" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary difficulty of IIN stems from the necessity of recognizing the target object across varying viewpoints and rejecting potential distractors. Existing map-based navigation methods largely adopt the representation form of Bird's Eye View (BEV) maps, which, however, lack the representation of detailed textures in a scene. To address the above issues, we propose a new Gaussian Splatting Navigation (abbreviated as GaussNav) framework for IIN task, which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The proposed framework enables the agent to not only memorize the geometry and semantic information of the scene, but also retain the textural features of objects. Our GaussNav framework demonstrates a significant leap in performance, evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to 0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="herau20243dgscalib" data-title="3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration" data-authors="Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux" data-year="2024" data-tags='["Misc", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'herau20243dgscalib', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/herau20243dgscalib.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux</p>
      <div class="paper-tags"><span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11577" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://qherau.github.io/3DGS-Calib/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=9_RE_4xkRcM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reliable multimodal sensor fusion algorithms re- quire accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high compu- tational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new ren- dering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve multimodal spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="suzuki2024fed3dgs" data-title="Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning" data-authors="Teppei Suzuki" data-year="2024" data-tags='["Distributed", "Large-Scale"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'suzuki2024fed3dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/suzuki2024fed3dgs.jpg" data-fallback="None" alt="Paper thumbnail for Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Teppei Suzuki</p>
      <div class="paper-tags"><span class="paper-tag">Distributed</span>
<span class="paper-tag">Large-Scale</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11460" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/DensoITLab/Fed3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with federated learning. Existing city-scale reconstruction methods typically adopt a centralized approach, which gathers all data in a central server and reconstructs scenes. The approach hampers scalability because it places a heavy load on the server and demands extensive data storage when reconstructing scenes on a scale beyond city-scale. In pursuit of a more scalable 3D reconstruction, we propose a federated learning framework with 3DGS, which is a decentralized framework and can potentially use distributed computational resources across millions of clients. We tailor a distillation-based model update scheme for 3DGS and introduce appearance modeling for handling non-IID data in the scenario of 3D reconstruction with federated learning. We simulate our method on several large-scale benchmarks, and our method demonstrates rendered image quality comparable to centralized approaches. In addition, we also simulate our method with data collected in different seasons, demonstrating that our framework can reflect changes in the scenes and our appearance modeling captures changes due to seasonal variations.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xiao2024bridging" data-title="Bridging 3D Gaussian and Mesh for Freeview Video Rendering" data-authors="Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao" data-year="2024" data-tags='["Avatar", "Dynamic", "Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xiao2024bridging', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xiao2024bridging.jpg" data-fallback="None" alt="Paper thumbnail for Bridging 3D Gaussian and Mesh for Freeview Video Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Bridging 3D Gaussian and Mesh for Freeview Video Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11453" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This is only a preview version of GauMesh. Recently, primitive-based rendering has been proven to achieve convincing results in solving the problem of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in the context of novel view synthesis, each type of primitive has its inherent defects in terms of representation ability. It is difficult to exploit the mesh to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D Gaussian Splatting) method usually produces artifacts or blurry pixels in the area with smooth geometry and sharp textures. As a result, it is difficult, even not impossible, to represent the complex and dynamic scene with a single type of primitive. To this end, we propose a novel approach, GauMesh, to bridge the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a sequence of tracked mesh as initialization, our goal is to simultaneously optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians, and the deformation field. At a specific time, we perform α-blending on the RGB and opacity values based on the merged and re-ordered z-buffers from mesh and 3D Gaussian rasterizations. This produces the final rendering, which is supervised by the ground-truth image. Experiments demonstrate that our approach adapts the appropriate type of primitives to represent the different parts of the dynamic scene and outperforms all the baseline methods in both quantitative and qualitative comparisons without losing render speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guo2024motionaware" data-title="None" data-authors="Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li" data-year="2024" data-tags='["Dynamic"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guo2024motionaware', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guo2024motionaware.jpg" data-fallback="None" alt="Paper thumbnail for None" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">None <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11447" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene reconstruction. However, existing methods focus mainly on extending static 3DGS into a time-variant representation, while overlooking the rich motion information carried by 2D observations, thus suffering from performance degradation and model redundancy. To address the above problem, we propose a novel motion-aware enhancement framework for dynamic scene reconstruction, which mines useful motion cues from optical flow to improve different paradigms of dynamic 3DGS. Specifically, we first establish a correspondence between 3D Gaussian movements and pixel-level flow. Then a novel flow augmentation method is introduced with additional insights into uncertainty and loss collaboration. Moreover, for the prevalent deformation-based paradigm that presents a harder optimization problem, a transient-aware deformation auxiliary module is proposed. We conduct extensive experiments on both multi-view and monocular scenes to verify the merits of our work. Compared with the baselines, our method shows significant superiority in both rendering quality and efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024bags" data-title="BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors" data-authors="Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen" data-year="2024" data-tags='["Code", "Diffusion", "Dynamic", "Monocular"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024bags', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024bags.jpg" data-fallback="None" alt="Paper thumbnail for BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BAGS: Building Animatable Gaussian Splatting from a Monocular Video with Diffusion Priors <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11427" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Michaelszj/bags" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Animatable 3D reconstruction has significant applications across various fields, primarily relying on artists' handcraft creation. Recently, some studies have successfully constructed animatable 3D models from monocular videos. However, these approaches require sufficient view coverage of the object within the input video and typically necessitate significant time and computational costs for training and rendering. This limitation restricts the practical applications. In this work, we propose a method to build animatable 3D Gaussian Splatting from monocular video with diffusion priors. The 3D Gaussian representations significantly accelerate the training and rendering process, and the diffusion priors allow the method to learn 3D models with limited viewpoints. We also present the rigid regularization to enhance the utilization of the priors. We perform an extensive evaluation across various real-world videos, demonstrating its superior performance compared to the current state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024beyond" data-title="Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF" data-authors="Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee" data-year="2024" data-tags='["Autonomous Driving", "Robotics", "Uncertainty"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024beyond', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024beyond.jpg" data-fallback="None" alt="Paper thumbnail for Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot Navigation and 3D Scene Understanding with FisherRF <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Uncertainty</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11396" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This work proposes a novel approach to bolster both the robot's risk assessment and safety measures while deepening its understanding of 3D scenes, which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian Splatting. To further enhance these capabilities, we incorporate additional sampled views from the environment with the RF model. One of our key contributions is the introduction of Risk-aware Environment Masking (RaEM), which prioritizes crucial information by selecting the next-best-view that maximizes the expected information gain. This targeted approach aims to minimize uncertainties surrounding the robot's path and enhance the safety of its navigation. Our method offers a dual benefit: improved robot safety and increased efficiency in risk-aware 3D scene reconstruction and understanding. Extensive experiments in real-world scenarios demonstrate the effectiveness of our proposed approach, highlighting its potential to establish a robust and safety-focused framework for active robot exploration and 3D scene understanding.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang20243dgsreloc" data-title="3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization" data-authors="Peng Jiang, Gaurav Pandey, Srikanth Saripalli" data-year="2024" data-tags='["Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang20243dgsreloc', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang20243dgsreloc.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual ReLocalization <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Peng Jiang, Gaurav Pandey, Srikanth Saripalli</p>
      <div class="paper-tags"><span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11367" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents a novel system designed for 3D mapping and visual relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and camera data to create accurate and visually plausible representations of the environment. By leveraging LiDAR data to initiate the training of the 3D Gaussian Splatting map, our system constructs maps that are both detailed and geometrically accurate. To mitigate excessive GPU memory usage and facilitate rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree. This preparation makes our method well-suited for visual localization tasks, enabling efficient identification of correspondences between the query image and the rendered image from the Gaussian Splatting map via normalized cross-correlation (NCC). Additionally, we refine the camera pose of the query image using feature-based matching and the Perspective-n-Point (PnP) technique. The effectiveness, adaptability, and precision of our system are demonstrated through extensive evaluation on the KITTI360 dataset.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tarun2024creating" data-title="Creating Seamless 3D Maps Using Radiance Fields" data-authors="Sai Tarun Sathyan, Thomas B. Kinsman" data-year="2024" data-tags='["Misc"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tarun2024creating', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tarun2024creating.jpg" data-fallback="None" alt="Paper thumbnail for Creating Seamless 3D Maps Using Radiance Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Creating Seamless 3D Maps Using Radiance Fields <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sai Tarun Sathyan, Thomas B. Kinsman</p>
      <div class="paper-tags"><span class="paper-tag">Misc</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11364.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">It is desirable to create 3D object models and 3D maps from 2D input images for applications such as navigation, virtual tourism, and urban planning. The traditional methods of creating 3D maps, (such as photogrammetry), require a large number of images and odometry. Additionally, traditional methods have difficulty with reflective surfaces and specular reflections; windows and chrome in the scene can be problematic. Google Road View is a familiar application, which uses traditional methods to fuse a collection of 2D input images into the illusion of a 3D map. However, Google Road View does not create an actual 3D object model, only a collection of views. The objective of this work is to create an actual 3D object model using updated techniques. Neural Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the capability to produce more precise and intricate 3D maps. Gaussian Splatting[4] is another contemporary technique. This investigation compares Neural Radiance Fields to Gaussian Splatting, and describes some of their inner workings. Our primary contribution is a method for improving the results of the 3D reconstructed models. Our results indicate that Gaussian Splatting was superior to the NeRF technique.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024geogaussian" data-title="GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering" data-authors="Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, Federico Tombari" data-year="2024" data-tags='["Code", "Densification", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024geogaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024geogaussian.jpg" data-fallback="None" alt="Paper thumbnail for GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, Federico Tombari</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11324.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yanyan-li.github.io/project/gs/geogaussian" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yanyan-li/GeoGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">During the Gaussian Splatting optimization process, the scene's geometry can gradually deteriorate if its structure is not deliberately preserved, especially in non-textured regions such as walls, ceilings, and furniture surfaces. This degradation significantly affects the rendering quality of novel views that deviate significantly from the viewpoints in the training data. To mitigate this issue, we propose a novel approach called GeoGaussian. Based on the smoothly connected areas observed from point clouds, this method introduces a novel pipeline to initialize thin Gaussians aligned with the surfaces, where the characteristic can be transferred to new generations through a carefully designed densification strategy. Finally, the pipeline ensures that the scene's geometry and texture are maintained through constrained optimization processes with explicit geometry constraints. Benefiting from the proposed architecture, the generative ability of 3D Gaussians is enhanced, especially in structured regions. Our proposed pipeline achieves state-of-the-art performance in novel view synthesis and geometric reconstruction, as evaluated qualitatively and quantitatively on public datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2024brightdreamer" data-title="BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis" data-authors="Lutao Jiang, Lin Wang" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2024brightdreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2024brightdreamer.jpg" data-fallback="None" alt="Paper thumbnail for BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lutao Jiang, Lin Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11273" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vlislab22.github.io/BrightDreamer/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lutao2021/BrightDreamer" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://vlislab22.github.io/BrightDreamer/videos/gui_demo.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Text-to-3D synthesis has recently seen intriguing advances by combining the text-to-image models with 3D representation methods, e.g., Gaussian Splatting (GS), via Score Distillation Sampling (SDS). However, a hurdle of existing methods is the low efficiency, per-prompt optimization for a single 3D object. Therefore, it is imperative for a paradigm shift from per-prompt optimization to one-stage generation for any unseen text prompts, which yet remains challenging. A hurdle is how to directly generate a set of millions of 3D Gaussians to represent a 3D object. This paper presents BrightDreamer, an end-to-end single-stage approach that can achieve generalizable and fast (77 ms) text-to-3D generation. Our key idea is to formulate the generation process as estimating the 3D deformation from an anchor shape with predefined positions. For this, we first propose a Text-guided Shape Deformation (TSD) network to predict the deformed shape and its new positions, used as the centers (one attribute) of 3D Gaussians. To estimate the other four attributes (i.e., scaling, rotation, opacity, and SH coefficient), we then design a novel Text-guided Triplane Generator (TTG) to generate a triplane representation for a 3D object. The center of each Gaussian enables us to transform the triplane feature into the four attributes. The generated 3D Gaussians can be finally rendered at 705 frames per second. Extensive experiments demonstrate the superiority of our method over existing methods. Also, BrightDreamer possesses a strong semantic understanding capability even for complex text prompts.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="deng2024compact" data-title="Compact 3D Gaussian Splatting For Dense Visual SLAM" data-authors="Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen" data-year="2024" data-tags='["SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'deng2024compact', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/deng2024compact.jpg" data-fallback="None" alt="Paper thumbnail for Compact 3D Gaussian Splatting For Dense Visual SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compact 3D Gaussian Splatting For Dense Visual SLAM <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11247.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent work has shown that 3D Gaussian-based SLAM enables high-quality reconstruction, accurate pose estimation, and real-time rendering of scenes. However, these approaches are built on a tremendous number of redundant 3D Gaussian ellipsoids, leading to high memory and storage costs, and slow training speed. To address the limitation, we propose a compact 3D Gaussian Splatting SLAM system that reduces the number and the parameter size of Gaussian ellipsoids. A sliding window-based masking strategy is first proposed to reduce the redundant ellipsoids. Then we observe that the covariance matrix (geometry) of most 3D Gaussian ellipsoids are extremely similar, which motivates a novel geometry codebook to compress 3D Gaussian geometric attributes, i.e., the parameters. Robust and accurate pose estimation is achieved by a global bundle adjustment method with reprojection loss. Extensive experiments demonstrate that our method achieves faster training and rendering speed while maintaining the state-of-the-art (SOTA) quality of the scene representation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024recent" data-title="Recent Advances in 3D Gaussian Splatting" data-authors="Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, Lin Gao" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024recent', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024recent.jpg" data-fallback="None" alt="Paper thumbnail for Recent Advances in 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Recent Advances in 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tong Wu, Yu-Jie Yuan, Ling-Xiao Zhang, Jie Yang, Yan-Pei Cao, Ling-Qi Yan, Lin Gao</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11134" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the rendering speed of novel view synthesis. Unlike neural implicit representations like Neural Radiance Fields (NeRF) that represent a 3D scene with position and viewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of Gaussian ellipsoids to model the scene so that efficient rendering can be accomplished by rasterizing Gaussian ellipsoids into images. Apart from the fast rendering speed, the explicit representation of 3D Gaussian Splatting facilitates editing tasks like dynamic reconstruction, geometry editing, and physical simulation. Considering the rapid change and growing number of works in this field, we present a literature review of recent 3D Gaussian Splatting methods, which can be roughly classified into 3D reconstruction, 3D editing, and other downstream applications by functionality. Traditional point-based rendering methods and the rendering formulation of 3D Gaussian Splatting are also illustrated for a better understanding of this technique. This survey aims to help beginners get into this field quickly and provide experienced researchers with a comprehensive overview, which can stimulate the future development of the 3D Gaussian Splatting representation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2024analyticsplatting" data-title="Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration" data-authors="Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, Kui Jia" data-year="2024" data-tags='["Antialiasing", "Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2024analyticsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2024analyticsplatting.jpg" data-fallback="None" alt="Paper thumbnail for Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhihao Liang, Qi Zhang, Wenbo Hu, Ying Feng, Lei Zhu, Kui Jia</p>
      <div class="paper-tags"><span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.11056.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lzhnb.github.io/project-pages/analytic-splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lzhnb/Analytic-Splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The 3D Gaussian Splatting (3DGS) gained its popularity recently by combining the advantages of both primitive-based and volumetric 3D representations, resulting in improved quality and efficiency for 3D scene rendering. However, 3DGS is not alias-free, and its rendering at varying resolutions could produce severe blurring or jaggies. This is because 3DGS treats each pixel as an isolated, single point rather than as an area, causing insensitivity to changes in the footprints of pixels. Consequently, this discrete sampling scheme inevitably results in aliasing, owing to the restricted sampling bandwidth. In this paper, we derive an analytical solution to address this issue. More specifically, we use a conditioned logistic function as the analytic approximation of the cumulative distribution function (CDF) in a one-dimensional Gaussian signal and calculate the Gaussian integral by subtracting the CDFs. We then introduce this approximation in the two-dimensional pixel shading, and present Analytic-Splatting, which analytically approximates the Gaussian integral within the 2D-pixel window area to better capture the intensity response of each pixel. Moreover, we use the approximated response of the pixel window integral area to participate in the transmittance calculation of volume rendering, making Analytic-Splatting sensitive to the changes in pixel footprint at different resolutions. Experiments on various datasets validate that our approach has better anti-aliasing capability that gives more details and better fidelity.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024darkgs" data-title="DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark" data-authors="Tianyi Zhang, Kaining Huang, Weiming Zhi, Matthew Johnson-Roberson" data-year="2024" data-tags='["Code", "Misc", "Robotics", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024darkgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024darkgs.jpg" data-fallback="None" alt="Paper thumbnail for DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for Robotic Exploration in the Dark <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tianyi Zhang, Kaining Huang, Weiming Zhi, Matthew Johnson-Roberson</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10814" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/tyz1030/darkgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.linkedin.com/posts/tianyi-zhang-396b0a186_darkgs-building-3d-gaussians-with-a-torch-activity-7197672371393019905-iY2-?utm_source=share&utm_medium=member_desktop" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cai2024gspose" data-title="GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object Pose Estimation" data-authors="Dingding Cai, Janne Heikkilä, Esa Rahtu" data-year="2024" data-tags='["Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cai2024gspose', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cai2024gspose.jpg" data-fallback="None" alt="Paper thumbnail for GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object Pose Estimation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-Pose: Cascaded Framework for Generalizable Segmentation-based 6D Object Pose Estimation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Dingding Cai, Janne Heikkilä, Esa Rahtu</p>
      <div class="paper-tags"><span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10683" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dingdingcai.github.io/gs-pose/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/dingdingcai/GSPose" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/SnJazusDLM8" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper introduces GS-Pose, an end-to-end framework for locating and estimating the 6D pose of objects. GS-Pose begins with a set of posed RGB images of a previously unseen object and builds three distinct representations stored in a database. At inference, GS-Pose operates sequentially by locating the object in the input image, estimating its initial 6D pose using a retrieval approach, and refining the pose with a render-and-compare method. The key insight is the application of the appropriate object representation at each stage of the process. In particular, for the refinement step, we utilize 3D Gaussian splatting, a novel differentiable rendering technique that offers high rendering speed and relatively low optimization time. Off-the-shelf toolchains and commodity hardware, such as mobile phones, can be used to capture new objects to be added to the database. Extensive evaluations on the LINEMOD and OnePose-LowTexture datasets demonstrate excellent performance, establishing the new state-of-the-art.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dahmani2024swag" data-title="SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians" data-authors="Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou" data-year="2024" data-tags='["In the Wild", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dahmani2024swag', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dahmani2024swag.jpg" data-fallback="None" alt="Paper thumbnail for SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</p>
      <div class="paper-tags"><span class="paper-tag">In the Wild</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10427.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="feng2024fdgaussian" data-title="FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model" data-authors="Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang" data-year="2024" data-tags='["Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'feng2024fdgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/feng2024fdgaussian.jpg" data-fallback="None" alt="Paper thumbnail for FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10242.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024ggrt" data-title="GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time" data-authors="Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han" data-year="2024" data-tags='["Code", "Poses", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024ggrt', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024ggrt.jpg" data-fallback="None" alt="Paper thumbnail for GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10147" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://3d-aigc.github.io/GGRt/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lifuguan/GGRt_official" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at ≥ 5 FPS and real-time rendering at ≥ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024texturegs" data-title="Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing" data-authors="Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang" data-year="2024" data-tags='["Code", "Editing", "Project", "Texturing", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024texturegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024texturegs.jpg" data-fallback="None" alt="Paper thumbnail for Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian Splatting Editing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Texturing</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.10050" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://slothfulxtx.github.io/TexGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/slothfulxtx/Texture-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=mn_vi4a_fu4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting, emerging as a groundbreaking approach, has drawn increasing attention for its capabilities of high-fidelity reconstruction and real-time rendering. However, it couples the appearance and geometry of the scene within the Gaussian attributes, which hinders the flexibility of editing operations, such as texture swapping. To address this issue, we propose a novel approach, namely Texture-GS, to disentangle the appearance from the geometry by representing it as a 2D texture mapped onto the 3D surface, thereby facilitating appearance editing. Technically, the disentanglement is achieved by our proposed texture mapping module, which consists of a UV mapping MLP to learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian intersections, and a learnable texture to capture the fine-grained appearance. Extensive experiments on the DTU dataset demonstrate that our method not only facilitates high-fidelity appearance editing but also achieves real-time rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024controllable" data-title="Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting" data-authors="Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024controllable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024controllable.jpg" data-fallback="None" alt="Paper thumbnail for Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09981.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lizhiqi49.github.io/MVControl/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/WU-CVGL/MVControl-threestudio" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2024densoft" data-title="Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience" data-authors="Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang" data-year="2024" data-tags='["Project", "Virtual Reality"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2024densoft', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2024densoft.jpg" data-fallback="None" alt="Paper thumbnail for Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive Experience <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Virtual Reality</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09973.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://metaverse-ai-lab-thu.github.io/Den-SOFT/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We have built a custom mobile multi-camera large-space dense light field capture system, which provides a series of high-quality and sufficiently dense light field images for various scenarios. Our aim is to contribute to the development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF, and 3D Gaussian splitting. More importantly, the collected dataset, which is much denser than existing datasets, may also inspire space-oriented light field reconstruction, which is potentially different from object-centric 3D reconstruction, for immersive VR/AR experiences. We utilized a total of 40 GoPro 10 cameras, capturing images of 5k resolution. The number of photos captured for each scene is no less than 1000, and the average density (view number within a unit sphere) is 134.68. It is also worth noting that our system is capable of efficiently capturing large outdoor scenes. Addressing the current lack of large-space and dense light field datasets, we made efforts to include elements such as sky, reflections, lights and shadows that are of interest to researchers in the field of 3D reconstruction during the data capture process. Finally, we validated the effectiveness of our provided dataset on three popular algorithms and also integrated the reconstructed 3DGS results into the Unity engine, demonstrating the potential of utilizing our datasets to enhance the realism of virtual reality (VR) and create feasible interactive spaces.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="swann2024touchgs" data-title="Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting" data-authors="Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III" data-year="2024" data-tags='["Code", "Project", "Rendering", "Robotics", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'swann2024touchgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/swann2024touchgs.jpg" data-fallback="None" alt="Paper thumbnail for Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09875.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://armlabstanford.github.io/touch-gs" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/armlabstanford/Touch-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=FqejaTEt7aU" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective and transparent objects.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zheng2024gaussiangrasper" data-title="GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping" data-authors="Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Robotics", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zheng2024gaussiangrasper', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zheng2024gaussiangrasper.jpg" data-fallback="None" alt="Paper thumbnail for GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09637" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://mrsecant.github.io/GaussianGrasper/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/MrSecant/GaussianGrasper" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhong2024reconstruction" data-title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians" data-authors="Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li" data-year="2024" data-tags='["Code", "Dynamic", "Physics", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhong2024reconstruction', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhong2024reconstruction.jpg" data-fallback="None" alt="Paper thumbnail for Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09434" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zlicheng.com/spring_gaus/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Colmar-zlicheng/Spring-Gaus" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jung2024raings" data-title="RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting" data-authors="Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim" data-year="2024" data-tags='["Code", "Densification", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jung2024raings', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jung2024raings.jpg" data-fallback="None" alt="Paper thumbnail for RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">RAIN-GS: Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09413" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ku-cvlab.github.io/RAIN-GS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/cvlab-kaist/RAIN-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS often fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR in general. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate INitialization Constraint for 3D Gaussian Splatting) that successfully trains 3D Gaussians from randomly initialized point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on standard datasets, largely improving the performance in all settings.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="di2024hyper3dgtextto3d" data-title="Hyper-3DG:Text-to-3D Gaussian Generation via Hypergraph" data-authors="Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao" data-year="2024" data-tags='["Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'di2024hyper3dgtextto3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/di2024hyper3dgtextto3d.jpg" data-fallback="None" alt="Paper thumbnail for Hyper-3DG:Text-to-3D Gaussian Generation via Hypergraph" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Hyper-3DG:Text-to-3D Gaussian Generation via Hypergraph <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09236.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named ``3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named ``Geometry and Texture Hypergraph Refiner (HGRefiner)''. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="feng2024a" data-title="A New Split Algorithm for 3D Gaussian Splatting" data-authors="Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu" data-year="2024" data-tags='["Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'feng2024a', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/feng2024a.jpg" data-fallback="None" alt="Paper thumbnail for A New Split Algorithm for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">A New Split Algorithm for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.09143" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and geometry generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an N-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu2024gaussctrl" data-title="GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing" data-authors="Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu" data-year="2024" data-tags='["Code", "Editing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu2024gaussctrl', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu2024gaussctrl.jpg" data-fallback="None" alt="Paper thumbnail for GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.08733.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussctrl.active.vision/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ActiveVisionLab/gaussctrl" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://gaussctrl.active.vision/assets/teaser_vid.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS). Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model. Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works. It leads to faster editing as well as higher visual quality. This is achieved by the two terms: (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps. (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations. Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024gaussianimage" data-title="GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting" data-authors="Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang" data-year="2024" data-tags='["2DGS", "Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024gaussianimage', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024gaussianimage.jpg" data-fallback="None" alt="Paper thumbnail for GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang</p>
      <div class="paper-tags"><span class="paper-tag">2DGS</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.08551" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xingtongge.github.io/GaussianImage-page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Xinjie-Q/GaussianImage" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3× lower GPU memory usage and 5× faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="saroha2024gaussian" data-title="Gaussian Splatting in Style" data-authors="Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers" data-year="2024" data-tags='["Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'saroha2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/saroha2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting in Style" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting in Style <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers</p>
      <div class="paper-tags"><span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.08498" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lu2024manigaussian" data-title="ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation" data-authors="Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang" data-year="2024" data-tags='["Code", "Dynamic", "Project", "Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lu2024manigaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lu2024manigaussian.jpg" data-fallback="None" alt="Paper thumbnail for ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.08321.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://guanxinglu.github.io/ManiGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/GuanxingLu/ManiGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1% in average success rate.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024stylegaussian" data-title="StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting" data-authors="Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu" data-year="2024" data-tags='["Code", "Project", "Style Transfer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024stylegaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024stylegaussian.jpg" data-fallback="None" alt="Paper thumbnail for StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Style Transfer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.07807.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kunhao-liu.github.io/StyleGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Kunhao-Liu/StyleGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image's style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2024semgaussslam" data-title="SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM" data-authors="Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang" data-year="2024" data-tags='["SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2024semgaussslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2024semgaussslam.jpg" data-fallback="None" alt="Paper thumbnail for SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang</p>
      <div class="paper-tags"><span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.07494.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D Gaussian representation, that enables accurate 3D semantic mapping, robust camera tracking, and high-quality rendering in real-time. In this system, we incorporate semantic feature embedding into 3D Gaussian representation, which effectively encodes semantic information within the spatial layout of the environment for precise semantic scene representation. Furthermore, we propose feature-level loss for updating 3D Gaussian representation, enabling higher-level guidance for 3D Gaussian optimization. In addition, to reduce cumulative drift and improve reconstruction accuracy, we introduce semantic-informed bundle adjustment leveraging semantic associations for joint optimization of 3D Gaussian representation and camera poses, leading to more robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates superior performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in novel-view semantic synthesis and 3D semantic mapping.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024dngaussian" data-title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization" data-authors="Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu" data-year="2024" data-tags='["Code", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024dngaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024dngaussian.jpg" data-fallback="None" alt="Paper thumbnail for DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.06912.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fictionarry.github.io/DNGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Fictionarry/DNGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=WKXCFNJHZ4o" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a 25× reduction in training time, and over 3000× faster rendering speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2024fregs" data-title="FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization" data-authors="Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing" data-year="2024" data-tags='["Densification"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2024fregs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2024fregs.jpg" data-fallback="None" alt="Paper thumbnail for FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing</p>
      <div class="paper-tags"><span class="paper-tag">Densification</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.06908.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="palandra2024gsedit" data-title="GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting" data-authors="Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà" data-year="2024" data-tags='["Editing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'palandra2024gsedit', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/palandra2024gsedit.jpg" data-fallback="None" alt="Paper thumbnail for GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.05154.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model. The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shao2024splattingavatar" data-title="SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting" data-authors="Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shao2024splattingavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shao2024splattingavatar.jpg" data-fallback="None" alt="Paper thumbnail for SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.05087.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://initialneil.github.io/SplattingAvatar" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/initialneil/SplattingAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=IzC-fLvdntA" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to simultaneously optimize the parameters of the Gaussians while walking on the triangle mesh. SplattingAvatar is a hybrid representation of virtual humans where the mesh represents low-frequency motion and surface deformation, while the Gaussians take over the high-frequency geometry and detailed appearance. Unlike existing deformation methods that rely on an MLP-based linear blend skinning (LBS) field for motion, we control the rotation and translation of the Gaussians directly by mesh, which empowers its compatibility with various animation techniques, e.g., skeletal animation, blend shapes, and mesh editing. Trainable from monocular videos for both full-body and head avatars, SplattingAvatar shows state-of-the-art rendering quality across multiple datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="peng2024bags" data-title="BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling" data-authors="Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa" data-year="2024" data-tags='["Code", "Deblurring", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'peng2024bags', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/peng2024bags.jpg" data-fallback="None" alt="Paper thumbnail for BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Deblurring</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.04926.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nwang43jhu.github.io/BAGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/snldmt/BAGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated benchmarks; however, images captured in real life are often blurry. In this work, we analyze the robustness of Gaussian-Splatting-based methods against various image blur, such as motion blur, defocus blur, downscaling blur, \etc. Under these degradations, Gaussian-Splatting-based methods tend to overfit and produce worse results than Neural-Radiance-Field-based methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling capacities such that a 3D-consistent and high quality scene can be reconstructed despite image-wise blur. Specifically, we model blur by estimating per-pixel convolution kernels from a Blur Proposal Network (BPN). BPN is designed to consider spatial, color, and depth variations of the scene to maximize modeling capacity. Additionally, BPN also proposes a quality-assessing mask, which indicates regions where blur occur. Finally, we introduce a coarse-to-fine kernel optimization scheme; this optimization scheme is fast and avoids sub-optimal solutions due to a sparse point cloud initialization, which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate that BAGS achieves photorealistic renderings under various challenging blur conditions and imaging geometry, while significantly improving upon existing approaches.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cai2024radiative" data-title="Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis" data-authors="TYuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille" data-year="2024" data-tags='["Medicine"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cai2024radiative', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cai2024radiative.jpg" data-fallback="None" alt="Paper thumbnail for Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">TYuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille</p>
      <div class="paper-tags"><span class="paper-tag">Medicine</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.04116.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73x inference speed. The application on sparse-view CT reconstruction also reveals the practical values of our method.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024splatnav" data-title="Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps" data-authors="Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager" data-year="2024" data-tags='["Robotics"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024splatnav', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024splatnav.jpg" data-fallback="None" alt="Paper thumbnail for Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager</p>
      <div class="paper-tags"><span class="paper-tag">Robotics</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.02751.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision. We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map. We then optimize a B-spline trajectory through this corridor. We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud. The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization. We also incorporate semantics into the GSplat in order to obtain better images for localization. All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction. We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun20233dgstream" data-title="3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos" data-authors="Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing" data-year="2023" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun20233dgstream', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun20233dgstream.jpg" data-fallback="None" alt="Paper thumbnail for 3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2403.01444.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://sjojok.github.io/3dgstream/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/SJoJoK/3DGStream" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specificallggy, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the naïve approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lin2024vastgaussian" data-title="VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction" data-authors="Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang" data-year="2024" data-tags='["Large-Scale"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lin2024vastgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lin2024vastgaussian.jpg" data-fallback="None" alt="Paper thumbnail for VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.17427.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vastgaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/kangpeilun/VastGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024gva" data-title="GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos" data-authors="Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang" data-year="2024" data-tags='["Avatar", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024gva', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024gva.jpg" data-fallback="None" alt="Paper thumbnail for GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.16607.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://3d-aigc.github.io/GEA/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2024specgaussian" data-title="Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting" data-authors="Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin" data-year="2024" data-tags='["Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2024specgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2024specgaussian.jpg" data-fallback="None" alt="Paper thumbnail for Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.15870.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ingra14m.github.io/Spec-Gaussian-website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ingra14m/Specular-Gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cheng2024gaussianpro" data-title="GaussianPro: 3D Gaussian Splatting with Progressive Propagation" data-authors="Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen" data-year="2024" data-tags='["Code", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cheng2024gaussianpro', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cheng2024gaussianpro.jpg" data-fallback="None" alt="Paper thumbnail for GaussianPro: 3D Gaussian Splatting with Progressive Propagation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianPro: 3D Gaussian Splatting with Progressive Propagation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.14650.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://kcheng1021.github.io/gaussianpro.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/kcheng1021/GaussianPro" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The advent of 3D Gaussian Splatting (3DGS) has recently brought about a revolution in the field of neural rendering, facilitating high-quality renderings at real-time speed. However, 3DGS heavily depends on the initialized point cloud produced by Structure-from-Motion (SfM) techniques. When tackling with large-scale scenes that unavoidably contain texture-less surfaces, the SfM techniques always fail to produce enough points in these surfaces and cannot provide good initialization for 3DGS. As a result, 3DGS suffers from difficult optimization and low-quality renderings. In this paper, inspired by classical multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that applies a progressive propagation strategy to guide the densification of the 3D Gaussians. Compared to the simple split and clone strategies used in 3DGS, our method leverages the priors of the existing reconstructed geometries of the scene and patch matching techniques to produce new Gaussians with accurate positions and orientations. Experiments on both large-scale and small-scale scenes validate the effectiveness of our method, where our method significantly surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in terms of PSNR.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tosi2024how" data-title="How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey" data-authors="Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tosi2024how', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tosi2024how.jpg" data-fallback="None" alt="Paper thumbnail for How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.13255.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="luo2024gaussianhair" data-title="GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians" data-authors="Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu" data-year="2024" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'luo2024gaussianhair', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/luo2024gaussianhair.jpg" data-fallback="None" alt="Paper thumbnail for GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Haimin Luo, Min Ouyang, Zijun Zhao, Suyi Jiang, Longwen Zhang, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.10483.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Hairstyle reflects culture and ethnicity at first glance. In the digital era, various realistic human hairstyles are also critical to high-fidelity digital human assets for beauty and inclusivity. Yet, realistic hair modeling and real-time rendering for animation is a formidable challenge due to its sheer number of strands, complicated structures of geometry, and sophisticated interaction with light. This paper presents GaussianHair, a novel explicit hair representation. It enables comprehensive modeling of hair geometry and appearance from images, fostering innovative illumination effects and dynamic animation capabilities. At the heart of GaussianHair is the novel concept of representing each hair strand as a sequence of connected cylindrical 3D Gaussian primitives. This approach not only retains the hair's geometric structure and appearance but also allows for efficient rasterization onto a 2D image plane, facilitating differentiable volumetric rendering. We further enhance this model with the "GaussianHair Scattering Model", adept at recreating the slender structure of hair strands and accurately capturing their local diffuse color in uniform lighting. Through extensive experiments, we substantiate that GaussianHair achieves breakthroughs in both geometric and appearance fidelity, transcending the limitations encountered in state-of-the-art methods for hair reconstruction. Beyond representation, GaussianHair extends to support editing, relighting, and dynamic rendering of hair, offering seamless integration with conventional CG pipeline workflows. Complementing these advancements, we have compiled an extensive dataset of real human hair, each with meticulously detailed strand geometry, to propel further research in this field.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2024gaussianobject" data-title="GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting" data-authors="Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2024gaussianobject', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2024gaussianobject.jpg" data-fallback="None" alt="Paper thumbnail for GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.10259.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussianobject.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/GaussianObject/GaussianObject" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/ozoI0tmW3r0?si=KcaHtvVnrexqaf58" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hamdi2024ges" data-title="GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering" data-authors="Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi" data-year="2024" data-tags='["Code", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hamdi2024ges', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hamdi2024ges.jpg" data-fallback="None" alt="Paper thumbnail for GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.10128.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://abdullahamdi.com/ges/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ajhamdi/ges-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/edSvNy3roV8?si=VGncH7op1OfqkEtx" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes. It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="melaskyriazi2024im3d" data-title=" IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation" data-authors="Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos" data-year="2024" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'melaskyriazi2024im3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/melaskyriazi2024im3d.jpg" data-fallback="None" alt="Paper thumbnail for  IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title"> IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.08682.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lukemelas.github.io/IM-3D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2024gala3d" data-title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting" data-authors="Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang" data-year="2024" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2024gala3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2024gala3d.jpg" data-fallback="None" alt="Paper thumbnail for GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.07207.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gala3d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fei20243d" data-title="3D Gaussian as a New Vision Era: A Survey" data-authors="Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fei20243d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fei20243d.jpg" data-fallback="None" alt="Paper thumbnail for 3D Gaussian as a New Vision Era: A Survey" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3D Gaussian as a New Vision Era: A Survey <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.07181.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="stanishevskii2024implicitdeepfake" data-title="ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting" data-authors="Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek" data-year="2024" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'stanishevskii2024implicitdeepfake', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/stanishevskii2024implicitdeepfake.jpg" data-fallback="None" alt="Paper thumbnail for ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake Generation using NeRF and Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.06390.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the recent rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object's shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object's characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Such techniques can have a form of artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or facial expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such relatively simple strategies can produce plausible 3D deepfake-based avatars.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2024headstudio" data-title="HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting" data-authors="Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang" data-year="2024" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2024headstudio', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2024headstudio.jpg" data-fallback="None" alt="Paper thumbnail for HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.06149.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zhenglinzhou.github.io/HeadStudio-ProjectPage/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ZhenglinZhou/HeadStudio/" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://zhenglinzhou.github.io/HeadStudio-ProjectPage/videos/demo_arxiv.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising outcomes obtained through 2D diffusion priors in recent works, current methods face challenges in achieving high-quality and animated avatars effectively. In this paper, we present HeadStudio, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animated avatars from text prompts. Our method drives 3D Gaussians semantically to create a flexible and achievable appearance through the intermediate FLAME representation. Specifically, we incorporate the FLAME into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2) FLAME-based score distillation sampling, utilizing FLAME-based fine-grained control signal to guide score distillation from the text prompt. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting visually appealing appearances. The avatars are capable of rendering high-quality real-time (≥40 fps) novel views at a resolution of 1024. They can be smoothly controlled by real-world speech and video. We hope that HeadStudio can advance digital avatar creation and that the present method can widely be applied across various domains.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2024large" data-title="Large Multi-View Gaussian Model for High-Resolution 3D Content Creation" data-authors="Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian" data-year="2024" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2024large', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2024large.jpg" data-fallback="None" alt="Paper thumbnail for Large Multi-View Gaussian Model for High-Resolution 3D Content Creation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Large Multi-View Gaussian Model for High-Resolution 3D Content Creation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.05054.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://me.kiui.moe/lgm/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/3DTopia/LGM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gao2024meshbased" data-title="Mesh-based Gaussian Splatting for Real-time Large-scale Deformation" data-authors="Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai" data-year="2024" data-tags='["Dynamic", "Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gao2024meshbased', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gao2024meshbased.jpg" data-fallback="None" alt="Paper thumbnail for Mesh-based Gaussian Splatting for Real-time Large-scale Deformation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mesh-based Gaussian Splatting for Real-time Large-scale Deformation <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.04796.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="rivero2024rig3dgs" data-title="Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos" data-authors="Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras" data-year="2024" data-tags='["Avatar", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'rivero2024rig3dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/rivero2024rig3dgs.jpg" data-fallback="None" alt="Paper thumbnail for Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.03723.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://shahrukhathar.github.io/2024/02/05/Rig3DGS.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="duan20244d" data-title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes" data-authors="Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen" data-year="2024" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'duan20244d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/duan20244d.jpg" data-fallback="None" alt="Paper thumbnail for 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.03307.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://weify627.github.io/4drotorgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/weify627/4D-Rotor-Gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/18zPuhQ3sSs" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024sgsslam" data-title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM" data-authors="Mingrui Li, Shuhong Liu, Heng Zhou" data-year="2024" data-tags='["Code", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024sgsslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024sgsslam.jpg" data-fallback="None" alt="Paper thumbnail for SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mingrui Li, Shuhong Liu, Heng Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.03246.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/ShuhongLL/SGS-SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=y83yw1E-oUo" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="waczyńska2024games" data-title="GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting" data-authors="Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek" data-year="2024" data-tags='["Code", "Dynamic", "Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'waczyńska2024games', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/waczyńska2024games.jpg" data-fallback="None" alt="Paper thumbnail for GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.01459.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/waczjoan/gaussian-mesh-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In recent years, a range of neural network-based methods for image rendering have been introduced. For instance, widely-researched neural radiance fields (NeRF) rely on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-theart technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that pin all Gaussians splats on the object surface (mesh). The unique contribution of our methods is defining Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain high-quality renders in the real-time generation of high-quality views. Furthermore, we demonstrate that in the absence of a predefined mesh, it is possible to fine-tune the initial mesh during the learning process.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2024360gs" data-title="360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming" data-authors="Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo" data-year="2024" data-tags='["360 degree", "Code", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2024360gs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2024360gs.jpg" data-fallback="None" alt="Paper thumbnail for 360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo</p>
      <div class="paper-tags"><span class="paper-tag">360 degree</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.00763.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/LeoDarcy/360GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of 360∘ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel 360∘ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2024optimal" data-title="On the Error Analysis of 3D Gaussian Splatting
and an Optimal Projection Strategy
" data-authors="Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo" data-year="2024" data-tags='["Code", "Perspective-correct", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2024optimal', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2024optimal.jpg" data-fallback="None" alt="Paper thumbnail for On the Error Analysis of 3D Gaussian Splatting
and an Optimal Projection Strategy
" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">On the Error Analysis of 3D Gaussian Splatting
and an Optimal Projection Strategy
 <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Perspective-correct</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.00752.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://letianhuang.github.io/op43dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/LetianHuang/op43dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance, and robustness in sparse viewpoints, leading to various improvements. However, there has been a notable lack of attention to the fundamental problem of projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which can accommodate a variety of camera models. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="radl2024stopthepop" data-title="StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering" data-authors="Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger" data-year="2024" data-tags='["Code", "Perspective-correct", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'radl2024stopthepop', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/radl2024stopthepop.jpg" data-fallback="None" alt="Paper thumbnail for StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Perspective-correct</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2402.00525.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://r4dl.github.io/StopThePop/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/r4dl/StopThePop" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/EmcXtHYhigk" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hu2024segment" data-title="Segment Anything in 3D Gaussians" data-authors="Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang" data-year="2024" data-tags='["Code", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hu2024segment', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hu2024segment.jpg" data-fallback="None" alt="Paper thumbnail for Segment Anything in 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Segment Anything in 3D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.17857.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/XuHu0529/SAGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed. Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain. Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc. In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters. We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians. Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods. We also propose a cross-view label-voting approach to assign labels from different views. In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="strong2024next" data-title="Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting" data-authors="Matthew Strong, Boshu Lei, Aiden Swann, Wen Jiang, Kostas Daniilidis, Monroe Kennedy III" data-year="2024" data-tags='["Code", "Misc", "Project", "Robotics", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'strong2024next', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/strong2024next.jpg" data-fallback="None" alt="Paper thumbnail for Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Matthew Strong, Boshu Lei, Aiden Swann, Wen Jiang, Kostas Daniilidis, Monroe Kennedy III</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Robotics</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.16663.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://armlabstanford.github.io/next-best-sense" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/armlabstanford/NextBestSense" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=NHgb_16-Plg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a framework for active next best view and touch selection for robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to represent scenes in a both photorealistic and geometrically accurate manner. However, in real-world, online robotic scenes where the number of views is limited given efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping and redundant. We address this issue by proposing an end-to-end online training and active view selection pipeline, which enhances the performance of 3DGS in few-view robotics settings. We first elevate the performance of few-shot 3DGS with a novel semantic depth alignment method using Segment Anything Model 2 (SAM2) that we supplement with Pearson depth and surface normal loss to improve color and depth reconstruction of real-world scenes. We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based on depth uncertainty. We perform online view selection on a real robot system during live 3DGS training. We motivate our improvements to few-shot GS scenes, and extend depth-based FisherRF to them, where we demonstrate both qualitative and quantitative improvements on challenging robot scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2024vrgs" data-title="VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality" data-authors="Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang" data-year="2024" data-tags='["Meshing", "Physics", "Virtual Reality"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2024vrgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2024vrgs.jpg" data-fallback="None" alt="Paper thumbnail for VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Virtual Reality</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.16663.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yingjiang96.github.io/VR-GS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain momentum, there's a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for content creation, editing, and interaction within these virtual spaces are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting in a Virtual Reality setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our Virtual Reality system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2024endo4dgs" data-title="Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting" data-authors="Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Mobarakol Islam, Hongliang Ren" data-year="2024" data-tags='["Dynamic", "Medicine"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2024endo4dgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2024endo4dgs.jpg" data-fallback="None" alt="Paper thumbnail for Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Mobarakol Islam, Hongliang Ren</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Medicine</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.16416.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes but are hampered by slow inference speed, prolonged training, and inconsistent depth estimation. Some previous work utilizes ground truth depth for optimization but is hard to acquire in the surgical domain. To overcome these obstacles, we present Endo-4DGS, a real-time endoscopic dynamic reconstruction approach that utilizes 3D Gaussian Splatting (GS) for 3D representation. Specifically, we propose lightweight MLPs to capture temporal dynamics with Gaussian deformation fields. To obtain a satisfactory Gaussian Initialization, we exploit a powerful depth estimation foundation model, Depth-Anything, to generate pseudo-depth maps as a geometry prior. We additionally propose confidence-guided learning to tackle the ill-pose problems in monocular depth estimation and enhance the depth-guided reconstruction with surface normal constraints and depth regularization. Our approach has been validated on two surgical datasets, where it can effectively render in real-time, compute efficiently, and reconstruct with remarkable accuracy.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="feng2024gaussian" data-title="Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting" data-authors="Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang" data-year="2024" data-tags='["Dynamic", "Physics", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'feng2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/feng2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.15318.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://amysteriouscat.github.io/GaussianSplashing/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=KgaR1ni-Egg&t" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hong2024livgaussmap" data-title="LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering" data-authors="Sheng Hong, Junjie He, Xinhu Zheng, Hesheng Wang, Hao Fang, Kangcheng Liu, Chunran Zheng, Shaojie Shen" data-year="2024" data-tags='["Large-Scale", "Lidar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hong2024livgaussmap', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hong2024livgaussmap.jpg" data-fallback="None" alt="Paper thumbnail for LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sheng Hong, Junjie He, Xinhu Zheng, Hesheng Wang, Hao Fang, Kangcheng Liu, Chunran Zheng, Shaojie Shen</p>
      <div class="paper-tags"><span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Lidar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.14857.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion. This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements. Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhuang2024tipeditor" data-title="TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts" data-authors="Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan" data-year="2024" data-tags='["Code", "Editing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhuang2024tipeditor', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhuang2024tipeditor.jpg" data-fallback="None" alt="Paper thumbnail for TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.14828.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zjy526223908.github.io/TIP-Editor/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zjy526223908/TIP-Editor" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://zjy526223908.github.io/TIP-Editor/static/videos/TIP_editor_video_short.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIPEditor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xiong2024gauuscene" data-title="GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting" data-authors="Butian Xiong, Zhuo Li, Zhen Li" data-year="2024" data-tags='["Code", "Large-Scale", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xiong2024gauuscene', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xiong2024gauuscene.jpg" data-fallback="None" alt="Paper thumbnail for GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Butian Xiong, Zhuo Li, Zhen Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Large-Scale</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.14032.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://saliteta.github.io/CUHKSZ_SMBU/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/saliteta/lidar_SfM_alignment" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km2. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhao2024psavatar" data-title="PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting" data-authors="Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu" data-year="2024" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhao2024psavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhao2024psavatar.jpg" data-fallback="None" alt="Paper thumbnail for PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.12900.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Despite much progress, achieving real-time high-fidelity head avatar animation is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency. Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time (≥ 25 fps at a resolution of 512 × 512 ).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2024endogaussian" data-title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction" data-authors="Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan" data-year="2024" data-tags='["Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2024endogaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2024endogaussian.jpg" data-fallback="None" alt="Paper thumbnail for EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.12561.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yifliu3.github.io/EndoGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/CUHK-AIM-Group/EndoGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100× gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2024endogs" data-title="EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting" data-authors="Lingting Zhu, Zhao Wang, Jiahao Cui, Zhenchao Jin, Guying Lin, Lequan Yu" data-year="2024" data-tags='["Code", "Medicine"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2024endogs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2024endogs.jpg" data-fallback="None" alt="Paper thumbnail for EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Lingting Zhu, Zhao Wang, Jiahao Cui, Zhenchao Jin, Guying Lin, Lequan Yu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Medicine</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.11535.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/HKU-MedAI/EndoGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision with spatial-temporal weight masks to optimize 3D targets with tool occlusion from a single viewpoint, and surface-aligned regularization terms to capture the much better geometry. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2024gaussianbody" data-title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting" data-authors="Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang" data-year="2024" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2024gaussianbody', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2024gaussianbody.jpg" data-fallback="None" alt="Paper thumbnail for GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.09720.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="pan2024fast" data-title="Fast Dynamic 3D Object Generation from a Single-view Video" data-authors="Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang" data-year="2024" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'pan2024fast', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/pan2024fast.jpg" data-fallback="None" alt="Paper thumbnail for Fast Dynamic 3D Object Generation from a Single-view Video" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Fast Dynamic 3D Object Generation from a Single-view Video <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.08742.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fudan-zvg.github.io/Efficient4D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/fudan-zvg/Efficient4D" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://fudan-zvg.github.io/Efficient4D/assets/video/demo.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Generating dynamic three-dimensional (3D) object from a single-view video is challenging due to the lack of 4D labeled data. Existing methods extend text-to-3D pipelines by transferring off-the-shelf image generation models such as score distillation sampling, but they are slow and expensive to scale (e.g., 150 minutes per object) due to the need for back-propagating the information-limited supervision signals through a large pretrained model. To address this limitation, we propose an efficient video-to-4D object generation framework called Efficient4D. It generates high-quality spacetime-consistent images under different camera views, and then uses them as labeled data to directly train a novel 4D Gaussian splatting model with explicit point cloud geometry, enabling real-time rendering under continuous camera trajectories. Extensive experiments on synthetic and real videos show that Efficient4D offers a remarkable 10-fold increase in speed when compared to prior art alternatives while preserving the same level of innovative view synthesis quality. For example, Efficient4D takes only 14 minutes to model a dynamic object.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bolanos2024gaussian" data-title="Gaussian Shadow Casting for Neural Characters" data-authors="Luis Bolanos, Shih-Yang Su, Helge Rhodin" data-year="2024" data-tags='["Avatar", "Code", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bolanos2024gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bolanos2024gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Shadow Casting for Neural Characters" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Shadow Casting for Neural Characters <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Luis Bolanos, Shih-Yang Su, Helge Rhodin</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.06116.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/LuisBolanos17/GaussianShadowCasting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dou2024cosseggaussians" data-title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians" data-authors="Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan" data-year="2024" data-tags='["Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dou2024cosseggaussians', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dou2024cosseggaussians.jpg" data-fallback="None" alt="Paper thumbnail for CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.05925.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://david-dou.github.io/CoSSegGaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method firstly optimizes Gaussian points’ position, convariance and color attributes under the supervision of RGB images. After Gaussian Locating, we distill multi-scale DINO features extracted from images through unprojection to each Gaussian, which is then incorporated with spatial features from the fast point features processing network, i.e. RandLA-Net. Then the shallow decoding MLP is applied to the multi-scale fused features to obtain compact segmentation. Experimental results show that our model can perform high-quality zero-shot scene segmentation, as our model outperforms other segmentation methods on both semantic and panoptic segmentation task, meanwhile consumes approximately only 10% segmenting time compared to NeRF-based segmentation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2024agg" data-title="AGG: Amortized Generative 3D Gaussians for Single Image to 3D" data-authors="Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat" data-year="2024" data-tags='["Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2024agg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2024agg.jpg" data-fallback="None" alt="Paper thumbnail for AGG: Amortized Generative 3D Gaussians for Single Image to 3D" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">AGG: Amortized Generative 3D Gaussians for Single Image to 3D <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.04099.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ir1d.github.io/AGG/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/jkwmp2UH0Ug?si=lBXjme-d9bVrXTNf" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2024a" data-title="A Survey on 3D Gaussian Splatting" data-authors="Guikun Chen, Wenguan Wang" data-year="2024" data-tags='["Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2024a', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2024a.jpg" data-fallback="None" alt="Paper thumbnail for A Survey on 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">A Survey on 3D Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Guikun Chen, Wenguan Wang</p>
      <div class="paper-tags"><span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.03890.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian splatting (3D GS) has recently emerged as a transformative technique in the explicit radiance field and computer graphics landscape. This innovative approach, characterized by the utilization of millions of 3D Gaussians, represents a significant departure from the neural radiance field (NeRF) methodologies, which predominantly use implicit, coordinate-based models to map spatial coordinates to pixel values. 3D GS, with its explicit scene representations and differentiable rendering algorithms, not only promises real-time rendering capabilities but also introduces unprecedented levels of control and editability. This positions 3D GS as a potential game-changer for the next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and the driving forces behind the advent of 3D GS, setting the stage for understanding its significance. A focal point of our discussion is the practical applicability of 3D GS. By facilitating real-time performance, 3D GS opens up a plethora of applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading 3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by identifying current challenges and suggesting potential avenues for future research in this domain. Through this survey, we aim to provide a valuable resource for both newcomers and seasoned researchers, fostering further exploration and advancement in applicable and explicit radiance field representation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="bai2024progress" data-title="Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human" data-authors="Song Bai, Jie Li" data-year="2024" data-tags='["Avatar", "Review"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'bai2024progress', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/bai2024progress.jpg" data-fallback="None" alt="Paper thumbnail for Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Song Bai, Jie Li</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Review</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.02620.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language inputs to transcend into human motion outputs. This paper aims to provide a comprehensive overview and summary of the relevant papers published mostly during the latter half year of 2023. It will begin by discussing the AI generated object models in 3D, followed by the generated 3D human models, and finally, the generated 3D human motions, culminating in a conclusive summary and a vision for the future.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="meyer2023pegasus" data-title="PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation" data-authors="Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae" data-year="2023" data-tags='["Code", "Misc", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'meyer2023pegasus', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/meyer2023pegasus.jpg" data-fallback="None" alt="Paper thumbnail for PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.02281.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://meyerls.github.io/pegasus_web/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/meyerls/PEGASUS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent representation learning with sparse training data, we introduce a novel flow-based temporal smoothing mechanism and a position-aware adaptive control strategy. Extensive experiments on Waymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 50/6000-fold acceleration in training/rendering over the best alternative.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zuo2024fmgs" data-title="FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding" data-authors="Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zuo2024fmgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zuo2024fmgs.jpg" data-fallback="None" alt="Paper thumbnail for FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.01970.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xingxingzuo.github.io/fmgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/google-research/foundation-model-embedded-3dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851× faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yan2024street" data-title="Street Gaussians for Modeling Dynamic Urban Scenes" data-authors="Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng" data-year="2024" data-tags='["Autonomous Driving", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yan2024street', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yan2024street.jpg" data-fallback="None" alt="Paper thumbnail for Street Gaussians for Modeling Dynamic Urban Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Street Gaussians for Modeling Dynamic Urban Scenes <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.01339.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zju3dv.github.io/street_gaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/zju3dv/street_gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper aims to tackle the problem of modeling dynamic urban street scenes from monocular videos. Recent methods extend NeRF by incorporating tracked vehicle poses to animate vehicles, enabling photo-realistic view synthesis of dynamic urban street scenes. However, significant limitations are their slow training and rendering speed, coupled with the critical need for high precision in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene representation that tackles all these limitations. Specifically, the dynamic urban street is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background. To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a dynamic spherical harmonics model for the dynamic appearance. The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 133 FPS (1066×1600 resolution) within half an hour of training. The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets. Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets. Furthermore, the proposed representation delivers performance on par with that achieved using precise ground-truth poses, despite relying only on poses from an off-the-shelf tracker.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lee2023deblurring" data-title="Deblurring 3D Gaussian Splatting" data-authors="Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park" data-year="2023" data-tags='["Code", "Deblurring", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lee2023deblurring', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lee2023deblurring.jpg" data-fallback="None" alt="Paper thumbnail for Deblurring 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Deblurring 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Deblurring</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.00834.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yin20234dgen" data-title="4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency" data-authors="Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, Yunchao Wei" data-year="2023" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yin20234dgen', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yin20234dgen.jpg" data-fallback="None" alt="Paper thumbnail for 4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, Yunchao Wei</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.17225.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vita-group.github.io/4DGen/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/4DGen" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=-bXyBKdpQ1o" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize entire dynamic 3D scene. However, as these pipelines generate 4D content from text or image inputs, they incur significant time and effort in prompt engineering through trial and error. This work introduces 4DGen, a novel, holistic framework for grounded 4D content generation that decomposes the 4D generation task into multiple stages. We identify static 3D assets and monocular video sequences as key components in constructing the 4D content. Our pipeline facilitates conditional 4D generation, enabling users to specify geometry (3D assets) and motion (monocular videos), thus offering superior control over content creation. Furthermore, we construct our 4D representation using dynamic 3D Gaussians, which permits efficient, high-resolution supervision through rendering during training, thereby facilitating high-quality 4D generation. Additionally, we employ spatial-temporal pseudo labels on anchor frames, along with seamless consistency priors implemented through 3D-aware score distillation sampling and smoothness regularizations. Compared to existing baselines, our approach yields competitive results in faithfully reconstructing input signals and realistically inferring renderings from novel viewpoints and timesteps. Most importantly, our method supports grounded generation, offering users enhanced control, a feature difficult to achieve with previous methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ren2023dreamgaussian4d" data-title="DreamGaussian4D: Generative 4D Gaussian Splatting" data-authors="Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu" data-year="2023" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ren2023dreamgaussian4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ren2023dreamgaussian4d.jpg" data-fallback="None" alt="Paper thumbnail for DreamGaussian4D: Generative 4D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamGaussian4D: Generative 4D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.17142.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jiawei-ren.github.io/projects/dreamgaussian4d/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/jiawei-ren/dreamgaussian4d" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Remarkable progress has been made in 4D content generation recently. However, existing methods suffer from long optimization time, lack of motion controllability, and a low level of detail. In this paper, we introduce DreamGaussian4D, an efficient 4D generation framework that builds on 4D Gaussian Splatting representation. Our key insight is that the explicit modeling of spatial transformations in Gaussian Splatting makes it more suitable for the 4D generation setting compared with implicit representations. DreamGaussian4D reduces the optimization time from several hours to just a few minutes, allows flexible control of the generated 3D motion, and produces animated meshes that can be efficiently rendered in 3D engines.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2023spacetime" data-title="Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis" data-authors="Zhan Li, Zhang Chen, Zhong Li, Yi Xu" data-year="2023" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2023spacetime', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2023spacetime.jpg" data-fallback="None" alt="Paper thumbnail for Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhan Li, Zhang Chen, Zhong Li, Yi Xu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.16812.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/oppo-us-research/SpacetimeGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=YsPPmf-E6Lg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qin2024langsplat" data-title="LangSplat: 3D Language Gaussian Splatting" data-authors="Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qin2024langsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qin2024langsplat.jpg" data-fallback="None" alt="Paper thumbnail for LangSplat: 3D Language Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LangSplat: 3D Language Gaussian Splatting <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.16084.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://langsplat.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/minghanqin/LangSplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=XMlyjsei-Es" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} × speedup compared to LERF at the resolution of 1440 × 1080.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lan20232dguided" data-title="2D-Guided 3D Gaussian Segmentation" data-authors="Kun Lan, Haoran Li, Haolin Shi, Wenjun Wu, Yong Liao, Lin Wang, Pengyuan Zhou" data-year="2023" data-tags='["Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lan20232dguided', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lan20232dguided.jpg" data-fallback="None" alt="Paper thumbnail for 2D-Guided 3D Gaussian Segmentation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">2D-Guided 3D Gaussian Segmentation <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Kun Lan, Haoran Li, Haolin Shi, Wenjun Wu, Yong Liao, Lin Wang, Pengyuan Zhou</p>
      <div class="paper-tags"><span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.16047.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian, as an explicit 3D representation method, has demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms of expressing complex scenes and training duration. These advantages signal a wide range of applications for 3D Gaussians in 3D understanding and editing. Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The existing segmentation methods are not only cumbersome but also incapable of segmenting multiple objects simultaneously in a short amount of time. In response, this paper introduces a 3D Gaussian segmentation method implemented with 2D segmentation as supervision. This approach uses input 2D segmentation maps to guide the learning of the added 3D Gaussian semantic information, while nearest neighbor clustering and statistical filtering refine the segmentation results. Experiments show that our concise method can achieve comparable performances on mIOU and mAcc for multi-object segmentation as previous single-object segmentation methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2023human101" data-title="Human101: Training 100+FPS Human Gaussians in 100s from 1 View" data-authors="Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2023human101', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2023human101.jpg" data-fallback="None" alt="Paper thumbnail for Human101: Training 100+FPS Human Gaussians in 100s from 1 View" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Human101: Training 100+FPS Human Gaussians in 100s from 1 View <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.15258.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://longxiang-ai.github.io/Human101/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/longxiang-ai/Human101" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jung2023deformable" data-title="Deformable 3D Gaussian Splatting for Animatable Human Avatars" data-authors="HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam" data-year="2023" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jung2023deformable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jung2023deformable.jpg" data-fallback="None" alt="Paper thumbnail for Deformable 3D Gaussian Splatting for Animatable Human Avatars" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Deformable 3D Gaussian Splatting for Animatable Human Avatars <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.15059.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2023dust3r" data-title="DUSt3R: Geometric 3D Vision Made Easy" data-authors="Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud" data-year="2023" data-tags='["3ster-based", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2023dust3r', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2023dust3r.jpg" data-fallback="None" alt="Paper thumbnail for DUSt3R: Geometric 3D Vision Made Easy" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DUSt3R: Geometric 3D Vision Made Easy <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</p>
      <div class="paper-tags"><span class="paper-tag">3ster-based</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.14132.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/naver/dust3r" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://europe.naverlabs.com/wp-content/uploads/2024/09/supmat_dust3r_final_light_h264.mp4?_=1" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="malarz2023gaussian" data-title="Gaussian Splatting with NeRF-based Color and Opacity" data-authors="Dawid Malarz, Weronika Smolak, Jacek Tabor, Sławomir Tadeja, Przemysław Spurek" data-year="2023" data-tags='["Code", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'malarz2023gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/malarz2023gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting with NeRF-based Color and Opacity" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting with NeRF-based Color and Opacity <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Dawid Malarz, Weronika Smolak, Jacek Tabor, Sławomir Tadeja, Przemysław Spurek</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.13729.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/gmum/ViewingDirectionGaussianSplatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects. By encoding the shape and color information within neural network weights, NeRFs excel at producing strikingly sharp novel views of 3D objects. Recently, numerous generalizations of NeRFs utilizing generative models have emerged, expanding its versatility. In contrast, Gaussian Splatting (GS) offers a similar renders quality with faster training and inference as it does not need neural networks to work. We encode information about the 3D objects in the set of Gaussian distributions that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are difficult to condition since they usually require circa hundred thousand Gaussian components. To mitigate the caveats of both models, we propose a hybrid model that uses GS representation of the 3D object's shape and NeRF-based encoding of color and opacity. Our model uses Gaussian distributions with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and neural network, which takes parameters of Gaussian and viewing direction to produce changes in color and opacity. Consequently, our model better describes shadows, light reflections, and transparency of 3D objects.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhang2023repaint123" data-title="Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting" data-authors="Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Munan Ning, Li Yuan" data-year="2023" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhang2023repaint123', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhang2023repaint123.jpg" data-fallback="None" alt="Paper thumbnail for Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Munan Ning, Li Yuan</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.13271.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pku-yuangroup.github.io/repaint123/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent one image to 3D generation methods commonly adopt Score Distillation Sampling (SDS). Despite the impressive results, there are multiple deficiencies including multi-view inconsistency, over-saturated and over-smoothed textures, as well as the slow generation speed. To address these deficiencies, we present Repaint123 to alleviate multi-view bias as well as texture degradation and speed up the generation process. The core idea is to combine the powerful image generation capability of the 2D diffusion model and the texture alignment ability of the repainting strategy for generating high-quality multi-view images with consistency. We further propose visibility-aware adaptive repainting strength for overlap regions to enhance the generated image quality in the repainting process. The generated high-quality and multi-view consistent images enable the use of simple Mean Square Error (MSE) loss for fast 3D content generation. We conduct extensive experiments and show that our method has a superior ability to generate high-quality 3D content with multi-view consistency and fine textures in 2 minutes from scratch.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="szymanowicz2023splatter" data-title="Splatter Image: Ultra-Fast Single-View 3D Reconstruction" data-authors="Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi" data-year="2023" data-tags='["Code", "Feed-Forward", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'szymanowicz2023splatter', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/szymanowicz2023splatter.jpg" data-fallback="None" alt="Paper thumbnail for Splatter Image: Ultra-Fast Single-View 3D Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Splatter Image: Ultra-Fast Single-View 3D Reconstruction <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.13150.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://szymanowiczs.github.io/splatter-image.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/szymanowiczs/splatter-image" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=pcKTf9SVh4g" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce the Splatter Image, an ultra-fast approach for monocular 3D object reconstruction which operates at 38 FPS. Splatter Image is based on Gaussian Splatting, which has recently brought real-time rendering, fast training, and excellent scaling to multi-view reconstruction. For the first time, we apply Gaussian Splatting in a monocular reconstruction setting. Our approach is learning-based, and, at test time, reconstruction only requires the feed-forward evaluation of a neural network. The main innovation of Splatter Image is the surprisingly straightforward design: it uses a 2D image-to-image network to map the input image to one 3D Gaussian per pixel. The resulting Gaussians thus have the form of an image, the Splatter Image. We further extend the method to incorporate more than one image as input, which we do by adding cross-view attention. Owning to the speed of the renderer (588 FPS), we can use a single GPU for training while generating entire images at each iteration in order to optimize perceptual metrics like LPIPS. On standard benchmarks, we demonstrate not only fast reconstruction but also better results than recent and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shaw2023swings" data-title="SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting" data-authors="Richard Shaw, Michal Nazarczuk, Jifei Song, Arthur Moreau, Sibi Catley-Chandar, Helisa Dhamo, Eduardo Perez-Pellitero" data-year="2023" data-tags='["Dynamic"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shaw2023swings', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shaw2023swings.jpg" data-fallback="None" alt="Paper thumbnail for SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Richard Shaw, Michal Nazarczuk, Jifei Song, Arthur Moreau, Sibi Catley-Chandar, Helisa Dhamo, Eduardo Perez-Pellitero</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.13308.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis has shown rapid progress recently, with methods capable of producing increasingly photorealistic results. 3D Gaussian Splatting has emerged as a promising method, producing high-quality renderings of scenes and enabling interactive viewing at real-time frame rates. However, it is limited to static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct dynamic scenes. We model a scene's dynamics using dynamic MLPs, learning deformations from temporally-local canonical representations to per-frame 3D Gaussians. To disentangle static and dynamic regions, tuneable parameters weigh each Gaussian's respective MLP parameters, improving the dynamics modelling of imbalanced scenes. We introduce a sliding window training strategy that partitions the sequence into smaller manageable windows to handle arbitrary length scenes while maintaining high rendering quality. We propose an adaptive sampling strategy to determine appropriate window size hyperparameters based on the scene's motion, balancing training overhead with visual quality. Training a separate dynamic 3D Gaussian model for each sliding window allows the canonical representation to change, enabling the reconstruction of scenes with significant geometric changes. Temporal consistency is enforced using a fine-tuning step with self-supervising consistency loss on randomly sampled novel views. As a result, our method produces high-quality renderings of general dynamic scenes with competitive quantitative performance, which can be viewed in real-time in our dynamic interactive viewer.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="morgenstern2023compact" data-title="Compact 3D Scene Representation via Self-Organizing Gaussian Grids" data-authors="Wieland Morgenstern, Florian Barthel, Anna Hilsmann, Peter Eisert" data-year="2023" data-tags='["Code", "Compression", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'morgenstern2023compact', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/morgenstern2023compact.jpg" data-fallback="None" alt="Paper thumbnail for Compact 3D Scene Representation via Self-Organizing Gaussian Grids" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compact 3D Scene Representation via Self-Organizing Gaussian Grids <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Wieland Morgenstern, Florian Barthel, Anna Hilsmann, Peter Eisert</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.13299.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/fraunhoferhhi/Self-Organizing-Gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=nb5U9xfx7-w" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting has recently emerged as a highly promising technique for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it utilizes efficient rasterization allowing for very fast rendering at high-quality. However, the storage size is significantly higher, which hinders practical deployment, e.g. on resource constrained devices. In this paper, we introduce a compact scene representation organizing the parameters of 3D Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a drastic reduction in storage requirements without compromising visual quality during rendering. Central to our idea is the explicit exploitation of perceptual redundancies present in natural scenes. In essence, the inherent nature of a scene allows for numerous permutations of Gaussian parameters to equivalently represent it. To this end, we propose a novel highly parallel algorithm that regularly arranges the high-dimensional Gaussian parameters into a 2D grid while preserving their neighborhood structure. During training, we further enforce local smoothness between the sorted parameters in the grid. The uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless integration with established renderers. Our method achieves a reduction factor of 17x to 42x in size for complex scenes with no increase in training time, marking a substantial leap forward in the domain of 3D scene distribution and consumption.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="charatan2023pixelsplat" data-title="pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction" data-authors="David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann" data-year="2023" data-tags='["Code", "Feed-Forward", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'charatan2023pixelsplat', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/charatan2023pixelsplat.jpg" data-fallback="None" alt="Paper thumbnail for pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Feed-Forward</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.12337.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://davidcharatan.com/pixelsplat/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/dcharatan/pixelsplat" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yuan2023gavatar" data-title="GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning" data-authors="Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal" data-year="2023" data-tags='["Avatar", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yuan2023gavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yuan2023gavatar.jpg" data-fallback="None" alt="Paper thumbnail for GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.11461.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nvlabs.github.io/GAvatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=PbCF1HzrKrs" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries and extract detailed meshes, we propose a novel SDF-based implicit mesh learning approach for 3D Gaussians that regularizes the underlying geometries and extracts highly detailed textured meshes. Our proposed method, GAvatar, enables the large-scale generation of diverse animatable avatars using only text prompts. GAvatar significantly surpasses existing methods in terms of both appearance and geometry quality, and achieves extremely fast rendering (100 fps) at 1K resolution.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2023gaufre" data-title="GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis" data-authors="Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao" data-year="2023" data-tags='["Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2023gaufre', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2023gaufre.jpg" data-fallback="None" alt="Paper thumbnail for GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View Synthesis <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.11458.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lynl7130.github.io/gaufre/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/YweWidWO8rI?si=jMssQdIXQV67kwzS" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a method for dynamic scene reconstruction using deformable 3D Gaussians that is tailored for monocular video. Building upon the efficiency of Gaussian splatting, our approach extends the representation to accommodate dynamic elements via a deformable set of Gaussians residing in a canonical space, and a time-dependent deformation field defined by a multi-layer perceptron (MLP). Moreover, under the assumption that most natural scenes have large regions that remain static, we allow the MLP to focus its representational power by additionally including a static Gaussian point cloud. The concatenated dynamic and static point clouds form the input for the Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Our method achieves results that are comparable to state-of-the-art dynamic neural radiance field methods while allowing much faster optimization and rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ouyang2023text2immersion" data-title="Text2Immersion: Generative Immersive Scene with 3D Gaussian" data-authors="Hao Ouyang, Kathryn Heal, Stephen Lombardi, Tiancheng Sun" data-year="2023" data-tags='["Diffusion", "Project", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ouyang2023text2immersion', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ouyang2023text2immersion.jpg" data-fallback="None" alt="Paper thumbnail for Text2Immersion: Generative Immersive Scene with 3D Gaussian" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Text2Immersion: Generative Immersive Scene with 3D Gaussian <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Hao Ouyang, Kathryn Heal, Stephen Lombardi, Tiancheng Sun</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.09242.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ken-ouyang.github.io/text2immersion/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Text2Immersion, an elegant method for producing high-quality 3D immersive scenes from text prompts. Our proposed pipeline initiates by progressively generating a Gaussian cloud using pre-trained 2D diffusion and depth estimation models. This is followed by a refining stage on the Gaussian cloud, interpolating and refining it to enhance the details of the generated scene. Distinct from prevalent methods that focus on single object or indoor scenes, or employ zoom-out trajectories, our approach generates diverse scenes with various objects, even extending to the creation of imaginary scenes. Consequently, Text2Immersion can have wide-ranging implications for various applications such as virtual reality, game development, and automated content creation. Extensive evaluations demonstrate that our system surpasses other methods in rendering quality and diversity, further progressing towards text-driven 3D scene generation.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qian20233dgsavatar" data-title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting" data-authors="Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qian20233dgsavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qian20233dgsavatar.jpg" data-fallback="None" alt="Paper thumbnail for 3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.09228.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://neuralbodies.github.io/3DGS-Avatar/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/mikeqzy/3dgs-avatar-release" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/FJ29U9OkmmU?si=5ua2mtpv5ei2n28Z" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zou2023triplane" data-title="Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers" data-authors="Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang" data-year="2023" data-tags='["Code", "Misc", "Project", "Transformer"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zou2023triplane', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zou2023triplane.jpg" data-fallback="None" alt="Paper thumbnail for Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Transformer</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.09147.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zouzx.github.io/TriplaneGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VAST-AI-Research/TriplaneGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="sun2023icomma" data-title="iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching" data-authors="Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang" data-year="2023" data-tags='["Poses"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'sun2023icomma', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/sun2023icomma.jpg" data-fallback="None" alt="Paper thumbnail for iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang</p>
      <div class="paper-tags"><span class="paper-tag">Poses</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.09031.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/YuanSun-XJTU/iComMa" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a method named iComMa to address the 6D pose estimation problem in computer vision. The conventional pose estimation methods typically rely on the target's CAD model or necessitate specific network training tailored to particular object classes. Some existing methods address mesh-free 6D pose estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming to overcome the aforementioned constraints. However, it still suffers from adverse initializations. By contrast, we model the pose estimation as the problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing and matching loss. In detail, a render-and-compare strategy is adopted for the precise estimation of poses. Additionally, a matching module is designed to enhance the model's robustness against adverse initializations by minimizing the distances between 2D keypoints. This framework systematically incorporates the distinctive characteristics and inherent rationale of render-and-compare and matching-based approaches. This comprehensive consideration equips the framework to effectively address a broader range of intricate and challenging scenarios, including instances with substantial angular deviations, all while maintaining a high level of prediction accuracy. Experimental results demonstrate the superior precision and robustness of our proposed jointly optimized framework when evaluated on synthetic and complex real-world data in challenging scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2023drivinggaussian" data-title="DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes" data-authors="Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang" data-year="2023" data-tags='["Autonomous Driving", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2023drivinggaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2023drivinggaussian.jpg" data-fallback="None" alt="Paper thumbnail for DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.07920.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://pkuvdig.github.io/DrivingGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present DrivingGaussian, an efficient and effective framework for surrounding dynamic autonomous driving scenes. For complex scenes with moving objects, we first sequentially and progressively model the static background of the entire scene with incremental static 3D Gaussians. We then leverage a composite dynamic Gaussian graph to handle multiple moving objects, individually reconstructing each object and restoring their accurate positions and occlusion relationships within the scene. We further use a LiDAR prior for Gaussian Splatting to reconstruct scenes with greater details and maintain panoramic consistency. DrivingGaussian outperforms existing methods in driving scene reconstruction and enables photorealistic surround-view synthesis with high-fidelity and multi-camera consistency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fu2023colmapfree" data-title="COLMAP-Free 3D Gaussian Splatting" data-authors="Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang" data-year="2023" data-tags='["Code", "Poses", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fu2023colmapfree', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fu2023colmapfree.jpg" data-fallback="None" alt="Paper thumbnail for COLMAP-Free 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">COLMAP-Free 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Poses</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.07504.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://oasisyang.github.io/colmap-free-3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NVlabs/CF-3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/IJtnx4keJvg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="matsuki2023gaussian" data-title="Gaussian Splatting SLAM" data-authors="Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, Andrew J. Davison" data-year="2023" data-tags='["Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'matsuki2023gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/matsuki2023gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting SLAM <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, Andrew J. Davison</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.06741.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://rmurai.co.uk/projects/GaussianSplattingSLAM/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/muskie82/MonoGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/x604ghp9R_Q?si=fPtz4kgBKFfcnQf3" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="pang2023ash" data-title="ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering" data-authors="Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'pang2023ash', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/pang2023ash.jpg" data-fallback="None" alt="Paper thumbnail for ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.05941.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vcai.mpi-inf.mpg.de/projects/ash/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/kv2000/ASH" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://vcai.mpi-inf.mpg.de/projects/ash/videos/video_for_page.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2023cogs" data-title="CoGS: Controllable Gaussian Splatting" data-authors="Heng Yu, Joel Julin, Zoltán Á. Milacski, Koichiro Niinuma, László A. Jeni" data-year="2023" data-tags='["Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2023cogs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2023cogs.jpg" data-fallback="None" alt="Paper thumbnail for CoGS: Controllable Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CoGS: Controllable Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Heng Yu, Joel Julin, Zoltán Á. Milacski, Koichiro Niinuma, László A. Jeni</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.05664.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://cogs2023.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Capturing and re-animating the 3D structure of articulated objects present significant barriers. On one hand, methods requiring extensively calibrated multi-view setups are prohibitively complex and resource-intensive, limiting their practical applicability. On the other hand, while single-camera Neural Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive training and rendering costs. 3D Gaussian Splatting would be a suitable alternative but for two reasons. Firstly, existing methods for 3D dynamic Gaussians require synchronized multi-view cameras, and secondly, the lack of controllability in dynamic scenarios. We present CoGS, a method for Controllable Gaussian Splatting, that enables the direct manipulation of scene elements, offering real-time control of dynamic scenes without the prerequisite of pre-computing control signals. We evaluated CoGS using both synthetic and real-world datasets that include dynamic objects that differ in degree of difficulty. In our evaluations, CoGS consistently outperformed existing dynamic and controllable neural representations in terms of visual fidelity.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shi2023gir" data-title="GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization" data-authors="Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, Jingdong Wang" data-year="2023" data-tags='["Project", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shi2023gir', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shi2023gir.jpg" data-fallback="None" alt="Paper thumbnail for GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, Errui Ding, Jingdong Wang</p>
      <div class="paper-tags"><span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.05133" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://3dgir.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This paper presents GIR, a 3D Gaussian Inverse Rendering method for relightable scene factorization. Compared to existing methods leveraging discrete meshes or neural implicit fields for inverse rendering, our method utilizes 3D Gaussians to estimate the material properties, illumination, and geometry of an object from multi-view images. Our study is motivated by the evidence showing that 3D Gaussian is a more promising backbone than neural fields in terms of performance, versatility, and efficiency. In this paper, we aim to answer the question: "How can 3D Gaussian be applied to improve the performance of inverse rendering?" To address the complexity of estimating normals based on discrete and often in-homogeneous distributed 3D Gaussian representations, we proposed an efficient self-regularization method that facilitates the modeling of surface normals without the need for additional supervision. To reconstruct indirect illumination, we propose an approach that simulates ray tracing. Extensive experiments demonstrate our proposed GIR's superior performance over existing methods across multiple tasks on a variety of widely used datasets in inverse rendering. This substantiates its efficacy and broad applicability, highlighting its potential as an influential tool in relighting and reconstruction.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2023learn" data-title="Learn to Optimize Denoising Scores for 3D Generation - A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting" data-authors="Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu and Guosheng Lin" data-year="2023" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2023learn', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2023learn.jpg" data-fallback="None" alt="Paper thumbnail for Learn to Optimize Denoising Scores for 3D Generation - A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Learn to Optimize Denoising Scores for 3D Generation - A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu and Guosheng Lin</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.04820.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yangxiaofeng.github.io/demo_diffusion_prior/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yangxiaofeng/LODS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a unified framework aimed at enhancing the diffusion priors for 3D generation tasks. Despite the critical importance of these tasks, existing methodologies often struggle to generate high-caliber results. We begin by examining the inherent limitations in previous diffusion priors. We identify a divergence between the diffusion priors and the training procedures of diffusion models that substantially impairs the quality of 3D generation. To address this issue, we propose a novel, unified framework that iteratively optimizes both the 3D model and the diffusion prior. Leveraging the different learnable parameters of the diffusion prior, our approach offers multiple configurations, affording various trade-offs between performance and implementation complexity. Notably, our experimental results demonstrate that our method markedly surpasses existing techniques, establishing new state-of-the-art in the realm of text-to-3D generation. Furthermore, our approach exhibits impressive performance on both NeRF and the newly introduced 3D Gaussian Splatting backbones. Additionally, our framework yields insightful contributions to the understanding of recent score distillation methods, such as the VSD and DDS loss.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="girish2023eagles" data-title="EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS" data-authors="Sharath Girish, Kamal Gupta, Abhinav Shrivastava" data-year="2023" data-tags='["Acceleration", "Code", "Densification", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'girish2023eagles', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/girish2023eagles.jpg" data-fallback="None" alt="Paper thumbnail for EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Sharath Girish, Kamal Gupta, Abhinav Shrivastava</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.04564.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://efficientgaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Sharath-girish/efficientgaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach results in scene representations with fewer Gaussians and quantized representations, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce memory by more than an order of magnitude all while maintaining the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x less memory and faster training/inference speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2023monogaussianavatar" data-title="MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar" data-authors="Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu" data-year="2023" data-tags='["Avatar", "Code", "Monocular", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2023monogaussianavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2023monogaussianavatar.jpg" data-fallback="None" alt="Paper thumbnail for MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.04558.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yufan1012.github.io/MonoGaussianAvatar" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yufan1012/MonoGaussianAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/3UvBkyPc-oc?si=SbveQKBLJh5GuhIY" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="saito2023relightable" data-title="Relightable Gaussian Codec Avatars" data-authors="Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam" data-year="2023" data-tags='["Avatar", "Code", "Project", "Relight", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'saito2023relightable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/saito2023relightable.jpg" data-fallback="None" alt="Paper thumbnail for Relightable Gaussian Codec Avatars" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Relightable Gaussian Codec Avatars <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.03704.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://shunsukesaito.github.io/rgca/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/facebookresearch/goliath" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://shunsukesaito.github.io/rgca/static/videos/full_video.mp4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The fidelity of relighting is bounded by both geometry and appearance representations. For geometry, both mesh and volumetric approaches have difficulty modeling intricate structures like 3D hair geometry. For appearance, existing relighting models are limited in fidelity and often too slow to render in real-time with high-resolution continuous environments. In this work, we present Relightable Gaussian Codec Avatars, a method to build high-fidelity relightable head avatars that can be animated to generate novel expressions. Our geometry model based on 3D Gaussians can capture 3D-consistent sub-millimeter details such as hair strands and pores on dynamic face sequences. To support diverse materials of human heads such as the eyes, skin, and hair in a unified manner, we present a novel relightable appearance model based on learnable radiance transfer. Together with global illumination-aware spherical harmonics for the diffuse components, we achieve real-time relighting with spatially all-frequency reflections using spherical Gaussians. This appearance model can be efficiently relit under both point light and continuous illumination. We further improve the fidelity of eye reflections and enable explicit gaze control by introducing relightable explicit eye models. Our method outperforms existing approaches without compromising real-time performance. We also demonstrate real-time relighting of avatars on a tethered consumer VR headset, showcasing the efficiency and fidelity of our avatars.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2023hifi4g" data-title="HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting" data-authors="Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu" data-year="2023" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2023hifi4g', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2023hifi4g.jpg" data-fallback="None" alt="Paper thumbnail for HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.03461.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nowheretrix.github.io/HiFi4G/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/917WVr2EHh4" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yugay2023gaussianslam" data-title="Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting" data-authors="Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald" data-year="2023" data-tags='["Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yugay2023gaussianslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yugay2023gaussianslam.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.10070.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://vladimiryugay.github.io/gaussian_slam/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VladimirYugay/Gaussian-SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=RZK1o_ija7M" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present the first neural RGBD SLAM method capable of photorealistically reconstructing real-world scenes. Despite modern SLAM methods achieving impressive results on synthetic datasets, they still struggle with real-world datasets. Our approach utilizes 3D Gaussians as a primary unit for our scene representation to overcome the limitations of the previous methods. We observe that classical 3D Gaussians are hard to use in a monocular setup: they can't encode accurate geometry and are hard to optimize with single-view sequential supervision. By extending classical 3D Gaussians to encode geometry, and designing a novel scene representation and the means to grow, and optimize it, we propose a SLAM system capable of reconstructing and rendering real-world datasets without compromising on speed and efficiency. We show that Gaussian-SLAM can reconstruct and photorealistically render real-world scenes. We evaluate our method on common synthetic and real-world datasets and compare it against other state-of-the-art SLAM methods. Finally, we demonstrate, that the final 3D scene representation that we obtain can be rendered in Real-time thanks to the efficient Gaussian Splatting rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhou2023feature" data-title="Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields" data-authors="Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi" data-year="2023" data-tags='["Code", "Editing", "Inpainting", "Language Embedding", "Project", "Segmentation", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhou2023feature', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhou2023feature.jpg" data-fallback="None" alt="Paper thumbnail for Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Inpainting</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.03203.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://feature-3dgs.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ShijieZhou-UCLA/feature-3dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=h4zmQsCV_Qw" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hu2023gauhuman" data-title="GauHuman: Articulated Gaussian Splatting from Monocular Human Videos" data-authors="Shoukang Hu Ziwei Liu" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hu2023gauhuman', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hu2023gauhuman.jpg" data-fallback="None" alt="Paper thumbnail for GauHuman: Articulated Gaussian Splatting from Monocular Human Videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GauHuman: Articulated Gaussian Splatting from Monocular Human Videos <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shoukang Hu Ziwei Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02973.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://skhu101.github.io/GauHuman/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/skhu101/GauHuman" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/embed/47772bgt5Xo" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1~2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="dhamo2023headgas" data-title="HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting" data-authors="Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero" data-year="2023" data-tags='["Avatar", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'dhamo2023headgas', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/dhamo2023headgas.jpg" data-fallback="None" alt="Paper thumbnail for HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02902.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.youtube.com/watch?v=fetxolufsgQ" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xu2023gaussian" data-title="Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians" data-authors="Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xu2023gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xu2023gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.03029.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yuelangx.github.io/gaussianheadavatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/YuelangX/Gaussian-Head-Avatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=kvrrI3EoM5g" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zheng2023gpsgaussian" data-title="GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis" data-authors="Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zheng2023gpsgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zheng2023gpsgaussian.jpg" data-fallback="None" alt="Paper thumbnail for GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02155.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://shunyuanzheng.github.io/GPS-Gaussian" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ShunyuanZheng/GPS-Gaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/TBIekcqt0j0" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="pokhariya2023manus" data-title="MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians" data-authors="Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar" data-year="2023" data-tags='["Code", "Misc", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'pokhariya2023manus', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/pokhariya2023manus.jpg" data-fallback="None" alt="Paper thumbnail for MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02137.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ivl.cs.brown.edu/research/manus.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/brown-ivl/manus" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Understanding how we grasp objects with our hands has important applications in areas like robotics and mixed reality. However, this challenging problem requires accurate modeling of the contact between hands and objects. To capture grasps, existing methods use skeletons, meshes, or parametric models that can cause misalignments resulting in inaccurate contacts. We present MANUS, a method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians. We build a novel articulated 3D Gaussians representation that extends 3D Gaussian splatting for high-fidelity representation of articulating hands. Since our representation uses Gaussian primitives, it enables us to efficiently and accurately estimate contacts between the hand and the object. For the most accurate results, our method requires tens of camera views that current datasets do not provide. We therefore build MANUS-Grasps, a new dataset that contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3 subjects, and comprising over 7M frames. In addition to extensive qualitative results, we also show that our method outperforms others on a quantitative contact evaluation method that uses paint transfer from the object to the hand.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="hu2023gaussianavatar" data-title="GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians" data-authors="Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'hu2023gaussianavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/hu2023gaussianavatar.jpg" data-fallback="None" alt="Paper thumbnail for GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02134.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://huliangxiao.github.io/GaussianAvatar" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/huliangxiao/GaussianAvatar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=a4g8Z9nCF-k&t=1s" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present GaussianAvatar, an efficient approach to creating realistic human avatars with dynamic 3D appearances from a single video. We start by introducing animatable 3D Gaussians to explicitly represent humans in various poses and clothing styles. Such an explicit and animatable representation can fuse 3D appearances more efficiently and consistently from 2D observations. Our representation is further augmented with dynamic properties to support pose-dependent appearance modeling, where a dynamic appearance network along with an optimizable feature tensor is designed to learn the motion-to-appearance mapping. Moreover, by leveraging the differentiable motion condition, our method enables a joint optimization of motions and appearances during avatar modeling, which helps to tackle the long-standing issue of inaccurate motion estimation in monocular settings. The efficacy of GaussianAvatar is validated on both the public dataset and our collected dataset, demonstrating its superior performances in terms of appearance quality and rendering efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="keetha2023splatam" data-title="SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM" data-authors="Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang," data-year="2023" data-tags='["Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'keetha2023splatam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/keetha2023splatam.jpg" data-fallback="None" alt="Paper thumbnail for SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang,</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02126.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://spla-tam.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/spla-tam/SplaTAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=35SX8DTdQLs" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2× state-of-theart performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ye2023mathematical" data-title="Mathematical Supplement for the gsplat Library" data-authors="Vickie Ye, Angjoo Kanazawa" data-year="2023" data-tags='["Misc"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ye2023mathematical', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ye2023mathematical.jpg" data-fallback="None" alt="Paper thumbnail for Mathematical Supplement for the gsplat Library" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mathematical Supplement for the gsplat Library <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Vickie Ye, Angjoo Kanazawa</p>
      <div class="paper-tags"><span class="paper-tag">Misc</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02121.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al. It provides a self-contained reference for the computations involved in the forward and backward passes of differentiable Gaussian splatting. To facilitate practical usage and development, we provide a user friendly Python API that exposes each component of the forward and backward passes in rasterization of [gsplat](https://github.com/nerfstudio-project/gsplat).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="qian2023gaussianavatars" data-title="GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians" data-authors="Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'qian2023gaussianavatars', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/qian2023gaussianavatars.jpg" data-fallback="None" alt="Paper thumbnail for GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02069" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://shenhanqian.github.io/gaussian-avatars" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ShenhanQian/GaussianAvatars" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/lVEY78RwU_I" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2023scgs" data-title="SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes" data-authors="Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi" data-year="2023" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2023scgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2023scgs.jpg" data-fallback="None" alt="Paper thumbnail for SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.14937.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://yihua7.github.io/SC-GS-web/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yihua7/SC-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/CYQYX_0xi5E" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications. Project page: https://yihua7.github.io/SC-GS-web/
</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wang2023gaussianhead" data-title="GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation" data-authors="Jie Wang, Jiu-Cheng Xie, Xianyan Li, Chi-Man Pun, Feng Xu, Hao Gao" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wang2023gaussianhead', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wang2023gaussianhead.jpg" data-fallback="None" alt="Paper thumbnail for GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jie Wang, Jiu-Cheng Xie, Xianyan Li, Chi-Man Pun, Feng Xu, Hao Gao</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.01632.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://chiehwangs.github.io/gaussian-head-page/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/chiehwangs/gaussian-head" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging. This paper presents GaussianHead, which models the actional human head with anisotropic 3D Gaussians. In our framework, a motion deformation field and multi-resolution tri-plane are constructed respectively to deal with the head's dynamic geometry and complex texture. Notably, we impose an exclusive derivation scheme on each Gaussian, which generates its multiple doppelgangers through a set of learnable parameters for position transformation. With this design, we can compactly and accurately encode the appearance information of Gaussians, even those fitting the head's particular components with sophisticated structures. In addition, an inherited derivation strategy for newly added Gaussians is adopted to facilitate training acceleration. Extensive experiments show that our method can produce high-fidelity renderings, outperforming state-of-the-art approaches in reconstruction, cross-identity reenactment, and novel view synthesis tasks.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xiang2023flashavatar" data-title="FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding" data-authors="Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xiang2023flashavatar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xiang2023flashavatar.jpg" data-fallback="None" alt="Paper thumbnail for FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jun Xiang, Xuan Gao, Yudong Guo, Juyong Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.02214v2" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ustc3dv.github.io/FlashAvatar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/USTC3DV/FlashAvatar-code" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose FlashAvatar, a novel and lightweight 3D animatable avatar representation that could reconstruct a digital avatar from a short monocular video sequence in minutes and render high-fidelity photo-realistic images at 300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D Gaussian field embedded in the surface of a parametric face model and learn extra spatial offset to model non-surface regions and subtle facial details. While full use of geometric priors can capture high-frequency facial details and preserve exaggerated expressions, proper initialization can help reduce the number of Gaussians, thus enabling super-fast rendering speed. Extensive experimental results demonstrate that FlashAvatar outperforms existing works regarding visual quality and personalized details and is almost an order of magnitude faster in rendering speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="das2023neural" data-title="Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction" data-authors="Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen" data-year="2023" data-tags='["Code", "Dynamic", "Monocular", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'das2023neural', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/das2023neural.jpg" data-fallback="None" alt="Paper thumbnail for Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Devikalyan Das, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Monocular</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.01196.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://geometric-rl.mpi-inf.mpg.de/npg/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/DevikalyanDas/npgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/ModbrI0GORU" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, highquality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object’s deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects, maintaining 3D consistency across novel views. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="durvasula2024distwar" data-title="DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines" data-authors="Sankeerth Durvasula, Adrian Zhao, Fan Chen, Ruofan Liang, Pawan Kumar Sanjaya, Nandita Vijaykumar" data-year="2024" data-tags='["Acceleration", "Code", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'durvasula2024distwar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/durvasula2024distwar.jpg" data-fallback="None" alt="Paper thumbnail for DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DISTWAR: Fast Differentiable Rendering on Raster-based Rendering Pipelines <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Sankeerth Durvasula, Adrian Zhao, Fan Chen, Ruofan Liang, Pawan Kumar Sanjaya, Nandita Vijaykumar</p>
      <div class="paper-tags"><span class="paper-tag">Acceleration</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.05345.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/Accelsnow/gaussian-splatting-distwar" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Differentiable rendering is a technique used in an important emerging class of visual computing applications that involves representing a 3D scene as a model that is trained from 2D images using gradient descent. Recent works (e.g. 3D Gaussian Splatting) use a rasterization pipeline to enable rendering high quality photo-realistic imagery at high speeds from these learned 3D models. These methods have been demonstrated to be very promising, providing state-of-art quality for many important tasks. However, training a model to represent a scene is still a time-consuming task even when using powerful GPUs. In this work, we observe that the gradient computation phase during training is a significant bottleneck on GPUs due to the large number of atomic operations that need to be processed. These atomic operations overwhelm atomic units in the L2 partitions causing stalls. To address this challenge, we leverage the observations that during the gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amounts of atomic traffic (since some threads may be inactive). We propose DISTWAR, a software-approach to accelerate atomic operations based on two key ideas: First, we enable warp-level reduction of threads at the SM sub-cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute the atomic computation between the warp-level reduction at the SM and the L2 atomic units to increase the throughput of atomic computation. Warps with many threads performing atomic updates to the same memory locations are scheduled at the SM, and the rest using L2 atomic units. We implement DISTWAR using existing warp-level primitives. We evaluate DISTWAR on widely used raster-based differentiable rendering workloads. We demonstrate significant speedups of 2.44x on average (up to 5.7x).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="cen2023segment" data-title="Segment Any 3D Gaussians" data-authors="Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian" data-year="2023" data-tags='["Code", "Editing", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'cen2023segment', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/cen2023segment.jpg" data-fallback="None" alt="Paper thumbnail for Segment Any 3D Gaussians" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Segment Any 3D Gaussians <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00860.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jumpat.github.io/SAGA/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Jumpat/SegAnyGAussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Interactive 3D segmentation in radiance fields is an appealing task since its importance in 3D scene understanding and manipulation. However, existing methods face challenges in either achieving fine-grained, multi-granularity segmentation or contending with substantial computational overhead, inhibiting real-time interaction. In this paper, we introduce Segment Any 3D GAussians (SAGA), a novel 3D interactive segmentation approach that seamlessly blends a 2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D segmentation results generated by the segmentation foundation model into 3D Gaussian point features through well-designed contrastive training. Evaluation on existing benchmarks demonstrates that SAGA can achieve competitive performance with state-of-the-art methods. Moreover, SAGA achieves multi-granularity segmentation and accommodates various prompts, including points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation within milliseconds, achieving nearly 1000× acceleration1 compared to previous SOTA.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="ye2023gaussian" data-title="Gaussian Grouping: Segment and Edit Anything in 3D Scenes" data-authors="Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke" data-year="2023" data-tags='["Code", "Editing", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'ye2023gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/ye2023gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Grouping: Segment and Edit Anything in 3D Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Grouping: Segment and Edit Anything in 3D Scenes <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00732.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ymq2017.github.io/gaussian-grouping/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lkeab/gaussian-grouping" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The recent Gaussian Splatting achieves high-quality and real-time novel-view synthesis of the 3D scenes. However, it is solely concentrated on the appearance and geometry modeling, while lacking in fine-grained object-level scene understanding. To address this issue, we propose Gaussian Grouping, which extends Gaussian Splatting to jointly reconstruct and segment anything in open-world 3D scenes. We augment each Gaussian with a compact Identity Encoding, allowing the Gaussians to be grouped according to their object instance or stuff membership in the 3D scene. Instead of resorting to expensive 3D labels, we supervise the Identity Encodings during the differentiable rendering by leveraging the 2D mask predictions by SAM, along with introduced 3D spatial consistency regularization. Comparing to the implicit NeRF representation, we show that the discrete and grouped 3D Gaussians can reconstruct, segment and edit anything in 3D with high visual quality, fine granularity and efficiency. Based on Gaussian Grouping, we further propose a local Gaussian Editing scheme, which shows efficacy in versatile scene editing applications, including 3D object removal, inpainting, colorization and scene recomposition.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zhu2023fsgs" data-title="FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting" data-authors="Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang" data-year="2023" data-tags='["Code", "Project", "Sparse", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zhu2023fsgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zhu2023fsgs.jpg" data-fallback="None" alt="Paper thumbnail for FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00451.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zehaozhu.github.io/FSGS/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/FSGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=CarJgsx3DQY" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2023neusg" data-title="NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance" data-authors="Hanlin Chen, Chen Li, Gim Hee Lee" data-year="2023" data-tags='["Meshing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2023neusg', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2023neusg.jpg" data-fallback="None" alt="Paper thumbnail for NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Hanlin Chen, Chen Li, Gim Hee Lee</p>
      <div class="paper-tags"><span class="paper-tag">Meshing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00846.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Existing neural implicit surface reconstruction methods have achieved impressive performance in multi-view 3D reconstruction by leveraging explicit geometry priors such as depth maps or point clouds as regularization. However, the reconstruction results still lack fine details because of the over-smoothed depth map or sparse point cloud. In this work, we propose a neural implicit surface reconstruction pipeline with guidance from 3D Gaussian Splatting to recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is that it can generate dense point clouds with detailed structure. Nonetheless, a naive adoption of 3D Gaussian Splatting can fail since the generated points are the centers of 3D Gaussians that do not necessarily lie on the surface. We thus introduce a scale regularizer to pull the centers close to the surface by enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine the point cloud from 3D Gaussians Splatting with the normal priors from the surface predicted by neural implicit models instead of using a fixed set of points as guidance. Consequently, the quality of surface reconstruction improves from the guidance of the more accurate 3D Gaussian splatting. By jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our approach benefits from both representations and generates complete surfaces with intricate details. Experiments on Tanks and Temples verify the effectiveness of our proposed method.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xiong2023sparsegs" data-title="SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting" data-authors="Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, Achuta Kadambi" data-year="2023" data-tags='["360 degree", "Code", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xiong2023sparsegs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xiong2023sparsegs.jpg" data-fallback="None" alt="Paper thumbnail for SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, Achuta Kadambi</p>
      <div class="paper-tags"><span class="paper-tag">360 degree</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00206.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://formycat.github.io/SparseGS-Real-Time-360-Sparse-View-Synthesis-using-Gaussian-Splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ForMyCat/SparseGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The problem of novel view synthesis has grown significantly in popularity recently with the introduction of Neural Radiance Fields (NeRFs) and other implicit scene representation methods. A recent advance, 3D Gaussian Splatting (3DGS), leverages an explicit representation to achieve real-time rendering with high-quality results. However, 3DGS still requires an abundance of training views to generate a coherent scene representation. In few shot settings, similar to NeRF, 3DGS tends to overfit to training views, causing background collapse and excessive floaters, especially as the number of training views are reduced. We propose a method to enable training coherent 3DGS-based radiance fields of 360 scenes from sparse training views. We find that using naive depth priors is not sufficient and integrate depth priors with generative and explicit constraints to reduce background collapse, remove floaters, and enhance consistency from unseen viewpoints. Experiments show that our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to 15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and inference cost.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kratimenos2023dynmf" data-title="DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting" data-authors="Agelos Kratimenos, Jiahui Lei, Kostas Daniilidis" data-year="2023" data-tags='["Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kratimenos2023dynmf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kratimenos2023dynmf.jpg" data-fallback="None" alt="Paper thumbnail for DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Agelos Kratimenos, Jiahui Lei, Kostas Daniilidis</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00112.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://agelosk.github.io/dynmf/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://agelosk.github.io/dynmf/img/presentation.mp4#t=0.8" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Accurately and efficiently modeling dynamic scenes and motions is considered so challenging a task due to temporal dynamics and motion complexity. To address these challenges, we propose DynMF, a compact and efficient representation that decomposes a dynamic scene into a few neural trajectories. We argue that the per-point motions of a dynamic scene can be decomposed into a small set of explicit or learned trajectories. Our carefully designed neural framework consisting of a tiny set of learned basis queried only in time allows for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while at the same time, requiring only double the storage compared to static scenes. Our neural representation adequately constrains the inherently underconstrained motion field of a dynamic scene leading to effective and fast optimization. This is done by biding each point to motion coefficients that enforce the per-point sharing of basis trajectories. By carefully applying a sparsity loss to the motion coefficients, we are able to disentangle the motions that comprise the scene, independently control them, and generate novel motion combinations that have never been seen before. We can reach state-of-the-art render quality within just 5 minutes of training and in less than half an hour, we can synthesize novel views of dynamic scenes with superior photorealistic quality. Our representation is interpretable, efficient, and expressive enough to offer real-time view synthesis of complex dynamic scene motions, in monocular and multi-view scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="p2023mdsplatting" data-title="MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes" data-authors="Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Mike Zheng Shou, Shuran Song, Jeffrey Ichnowski" data-year="2023" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'p2023mdsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/p2023mdsplatting.jpg" data-fallback="None" alt="Paper thumbnail for MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Mike Zheng Shou, Shuran Song, Jeffrey Ichnowski</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00583" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://md-splatting.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/momentum-robotics-lab/md-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Accurate 3D tracking in highly deformable scenes with occlusions and shadows can facilitate new applications in robotics, augmented reality, and generative AI. However, tracking under these conditions is extremely challenging due to the ambiguity that arises with large deformations, shadows, and occlusions. We introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view synthesis, using video captures of a dynamic scene from various camera poses. MD-Splatting builds on recent advances in Gaussian splatting, a method that learns the properties of a large number of Gaussians for state-of-the-art and fast novel view synthesis. MD-Splatting learns a deformation function to project a set of Gaussians with non-metric, thus canonical, properties into metric space. The deformation function uses a neural-voxel encoding and a multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow scalar. We enforce physics-inspired regularization terms based on local rigidity, conservation of momentum, and isometry, which leads to trajectories with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking on highly deformable scenes with shadows and occlusions. Compared to state-of-the-art, we improve 3D tracking by an average of 23.9 %, while simultaneously achieving high-quality novel view synthesis. With sufficient texture such as in scene 6, MD-Splatting achieves a median tracking error of 3.39 mm on a cloth of 1 x 1 meters in size</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lu2023scaffoldgs" data-title="Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering" data-authors="Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai" data-year="2023" data-tags='["Code", "Densification", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lu2023scaffoldgs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lu2023scaffoldgs.jpg" data-fallback="None" alt="Paper thumbnail for Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Densification</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2312.00109.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://city-super.github.io/scaffold-gs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/city-super/Scaffold-GS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural rendering methods have significantly advanced photo-realistic 3D scene rendering in various academic and industrial applications. The recent 3D Gaussian Splatting method has achieved the state-of-the-art rendering quality and speed combining the benefits of both primitive-based representations and volumetric representations. However, it often leads to heavily redundant Gaussians that try to fit every training view, neglecting the underlying scene geometry. Consequently, the resulting model becomes less robust to significant view changes, texture-less area and lighting effects. We introduce Scaffold-GS, which uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. Anchor growing and pruning strategies are developed based on the importance of neural Gaussians to reliably improve the scene coverage. We show that our method effectively reduces redundant Gaussians while delivering high-quality rendering. We also demonstrates an enhanced capability to accommodate scenes with varying levels-of-detail and view-dependent observations, without sacrificing the rendering speed.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2023periodic" data-title="Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering" data-authors="Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang" data-year="2023" data-tags='["Autonomous Driving", "Code", "Misc", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2023periodic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2023periodic.jpg" data-fallback="None" alt="Paper thumbnail for Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Autonomous Driving</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.18561.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fudan-zvg.github.io/PVG/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/fudan-zvg/PVG" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Modeling dynamic, large-scale urban scenes is challenging due to their highly intricate geometric structures and unconstrained dynamics in both space and time. Prior methods often employ high-level architectural priors, separating static and dynamic elements, resulting in suboptimal capture of their synergistic interactions. To address this challenge, we present a unified representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon the efficient 3D Gaussian splatting technique, originally designed for static scene representation, by introducing periodic vibration-based temporal dynamics. This innovation enables PVG to elegantly and uniformly represent the characteristics of various objects and elements in dynamic urban scenes. To enhance temporally coherent representation learning with sparse training data, we introduce a novel flow-based temporal smoothing mechanism and a position-aware adaptive control strategy. Extensive experiments on Waymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art alternatives in both reconstruction and novel view synthesis for both dynamic and static scenes. Notably, PVG achieves this without relying on manually labeled object bounding boxes or expensive optical flow estimation. Moreover, PVG exhibits 50/6000-fold acceleration in training/rendering over the best alternative.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shi2024language" data-title="Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding" data-authors="Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, Shao-Hua Guan" data-year="2024" data-tags='["Code", "Language Embedding", "Project", "Segmentation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shi2024language', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shi2024language.jpg" data-fallback="None" alt="Paper thumbnail for Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, Shao-Hua Guan</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Language Embedding</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Segmentation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.18482.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://buaavrcg.github.io/LEGaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/buaavrcg/LEGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language-embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language-embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="navaneet2023compact3d" data-title="Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization" data-authors="KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, Hamed Pirsiavash" data-year="2023" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'navaneet2023compact3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/navaneet2023compact3d.jpg" data-fallback="None" alt="Paper thumbnail for Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector Quantization <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, Hamed Pirsiavash</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.18159.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ucdvision.github.io/compact3d/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/UCDvision/compact3d" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussian Splatting is a new method for modeling and rendering 3D radiance fields that achieves much faster learning and rendering time compared to SOTA NeRF methods. However, it comes with a drawback in the much larger storage demand compared to NeRF methods since it needs to store the parameters for several 3D Gaussians. We notice that many Gaussians may share similar parameters, so we introduce a simple vector quantization method based on \kmeans algorithm to quantize the Gaussian parameters. Then, we store the small codebook along with the index of the code for each Gaussian. Moreover, we compress the indices further by sorting them and using a method similar to run-length encoding. We do extensive experiments on standard benchmarks as well as a new benchmark which is an order of magnitude larger than the standard benchmarks. We show that our simple yet effective method can reduce the storage cost for the original 3D Gaussian Splatting method by a factor of almost 20× with a very small drop in the quality of rendered images.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kocabas2023hugs" data-title="HUGS: Human Gaussian Splats" data-authors="Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kocabas2023hugs', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kocabas2023hugs.jpg" data-fallback="None" alt="Paper thumbnail for HUGS: Human Gaussian Splats" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HUGS: Human Gaussian Splats <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17910.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://machinelearning.apple.com/research/hugs" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/apple/ml-hugs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="vilesov2023cg3d" data-title="CG3D: Compositional Generation for Text-to-3D" data-authors="Alexander Vilesov, Pradyumna Chari, Achuta Kadambi" data-year="2023" data-tags='["Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'vilesov2023cg3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/vilesov2023cg3d.jpg" data-fallback="None" alt="Paper thumbnail for CG3D: Compositional Generation for Text-to-3D" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">CG3D: Compositional Generation for Text-to-3D <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Alexander Vilesov, Pradyumna Chari, Achuta Kadambi</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17907.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://asvilesov.github.io/CG3D/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=FMAVeolsE7s" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2023fisherrf" data-title="FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information" data-authors="Wen Jiang, Boshu Lei, Kostas Daniilidis" data-year="2023" data-tags='["Code", "Misc", "Project", "Uncertainty"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2023fisherrf', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2023fisherrf.jpg" data-fallback="None" alt="Paper thumbnail for FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">FisherRF: Active View Selection and Uncertainty Quantification for Radiance Fields using Fisher Information <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Wen Jiang, Boshu Lei, Kostas Daniilidis</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Misc</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Uncertainty</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17874.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jiangwenpl.github.io/FisherRF/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JiangWenPL/FisherRF" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields. Neural Radiance Fields (NeRF) have greatly advanced image rendering and reconstruction, but the limited availability of 2D images poses uncertainties stemming from occlusions, depth ambiguities, and imaging errors. Efficiently selecting informative views becomes crucial, and quantifying NeRF model uncertainty presents intricate challenges. Existing approaches either depend on model architecture or are based on assumptions regarding density distributions that are not generally applicable. By leveraging Fisher Information, we efficiently quantify observed information within Radiance Fields without ground truth data. This can be used for the next best view selection and pixel-wise uncertainty quantification. Our method overcomes existing limitations on model architecture and effectiveness, achieving state-of-the-art results in both view selection and uncertainty quantification, demonstrating its potential to advance the field of Radiance Fields. Our method with the 3D Gaussian Splatting backend could perform view selections at 70 fps.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="abdal2023gaussian" data-title="Gaussian Shell Maps for Efficient 3D Human Generation" data-authors="Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'abdal2023gaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/abdal2023gaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Shell Maps for Efficient 3D Human Generation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Shell Maps for Efficient 3D Human Generation <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, Gordon Wetzstein</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17857" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://rameenabdal.github.io/GaussianShellMaps/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/computational-imaging/GSM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell–based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary userdefined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves highquality multi-view consistent renderings at a native resolution of 512 × 512 pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jiang2023gaussianshader" data-title="GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces" data-authors="Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, Yuexin Ma" data-year="2023" data-tags='["Code", "Project", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jiang2023gaussianshader', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jiang2023gaussianshader.jpg" data-fallback="None" alt="Paper thumbnail for GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, Yuexin Ma</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17977.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://asparagus15.github.io/GaussianShader.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Asparagus15/GaussianShader" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The advent of neural 3D Gaussians has recently brought about a revolution in the field of neural rendering, facilitating the generation of high-quality renderings at real-time speeds. However, the explicit and discrete representation encounters challenges when applied to scenes featuring reflective surfaces. In this paper, we present GaussianShader, a novel method that applies a simplified shading function on 3D Gaussians to enhance the neural rendering in scenes with reflective surfaces while preserving the training and rendering efficiency. The main challenge in applying the shading function lies in the accurate normal estimation on discrete 3D Gaussians. Specifically, we proposed a novel normal estimation framework based on the shortest axis directions of 3D Gaussians with a delicately designed loss to make the consistency between the normals and the geometries of Gaussian spheres. Experiments show that GaussianShader strikes a commendable balance between efficiency and visual quality. Our method surpasses Gaussian Splatting in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When compared to prior works handling reflective surfaces, such as Ref-NeRF, our optimization time is significantly accelerated (23h vs. 0.58h). Please click on our project website to see more results.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fan2023lightgaussian" data-title="LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS" data-authors="Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang" data-year="2023" data-tags='["Code", "Compression", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fan2023lightgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fan2023lightgaussian.jpg" data-fallback="None" alt="Paper thumbnail for LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17245.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://lightgaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/VITA-Group/LightGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=470hul75bSM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency. To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses. In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liu2023humangaussian" data-title="HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting" data-authors="Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu" data-year="2023" data-tags='["Avatar", "Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liu2023humangaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liu2023humangaussian.jpg" data-fallback="None" alt="Paper thumbnail for HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17061.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://alvinliu0.github.io/projects/HumanGaussian" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/alvinliu0/HumanGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=S3djzHoqPKY" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2023pointn" data-title="Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields" data-authors="Jiajun Huang, Hongchuan Yu" data-year="2023" data-tags='["Editing"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2023pointn', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2023pointn.jpg" data-fallback="None" alt="Paper thumbnail for Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Point'n Move: Interactive Scene Object Manipulation on Gaussian Splatting Radiance Fields <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiajun Huang, Hongchuan Yu</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16737.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose Point'n Move, a method that achieves interactive scene object manipulation with exposed region inpainting. Interactivity here further comes from intuitive object selection and real-time editing. To achieve this, we adopt Gaussian Splatting Radiance Field as the scene representation and fully leverage its explicit nature and speed advantage. Its explicit representation formulation allows us to devise a 2D prompt points to 3D mask dual-stage self-prompting segmentation algorithm, perform mask refinement and merging, minimize change as well as provide good initialization for scene inpainting and perform editing in real-time without per-editing training, all leads to superior quality and performance. We test our method by performing editing on both forward-facing and 360 scenes. We also compare our method against existing scene object removal methods, showing superior quality despite being more capable and having a speed advantage.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="huang2023photoslam" data-title="Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras" data-authors="Huajian Huang, Longwei Li, Hui Cheng, Sai-Kit Yeung" data-year="2023" data-tags='["Code", "Project", "SLAM", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'huang2023photoslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/huang2023photoslam.jpg" data-fallback="None" alt="Paper thumbnail for Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Huajian Huang, Longwei Li, Hui Cheng, Sai-Kit Yeung</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16728.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://huajianup.github.io/research/Photo-SLAM/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/HuajianUP/Photo-SLAM" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=WKXCFNJHZ4o" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The integration of neural rendering and the SLAM system recently showed promising results in joint localization and photorealistic view reconstruction. However, existing methods, fully relying on implicit representations, are so resource-hungry that they cannot run on portable devices, which deviates from the original intention of SLAM. In this paper, we present Photo-SLAM, a novel SLAM framework with a hyper primitives map. Specifically, we simultaneously exploit explicit geometric features for localization and learn implicit photometric features to represent the texture information of the observed environment. In addition to actively densifying hyper primitives based on geometric features, we further introduce a Gaussian-Pyramid-based training method to progressively learn multi-level features, enhancing photorealistic mapping performance. The extensive experiments with monocular, stereo, and RGB-D datasets prove that our proposed system Photo-SLAM significantly outperforms current state-of-the-art SLAM systems for online photorealistic mapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times faster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time speed using an embedded platform such as Jetson AGX Orin, showing the potential of robotics applications.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="moreau2023human" data-title="Human Gaussian Splatting: Real-time Rendering of Animatable Avatars" data-authors="Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero" data-year="2023" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'moreau2023human', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/moreau2023human.jpg" data-fallback="None" alt="Paper thumbnail for Human Gaussian Splatting: Real-time Rendering of Animatable Avatars" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Human Gaussian Splatting: Real-time Rendering of Animatable Avatars <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17113.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://perezpellitero.github.io/projects/hugs/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://www.youtube.com/watch?v=R3CHg46SIfo" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">This work addresses the problem of real-time rendering of photorealistic human body avatars learned from multi-view videos. While the classical approaches to model and render virtual humans generally use a textured mesh, recent research has developed neural body representations that achieve impressive visual quality. However, these models are difficult to render in real-time and their quality degrades when the character is animated with body poses different than the training observations. We propose an animatable human model based on 3D Gaussian Splatting, that has recently emerged as a very efficient alternative to neural radiance fields. The body is represented by a set of gaussian primitives in a canonical space which is deformed with a coarse to fine approach that combines forward skinning and local non-rigid refinement. We describe how to learn our Human Gaussian Splatting (HuGS) model in an end-to-end fashion from multi-view observations, and evaluate it against the state-of-the-art approaches for novel pose synthesis of clothed body. Our method achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4 dataset while being able to render in real-time (80 fps for 512x512 resolution).</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yan2023multiscale" data-title="Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering" data-authors="Zhiwen Yan, Weng Fei Low, Yu Chen, Gim Hee Lee" data-year="2023" data-tags='["Antialiasing", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yan2023multiscale', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yan2023multiscale.jpg" data-fallback="None" alt="Paper thumbnail for Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhiwen Yan, Weng Fei Low, Yu Chen, Gim Hee Lee</p>
      <div class="paper-tags"><span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.17089.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D Gaussians have recently emerged as a highly efficient representation for 3D reconstruction and rendering. Despite its high rendering quality and speed at high resolutions, they both deteriorate drastically when rendered at lower resolutions or from far away camera position. During low resolution or far away rendering, the pixel size of the image can fall below the Nyquist frequency compared to the screen size of each splatted 3D Gaussian and leads to aliasing effect. The rendering is also drastically slowed down by the sequential alpha blending of more splatted Gaussians per pixel. To address these issues, we propose a multi-scale 3D Gaussian splatting algorithm, which maintains Gaussians at different scales to represent the same scene. Higher-resolution images are rendered with more small Gaussians, and lower-resolution images are rendered with fewer larger Gaussians. With similar training time, our algorithm can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at 4×-128× scale rendering on Mip-NeRF360 dataset compared to the single scale 3D Gaussian splatting.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lei2023gart" data-title="GART: Gaussian Articulated Template Models" data-authors="Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, Kostas Daniilidis" data-year="2023" data-tags='["Avatar"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lei2023gart', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lei2023gart.jpg" data-fallback="None" alt="Paper thumbnail for GART: Gaussian Articulated Template Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GART: Gaussian Articulated Template Models <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, Kostas Daniilidis</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16099.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://www.cis.upenn.edu/~leijh/projects/gart/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JiahuiLei/GART" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=-xYNtIlW4WY" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Gaussian Articulated Template Model GART, an explicit, efficient, and expressive representation for non-rigid articulated subject capturing and rendering from monocular videos. GART utilizes a mixture of moving 3D Gaussians to explicitly approximate a deformable subject's geometry and appearance. It takes advantage of a categorical template model prior (SMPL, SMAL, etc.) with learnable forward skinning while further generalizing to more complex non-rigid deformations with novel latent bones. GART can be reconstructed via differentiable rendering from monocular videos in seconds or minutes and rendered in novel poses faster than 150fps.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2023animatable" data-title="Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling" data-authors="Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu" data-year="2023" data-tags='["Avatar", "Code", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2023animatable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2023animatable.jpg" data-fallback="None" alt="Paper thumbnail for Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16096.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://animatable-gaussians.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lizhe00/AnimatableGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=kOmZxD0HxZI" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front & back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="gao2023relightable" data-title="Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing" data-authors="Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, Yao Yao" data-year="2023" data-tags='["Code", "Project", "Ray Tracing", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'gao2023relightable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/gao2023relightable.jpg" data-fallback="None" alt="Paper thumbnail for Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, Yao Yao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Ray Tracing</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16043.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nju-3dv.github.io/projects/Relightable3DGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NJU-3DV/Relightable3DGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a novel differentiable point-based rendering framework for material and lighting decomposition from multi-view images, enabling editing, ray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D scene is represented as a set of relightable 3D Gaussian points, where each point is additionally associated with a normal direction, BRDF parameters, and incident lights from different directions. To achieve robust lighting estimation, we further divide incident lights of each point into global and local components, as well as view-dependent visibilities. The 3D scene is optimized through the 3D Gaussian Splatting technique while BRDF and lighting are decomposed by physically-based differentiable rendering. Moreover, we introduce an innovative point-based ray-tracing approach based on the bounding volume hierarchy for efficient visibility baking, enabling real-time rendering and relighting of 3D Gaussian points with accurate shadow effects. Extensive experiments demonstrate improved BRDF estimation and novel view rendering results compared to state-of-the-art material estimation approaches. Our framework showcases the potential to revolutionize the mesh-based graphics pipeline with a relightable, traceable, and editable rendering pipeline solely based on point cloud.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="fang2023gaussianeditor" data-title="GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions" data-authors="Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, Qi Tian" data-year="2023" data-tags='["Editing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'fang2023gaussianeditor', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/fang2023gaussianeditor.jpg" data-fallback="None" alt="Paper thumbnail for GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, Qi Tian</p>
      <div class="paper-tags"><span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16037.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gaussianeditor.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/KWtALsigR3k?si=h6-A44brd5rm3_CM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes -- 2 hours)</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yu2023mipsplatting" data-title="Mip-Splatting Alias-free 3D Gaussian Splatting" data-authors="Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger" data-year="2023" data-tags='["Antialiasing", "Code", "Project", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yu2023mipsplatting', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yu2023mipsplatting.jpg" data-fallback="None" alt="Paper thumbnail for Mip-Splatting Alias-free 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Mip-Splatting Alias-free 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger</p>
      <div class="paper-tags"><span class="paper-tag">Antialiasing</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16493.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://niujinshuchong.github.io/mip-splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/autonomousvision/mip-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, reaching high fidelity and efficiency. However, strong artifacts can be observed when changing the sampling rate, e.g., by changing focal length or camera distance. We find that the source for this phenomenon can be attributed to the lack of 3D frequency constraints and the usage of a 2D dilation filter. To address this problem, we introduce a 3D smoothing filter which constrains the size of the 3D Gaussian primitives based on the maximal sampling frequency induced by the input views, eliminating high frequency artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip filter, which simulates a 2D box filter, effectively mitigates aliasing and dilation issues. Our comprehensive evaluation, including scenarios such as training on single-scale images and testing on multiple scales, validates the effectiveness of our approach.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2023gsir" data-title="GS-IR: 3D Gaussian Splatting for Inverse Rendering" data-authors="Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, Kui Jia" data-year="2023" data-tags='["Code", "Project", "Relight", "Rendering"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2023gsir', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2023gsir.jpg" data-fallback="None" alt="Paper thumbnail for GS-IR: 3D Gaussian Splatting for Inverse Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-IR: 3D Gaussian Splatting for Inverse Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, Kui Jia</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Relight</span>
<span class="paper-tag">Rendering</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.16473.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/lzhnb/GS-IR" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/lzhnb/GS-IR" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian Splatting (GS) that leverages forward mapping volume rendering to achieve photorealistic novel view synthesis and relighting results. Unlike previous works that use implicit neural representations and volume rendering (e.g. NeRF), which suffer from low expressive power and high computational complexity, we extend GS, a top-performance representation for novel view synthesis, to estimate scene geometry, surface material, and environment illumination from multi-view images captured under unknown lighting conditions. There are two main problems when introducing GS to inverse rendering: 1) GS does not support producing plausible normal natively; 2) forward mapping (e.g. rasterization and splatting) cannot trace the occlusion like backward mapping (e.g. ray tracing). To address these challenges, our GS-IR proposes an efficient optimization scheme that incorporates a depth-derivation-based regularization for normal estimation and a baking-based occlusion to model indirect lighting. The flexible and expressive GS representation allows us to achieve fast and compact geometry reconstruction, photorealistic novel view synthesis, and effective physically-based rendering. We demonstrate the superiority of our method over baseline methods through qualitative and quantitative evaluations on various challenging scenes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2023gaussianeditor" data-title="GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting" data-authors="Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin" data-year="2023" data-tags='["Code", "Editing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2023gaussianeditor', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2023gaussianeditor.jpg" data-fallback="None" alt="Paper thumbnail for GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.14521.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://buaacyw.github.io/gaussian-editor/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/buaacyw/GaussianEditor" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/TdZIICSFqsU?si=-U4tyOvaAPqIROYn" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">3D editing plays a crucial role in many areas such as gaming and virtual reality. Traditional 3D editing methods, which rely on representations like meshes and point clouds, often fall short in realistically depicting complex scenes. On the other hand, methods based on implicit 3D representations, like Neural Radiance Field (NeRF), render complex scenes effectively but suffer from slow processing speeds and limited control over specific scene areas. In response to these challenges, our paper presents GaussianEditor, an innovative and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D representation technique. GaussianEditor enhances precision and control in editing through our proposed Gaussian Semantic Tracing, which traces the editing target throughout the training process. Additionally, we propose hierarchical Gaussian splatting (HGS) to achieve stabilized and fine results under stochastic generative guidance from 2D diffusion models. We also develop editing strategies for efficient object removal and integration, a challenging task for existing methods. Our comprehensive experiments demonstrate GaussianEditor's superior control, efficacy, and rapid performance, marking a significant advancement in 3D editing.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chan2023compact" data-title="Compact 3D Gaussian Representation for Radiance Field" data-authors="Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park" data-year="2023" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chan2023compact', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chan2023compact.jpg" data-fallback="None" alt="Paper thumbnail for Compact 3D Gaussian Representation for Radiance Field" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compact 3D Gaussian Representation for Radiance Field <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.13681.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://maincold2.github.io/c3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/maincold2/Compact-3DGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in capturing complex 3D scenes with high fidelity. However, one persistent challenge that hinders the widespread adoption of NeRFs is the computational bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussisan-based representation and adopts the rasterization pipeline to render the images rather than volumetric rendering, achieving very fast rendering speed and promising image quality. However, a significant drawback arises as 3DGS entails a substantial number of 3D Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while preserving high performance. In addition, we propose a compact but effective representation of view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics. Finally, we learn codebooks to compactly represent the geometric attributes of Gaussian by vector quantization. In our extensive experiments, we consistently show over 10× reduced storage and enhanced rendering speed, while maintaining the quality of the scene representation, compared to 3DGS. Our work provides a comprehensive framework for 3D scene representation, achieving high performance, fast training, compactness, and real-time rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chung2023depthregularized" data-title="Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images" data-authors="Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee" data-year="2023" data-tags='["Code", "Project", "Sparse"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chung2023depthregularized', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chung2023depthregularized.jpg" data-fallback="None" alt="Paper thumbnail for Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Sparse</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.13398.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://robot0321.github.io/DepthRegGS/index.html" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/robot0321/DepthRegularizedGS" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting. Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality. However, it tends to overfit the training views when only a small number of images are available. To address this issue, we introduce a dense depth map as a geometry guide to mitigate overfitting. We obtained the depth map using a pre-trained monocular depth estimation model and aligning the scale and offset using sparse COLMAP feature points. The adjusted depth aids in the color-based optimization of 3D Gaussian splatting, mitigating floating artifacts, and ensuring adherence to geometric constraints. We verify the proposed method on the NeRF-LLFF dataset with varying numbers of few images. Our approach demonstrates robust geometry compared to the original method that relies solely on images.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chung2023luciddreamer" data-title="LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes" data-authors="Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee" data-year="2023" data-tags='["Code", "Diffusion", "Project", "World Generation"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chung2023luciddreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chung2023luciddreamer.jpg" data-fallback="None" alt="Paper thumbnail for LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">World Generation</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.13384.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://luciddreamer-cvlab.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/luciddreamer-cvlab/LucidDreamer?tab=readme-ov-file" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="guédon2023sugar" data-title="SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering" data-authors="Antoine Guédon, Vincent Lepetit" data-year="2023" data-tags='["Code", "Meshing", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'guédon2023sugar', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/guédon2023sugar.jpg" data-fallback="None" alt="Paper thumbnail for SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Antoine Guédon, Vincent Lepetit</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Meshing</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.12775.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://imagine.enpc.fr/~guedona/sugar/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/Anttwo/SuGaR" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=MAkFyWfiBQo.&t" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to sample points on the real surface of the scene and extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="katsumata2023a" data-title="A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis" data-authors="Kai Katsumata, Duc Minh Vo, Hideki Nakayama" data-year="2023" data-tags='["Code", "Compression", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'katsumata2023a', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/katsumata2023a.jpg" data-fallback="None" alt="Paper thumbnail for A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Kai Katsumata, Duc Minh Vo, Hideki Nakayama</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.12897.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://compactdynamic3dgaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/raven38/EfficientDynamic3DGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In novel view synthesis of scenes from multiple input views, 3D Gaussian splatting emerges as a viable alternative to existing radiance field approaches, delivering great visual quality and real-time rendering. While successful in static scenes, the present advancement of 3D Gaussian representation, however, faces challenges in dynamic scenes in terms of memory consumption and the need for numerous observations per time step, due to the onus of storing 3D Gaussian parameters per time step. In this study, we present an efficient 3D Gaussian representation tailored for dynamic scenes in which we define positions and rotations as functions of time while leaving other time-invariant properties of the static 3D Gaussian unchanged. Notably, our representation reduces memory usage, which is consistent regardless of the input sequence length. Additionally, it mitigates the risk of overfitting observed frames by accounting for temporal changes. The optimization of our Gaussian representation based on image and flow reconstruction results in a powerful framework for dynamic scene view synthesis in both monocular and multi-view cases. We obtain the highest rendering speed of 118 frames per second (FPS) at a resolution of 1352×1014 with a single GPU, showing the practical usability and effectiveness of our proposed method in dynamic scene rendering scenarios</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="xie2023physgaussian" data-title="PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics" data-authors="Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang" data-year="2023" data-tags='["Code", "Physics", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'xie2023physgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/xie2023physgaussian.jpg" data-fallback="None" alt="Paper thumbnail for PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Physics</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.12198.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://xpandora.github.io/PhysGaussian/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/XPandora/PhysGaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/watch?v=V96GfcMUH2Q" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or any other geometry embedding, highlighting the principle of "what you see is what you simulate (WS2)." Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yan2023gsslam" data-title="GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting" data-authors="Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li" data-year="2023" data-tags='["Code", "Project", "SLAM"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yan2023gsslam', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yan2023gsslam.jpg" data-fallback="None" alt="Paper thumbnail for GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">SLAM</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.11700.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gs-slam.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we introduce GS-SLAM that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released upon acceptance.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="liang2023luciddreamer" data-title="LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching" data-authors="Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen" data-year="2023" data-tags='["Code", "Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'liang2023luciddreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/liang2023luciddreamer.jpg" data-fallback="None" alt="Paper thumbnail for LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.11284.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://github.com/EnVision-Research/LucidDreamer" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="li2023gaussiandiffusion" data-title="GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise" data-authors="Xinhai Li, Huaibin Wang, Kuo-Kun Tseng" data-year="2023" data-tags='["Diffusion"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'li2023gaussiandiffusion', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/li2023gaussiandiffusion.jpg" data-fallback="None" alt="Paper thumbnail for GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Xinhai Li, Huaibin Wang, Kuo-Kun Tseng</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.11221.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain. However, the amalgamation of Nerf and 2D diffusion models frequently yields oversaturated images, posing severe limitations on downstream industrial applications due to the constraints of pixelwise rendering method. Gaussian splatting has recently superseded the traditional pointwise sampling technique prevalent in NeRF-based methodologies, revolutionizing various aspects of 3D reconstruction. This paper introduces a novel text to 3D content generation framework based on Gaussian splatting, enabling fine control over image saturation through individual Gaussian sphere transparencies, thereby producing more realistic images. The challenge of achieving multi-view consistency in 3D generation significantly impedes modeling complexity and accuracy. Taking inspiration from SJC, we explore employing multi-view noise distributions to perturb images generated by 3D Gaussian splatting, aiming to rectify inconsistencies in multi-view geometry. We ingeniously devise an efficient method to generate noise that produces Gaussian noise from diverse viewpoints, all originating from a shared noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap models in local minima, causing artifacts like floaters, burrs, or proliferative elements. To mitigate these issues, we propose the variational Gaussian splatting technique to enhance the quality and stability of 3D appearance. To our knowledge, our approach represents the first comprehensive utilization of Gaussian splatting across the entire spectrum of 3D content generation processes.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="jena2023splatarmor" data-title="SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos" data-authors="Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choudhary, Brandon Smith, Pratik Chaudhari, James Gee" data-year="2023" data-tags='["Avatar", "Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'jena2023splatarmor', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/jena2023splatarmor.jpg" data-fallback="None" alt="Paper thumbnail for SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choudhary, Brandon Smith, Pratik Chaudhari, James Gee</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.10812.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://jenaroh.it/splatarmor/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/rohitrango/splatarmor" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We propose SplatArmor, a novel approach for recovering detailed and animatable human models by `armoring' a parameterized body model with 3D Gaussians. Our approach represents the human as a set of 3D Gaussians within a canonical space, whose articulation is defined by extending the skinning of the underlying SMPL geometry to arbitrary locations in the canonical space. To account for pose-dependent effects, we introduce a SE(3) field, which allows us to capture both the location and anisotropy of the Gaussians. Furthermore, we propose the use of a neural color field to provide color regularization and 3D supervision for the precise positioning of these Gaussians. We show that Gaussian splatting provides an interesting alternative to neural rendering based methods by leverging a rasterization primitive without facing any of the non-differentiability and optimization challenges typically faced in such approaches. The rasterization paradigms allows us to leverage forward skinning, and does not suffer from the ambiguities associated with inverse skinning and warping. We show compelling results on the ZJU MoCap and People Snapshot datasets, which underscore the effectiveness of our method for controllable human synthesis.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="niedermayr2024compressed" data-title="Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis" data-authors="Simon Niedermayr, Josef Stumpfegger, Rüdiger Westermann" data-year="2024" data-tags='["Code", "Compression", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'niedermayr2024compressed', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/niedermayr2024compressed.jpg" data-fallback="None" alt="Paper thumbnail for Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis <span class="paper-year">(2024)</span></h2>
      <p class="paper-authors">Simon Niedermayr, Josef Stumpfegger, Rüdiger Westermann</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Compression</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2401.02436.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://keksboter.github.io/c3dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/KeKsBoTer/c3dgs" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian splat representation has been introduced for novel view synthesis from sparse image sets. Making such representations suitable for applications like network streaming and rendering on low-power devices requires significantly reduced memory consumption as well as improved rendering efficiency. We propose a compressed 3D Gaussian splat representation that utilizes sensitivity-aware vector clustering with quantization-aware training to compress directional colors and Gaussian parameters. The learned codebooks have low bitrates and achieve a compression rate of up to 31× on real-world scenes with only minimal degradation of visual quality. We demonstrate that the compressed splat representation can be efficiently rendered with hardware rasterization on lightweight GPUs at up to 4× higher framerates than reported via an optimized GPU compute pipeline. Extensive experiments across multiple datasets demonstrate the robustness and rendering speed of the proposed approach.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="zielonka2023drivable" data-title="Drivable 3D Gaussian Avatars" data-authors="Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, Javier Romero" data-year="2023" data-tags='["Avatar", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'zielonka2023drivable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/zielonka2023drivable.jpg" data-fallback="None" alt="Paper thumbnail for Drivable 3D Gaussian Avatars" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Drivable 3D Gaussian Avatars <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, Javier Romero</p>
      <div class="paper-tags"><span class="paper-tag">Avatar</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2311.08581.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://zielon.github.io/d3ga/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://youtu.be/C4IT1gnkaF0?si=zUJLm8adM68pVvR8" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either accurate 3D registrations during training, dense input images during testing, or both. The ones based on neural radiance fields also tend to be prohibitively slow for telepresence applications. This work uses the recently presented 3D Gaussian Splatting (3DGS) technique to render realistic humans at real-time framerates, using dense calibrated multi-view videos as input. To deform those primitives, we depart from the commonly used point deformation method of linear blend skinning (LBS) and use a classic volumetric deformation method: cage deformations. Given their smaller size, we drive these deformations with joint angles and keypoints, which are more suitable for communication applications. Our experiments on nine subjects with varied body shapes, clothes, and motions obtain higher-quality results than state-of-the-art methods when using the same training and test data.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2023realtime" data-title="Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting" data-authors="Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang" data-year="2023" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2023realtime', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2023realtime.jpg" data-fallback="None" alt="Paper thumbnail for Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2310.10642.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://fudan-zvg.github.io/4d-gaussian-splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/fudan-zvg/4d-gaussian-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/3cXC9e4CujM" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yi12023gaussiandreamer" data-title="GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors" data-authors="Taoran Yi1, Jiemin Fang, Guanjun Wu1, Lingxi Xie, Xiaopeng Zhang," data-year="2023" data-tags='["Code", "Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yi12023gaussiandreamer', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yi12023gaussiandreamer.jpg" data-fallback="None" alt="Paper thumbnail for GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Taoran Yi1, Jiemin Fang, Guanjun Wu1, Lingxi Xie, Xiaopeng Zhang,</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2310.08529.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://taoranyi.com/gaussiandreamer/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hustvl/GaussianDreamer" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D generation framework, named as GaussianDreamer, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our GaussianDreamer can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="wu20234d" data-title="4D Gaussian Splatting for Real-Time Dynamic Scene Rendering" data-authors="Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Tian Qi, Xinggang Wang" data-year="2023" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'wu20234d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/wu20234d.jpg" data-fallback="None" alt="Paper thumbnail for 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Tian Qi, Xinggang Wang</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2310.08528.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://guanjunwu.github.io/4dgs/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/hustvl/4DGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/_wRbq8KnaVg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800×800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art method.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="lin2023gaussianflow" data-title="Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle" data-authors="Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao" data-year="2023" data-tags='["Code", "Dynamic", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'lin2023gaussianflow', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/lin2023gaussianflow.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2310.08528.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://nju-3dv.github.io/projects/Gaussian-Flow" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/NJU-3DV/Gaussian-Flow" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Gaussian-Flow, a novel point-based approach for fast dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos. In contrast to the prevalent NeRF-based approaches hampered by slow training and rendering speeds, our approach harnesses recent advancements in point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain Deformation Model (DDDM) is proposed to explicitly model attribute deformations of each Gaussian point, where the time-dependent residual of each attribute is captured by a polynomial fitting in the time domain, and a Fourier series fitting in the frequency domain. The proposed DDDM is capable of modeling complex scene deformations across long video footage, eliminating the need for training separate 3DGS for each frame or introducing an additional implicit neural field to model 3D dynamics. Moreover, the explicit deformation modeling for discretized Gaussian points ensures ultra-fast training and rendering of a 4D scene, which is comparable to the original 3DGS designed for static 3D reconstruction. Our proposed approach showcases a substantial efficiency improvement, achieving a 5× faster training speed compared to the per-frame 3DGS modeling. In addition, quantitative results demonstrate that the proposed Gaussian-Flow significantly outperforms previous leading methods in novel view rendering quality.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="tang2023dreamgaussian" data-title="DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation" data-authors="Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng" data-year="2023" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'tang2023dreamgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/tang2023dreamgaussian.jpg" data-fallback="None" alt="Paper thumbnail for DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2309.16653.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dreamgaussian.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/dreamgaussian/dreamgaussian" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/live/l956ye13F8M?si=ZkvFL_lsY5OQUB7e" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="chen2023textto3d" data-title="Text-to-3D using Gaussian Splatting" data-authors="Zilong Chen, Feng Wang, Huaping Liu" data-year="2023" data-tags='["Code", "Diffusion", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'chen2023textto3d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/chen2023textto3d.jpg" data-fallback="None" alt="Paper thumbnail for Text-to-3D using Gaussian Splatting" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Text-to-3D using Gaussian Splatting <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Zilong Chen, Feng Wang, Huaping Liu</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2309.16585.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://gsgen3d.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/gsgen3d/gsgen" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://streamable.com/28snte" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">In this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN), a novel approach for generating high-quality 3D objects. Previous methods suffer from inaccurate geometry and limited fidelity due to the absence of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a recent state-of-the-art representation, to address existing shortcomings by exploiting the explicit nature that enables the incorporation of 3D prior. Specifically, our method adopts a pro- gressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under a 3D geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative refinement to enrich details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D content with delicate details and more accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="yang2023deformable" data-title="Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction" data-authors="Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin" data-year="2023" data-tags='["Code", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'yang2023deformable', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/yang2023deformable.jpg" data-fallback="None" alt="Paper thumbnail for Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2309.13101.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://ingra14m.github.io/Deformable-Gaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/ingra14m/Deformable-3D-Gaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Implicit neural representation has opened up new avenues for dynamic scene reconstruction and rendering. Nonetheless, state-of-the-art methods of dynamic neural rendering rely heavily on these implicit representations, which frequently struggle with accurately capturing the intricate details of objects in the scene. Furthermore, implicit methods struggle to achieve real-time rendering in general dynamic scenes, limiting their use in a wide range of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using explicit 3D Gaussians and learns Gaussians in canonical space with a deformation field to model monocular dynamic scenes. We also introduced a smoothing training mechanism with no extra overhead to mitigate the impact of inaccurate poses in real datasets on the smoothness of time interpolation tasks. Through differential gaussian rasterization, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time synthesis, and real-time rendering.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="luiten2023dynamic" data-title="Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis" data-authors="Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan" data-year="2023" data-tags='["Code", "Dynamic", "Project", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'luiten2023dynamic', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/luiten2023dynamic.jpg" data-fallback="None" alt="Paper thumbnail for Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan</p>
      <div class="paper-tags"><span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2308.09713.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://dynamic3dgaussians.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/JonathonLuiten/Dynamic3DGaussians" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://www.youtube.com/live/hDuy1TgD8I4?si=6oGN0IYnPRxOibpg" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians’ motion and rotation with local rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="kerblgaussian" data-title="Gaussian Splatting for Real-Time Radiance Field Rendering" data-authors="Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis" data-year="2023" data-tags='["Classic Work", "Code", "Dynamic", "Project", "Rendering", "Video"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'kerblgaussian', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/kerblgaussian.jpg" data-fallback="None" alt="Paper thumbnail for Gaussian Splatting for Real-Time Radiance Field Rendering" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Gaussian Splatting for Real-Time Radiance Field Rendering <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</p>
      <div class="paper-tags"><span class="paper-tag">Classic Work</span>
<span class="paper-tag">Code</span>
<span class="paper-tag">Dynamic</span>
<span class="paper-tag">Project</span>
<span class="paper-tag">Rendering</span>
<span class="paper-tag">Video</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2308.04079.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<a href="https://github.com/graphdeco-inria/gaussian-splatting" class="paper-link" target="_blank" rel="noopener">💻 Code</a>
<a href="https://youtu.be/T_kXY43VZnk?si=DrkbDFxQAv5scQNT" class="paper-link" target="_blank" rel="noopener">🎥 Video</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows real-time rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="shao2023control4d" data-title="Control4D: Efficient 4D Portrait Editing with Text" data-authors="Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu" data-year="2023" data-tags='["Dynamic", "Editing", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'shao2023control4d', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/shao2023control4d.jpg" data-fallback="None" alt="Paper thumbnail for Control4D: Efficient 4D Portrait Editing with Text" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Control4D: Efficient 4D Portrait Editing with Text <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu</p>
      <div class="paper-tags"><span class="paper-tag">Dynamic</span>
<span class="paper-tag">Editing</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2305.20082.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://control4darxiv.github.io/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">We introduce Control4D, an innovative framework for editing dynamic 4D portraits using text instructions. Our method addresses the prevalent challenges in 4D editing, notably the inefficiencies of existing 4D representations and the inconsistent editing effect caused by diffusion-based editors. We first propose GaussianPlanes, a novel 4D representation that makes Gaussian Splatting more structured by applying plane-based decomposition in 3D space and time. This enhances both efficiency and robustness in 4D editing. Furthermore, we propose to leverage a 4D generator to learn a more continuous generation space from inconsistent edited images produced by the diffusion-based editor, which effectively improves the consistency and quality of 4D editing. Comprehensive evaluation demonstrates the superiority of Control4D, including significantly reduced training time, high-quality rendering, and spatial-temporal consistency in 4D portrait editing.</div></div>
    </div>
  </div>
</div>
<div class="paper-row" data-id="blattmann2023align" data-title="Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models" data-authors="Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis" data-year="2023" data-tags='["Diffusion", "Project"]'>
  <div class="paper-card">
    <input type="checkbox" class="selection-checkbox" onclick="handleCheckboxClick(event, 'blattmann2023align', this)">
    <div class="paper-number"></div>
    <div class="paper-thumbnail">
      <img data-src="assets/thumbnails/blattmann2023align.jpg" data-fallback="None" alt="Paper thumbnail for Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models" class="lazy" loading="lazy"/>
    </div>
    <div class="paper-content">
      <h2 class="paper-title">Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models <span class="paper-year">(2023)</span></h2>
      <p class="paper-authors">Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis</p>
      <div class="paper-tags"><span class="paper-tag">Diffusion</span>
<span class="paper-tag">Project</span></div>
      <div class="paper-links"><a href="https://arxiv.org/pdf/2304.08818.pdf" class="paper-link" target="_blank" rel="noopener">📄 Paper</a>
<a href="https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/" class="paper-link" target="_blank" rel="noopener">🌐 Project</a>
<button class="abstract-toggle" onclick="toggleAbstract(this)">📖 Show Abstract</button>
<div class="paper-abstract">Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques.</div></div>
    </div>
  </div>
</div>
        </div>
    </div>

    <!-- JavaScript -->
    <script>
        const state = {
    selectedPapers: new Set(),
    isSelectionMode: false,
    includeTags: new Set(),
    excludeTags: new Set(),
    onlyShowSelected: false
};
function debounce(fn, delay) {
    let timeout;
    return (...args) => {
        if (timeout) clearTimeout(timeout);
        timeout = setTimeout(() => fn(...args), delay);
    };
}

function updateURL() {
    const params = new URLSearchParams();
    if (searchInput.value) {
        params.set('search', searchInput.value);
    }
    if (yearFilter.value !== 'all') {
        params.set('year', yearFilter.value);
    }
    if (state.includeTags.size > 0) {
        params.set('include', Array.from(state.includeTags).join(','));
    }
    if (state.excludeTags.size > 0) {
        params.set('exclude', Array.from(state.excludeTags).join(','));
    }
    if (state.selectedPapers.size > 0) {
        params.set('selected', Array.from(state.selectedPapers).join(','));
        if (state.onlyShowSelected) {
            params.set('show_selected', 'true');
        }
    }
    const newSearch = params.toString() ? `?${params.toString()}` : '';
    window.history.replaceState(
        { filters: params.toString() },
        '',
        `${window.location.pathname}${newSearch}`
    );
}

function updatePaperNumbers() {
    let num = 1;
    document.querySelectorAll('.paper-row.visible').forEach(row => {
        const numElem = row.querySelector('.paper-number');
        numElem.textContent = num++;
    });
}
function filterPapers() {
    // Show/hide non-paper elements regardless of filter state
    document.querySelectorAll('.papers-grid > *').forEach(el => {
        if (!el.classList.contains('paper-row')) {
            el.style.display = 'block'; // Always show headers, donation box, etc.
        }
    });

    if (state.onlyShowSelected) {
        // When showing only selected papers, hide all non-selected papers
        paperCards.forEach(row => {
            const id = row.getAttribute('data-id');
            row.classList.toggle('visible', state.selectedPapers.has(id));
        });
    } else {
        // Normal filtering
        const sTerm = searchInput.value.toLowerCase();
        const selYear = yearFilter.value;
        
        paperCards.forEach(row => {
            const title = row.getAttribute('data-title').toLowerCase();
            const authors = row.getAttribute('data-authors').toLowerCase();
            const year = row.getAttribute('data-year');
            const tags = JSON.parse(row.getAttribute('data-tags'));

            const matchSearch = title.includes(sTerm) || authors.includes(sTerm);
            const matchYear = (selYear === 'all') || (year === selYear);
            const matchInc = (state.includeTags.size === 0) || [...state.includeTags].every(t => tags.includes(t));
            const matchExc = (state.excludeTags.size === 0) || ![...state.excludeTags].some(t => tags.includes(t));

            const visible = matchSearch && matchYear && matchInc && matchExc;
            row.classList.toggle('visible', visible);
        });
    }
    
    updatePaperNumbers();
    lazyLoadInstance.update();
    updateURL();
}

function clearSearch() {
    searchInput.value = '';
    filterPapers();
}

function initializeFilters() {
    // Tag filter clicks
    tagFilters.forEach(tagFilter => {
        tagFilter.addEventListener('click', () => {
            const tag = tagFilter.getAttribute('data-tag');
            if (!tagFilter.classList.contains('include') && !tagFilter.classList.contains('exclude')) {
                tagFilter.classList.add('include');
                state.includeTags.add(tag);
            } else if (tagFilter.classList.contains('include')) {
                tagFilter.classList.remove('include');
                tagFilter.classList.add('exclude');
                state.includeTags.delete(tag);
                state.excludeTags.add(tag);
            } else {
                tagFilter.classList.remove('exclude');
                state.excludeTags.delete(tag);
            }
            filterPapers();
        });
    });

    // Search input
    searchInput.addEventListener('input', debounce(filterPapers, 150));
    
    // Year filter
    yearFilter.addEventListener('change', filterPapers);
}
function toggleSelectedOnly() {
    state.onlyShowSelected = !state.onlyShowSelected;
    const button = document.querySelector('.preview-header-right .control-button.show-selected');
    
    if (button) {
        button.innerHTML = state.onlyShowSelected ? 
            '<i class="fas fa-list"></i> Show All Papers' :
            '<i class="fas fa-filter"></i> Show Selected Only';
    }
    
    // Update the URL first
    const url = new URL(window.location.href);
    if (state.onlyShowSelected) {
        url.searchParams.set('show_selected', 'true');
    } else {
        url.searchParams.delete('show_selected');
    }
    window.history.replaceState({}, '', url.toString());
    
    // Then update the display
    filterPapers();
}

function toggleSelectionMode() {
    state.isSelectionMode = !state.isSelectionMode;
    document.body.classList.toggle('selection-mode', state.isSelectionMode);

    // Update toggle button icon and tooltip
    const toggleButton = document.querySelector('.selection-mode-toggle');
    if (toggleButton) {
        toggleButton.innerHTML = state.isSelectionMode ? 
            `<i class="fas fa-times"></i><span class="tooltip">Exit Selection Mode</span>` :
            `<i class="fas fa-list-check"></i><span class="tooltip">Enter Selection Mode</span>`;
    }

    // Handle visibility and selection display
    if (!state.isSelectionMode) {
        if (state.onlyShowSelected) {
            state.onlyShowSelected = false;
            const button = document.querySelector('.preview-header-right .control-button.show-selected');
            if (button) {
                button.innerHTML = '<i class="fas fa-filter"></i> Show Selected Only';
            }
            const url = new URL(window.location.href);
            url.searchParams.delete('show_selected');
            window.history.replaceState({}, '', url.toString());
        }
        filterPapers();
    }

    updateSelectionCount();
}

function clearSelection() {
    state.selectedPapers.clear();
    state.onlyShowSelected = false;
    document.querySelectorAll('.paper-card').forEach(card => {
        card.classList.remove('selected');
        const checkbox = card.querySelector('.selection-checkbox');
        if (checkbox) checkbox.checked = false;
    });
    document.getElementById('selectionPreview').innerHTML = '';
    
    // Update button state
    const button = document.querySelector('.preview-header-right .control-button.show-selected');
    if (button) {
        button.innerHTML = '<i class="fas fa-filter"></i> Show Selected Only';
    }
    
    // Update URL and display
    const url = new URL(window.location.href);
    url.searchParams.delete('show_selected');
    url.searchParams.delete('selected');
    window.history.replaceState({}, '', url.toString());
    
    updateSelectionCount();
    filterPapers();
}

function togglePaperSelection(paperId, checkbox) {
    if (!state.isSelectionMode) return;
    
    const paperCard = checkbox.closest('.paper-card');
    const paperRow = paperCard.closest('.paper-row');

    if (checkbox.checked) {
        // Add to selection
        state.selectedPapers.add(paperId);
        paperCard.classList.add('selected');

        // Create preview item
        const title = paperRow.getAttribute('data-title');
        const authors = paperRow.getAttribute('data-authors');
        const year = paperRow.getAttribute('data-year');

        const previewItem = document.createElement('div');
        previewItem.className = 'preview-item';
        previewItem.setAttribute('data-paper-id', paperId);
        previewItem.innerHTML = `
            <div class="preview-content" onclick="scrollToPaper('${paperId}')">
                <div class="preview-title">${title} (${year})</div>
                <div class="preview-authors">${authors}</div>
            </div>
            <button class="preview-remove" onclick="event.stopPropagation(); removeFromSelection('${paperId}')">
                <i class="fas fa-times"></i>
            </button>
        `;
        document.getElementById('selectionPreview').appendChild(previewItem);
    } else {
        removeFromSelection(paperId);
    }
    updateSelectionCount();
    if (state.onlyShowSelected) {
        filterPapers();
    }
    updateURL();
}

function removeFromSelection(paperId) {
    const checkbox = document.querySelector(`.paper-row[data-id="${paperId}"] .selection-checkbox`);
    if (checkbox) {
        checkbox.checked = false;
        state.selectedPapers.delete(paperId);

        const paperCard = checkbox.closest('.paper-card');
        if (paperCard) {
            paperCard.classList.remove('selected');
        }

        const previewItem = document.querySelector(`.preview-item[data-paper-id="${paperId}"]`);
        if (previewItem) {
            previewItem.remove();
        }

        updateSelectionCount();
        if (state.onlyShowSelected) {
            filterPapers();
        }
        updateURL();
    }
}

function updateSelectionCount() {
    const counter = document.querySelector('.selection-counter');
    counter.textContent = `${state.selectedPapers.size} paper${state.selectedPapers.size === 1 ? '' : 's'} selected`;
}

function handleCheckboxClick(ev, paperId, checkbox) {
    ev.stopPropagation();
    togglePaperSelection(paperId, checkbox);
}

function scrollToPaper(paperId) {
    const paperRow = document.querySelector(`.paper-row[data-id="${paperId}"]`);
    if (paperRow) {
        paperRow.scrollIntoView({ behavior: 'smooth', block: 'center' });
        
        const paperCard = paperRow.querySelector('.paper-card');
        if (paperCard) {
            paperCard.style.transition = 'background-color 0.3s ease';
            paperCard.style.backgroundColor = '#f0f9ff';
            setTimeout(() => {
                paperCard.style.backgroundColor = '';
            }, 1500);
        }
    }
}
function showShareModal() {
    if (state.selectedPapers.size === 0) {
        alert('Please select at least one paper to share.');
        return;
    }
    const shareUrl = new URL(window.location.href);
    shareUrl.searchParams.set('selected', Array.from(state.selectedPapers).join(','));
    if (state.onlyShowSelected) {
        shareUrl.searchParams.set('show_selected', 'true');
    } else {
        shareUrl.searchParams.delete('show_selected');
    }
    document.getElementById('shareUrl').value = shareUrl.toString();
    document.getElementById('shareModal').classList.add('show');
}

function hideShareModal() {
    document.getElementById('shareModal').classList.remove('show');
}

async function copyShareLink() {
    const shareUrl = document.getElementById('shareUrl');
    try {
        await navigator.clipboard.writeText(shareUrl.value);
        const copyButton = document.querySelector('.share-url-container .control-button');
        const origText = copyButton.innerHTML;
        copyButton.innerHTML = '<i class="fas fa-check"></i> Copied!';
        setTimeout(() => {
            copyButton.innerHTML = origText;
        }, 2000);
    } catch(e) {
        alert('Failed to copy link. Please copy manually.');
    }
}

function copyBitcoinAddress() {
    const address = document.querySelector('.bitcoin-address').textContent;
    navigator.clipboard.writeText(address).then(() => {
        const button = document.querySelector('.copy-button');
        const originalText = button.innerHTML;
        button.innerHTML = '<i class="fas fa-check"></i> Copied!';
        setTimeout(() => {
            button.innerHTML = originalText;
        }, 2000);
    });
}

function applyURLParams() {
    const params = new URLSearchParams(window.location.search);
    
    // First, check if we have selected papers
    const selPapers = params.get('selected');
    if (selPapers) {
        const arr = selPapers.split(',');
        if (arr.length > 0) {
            // Enter selection mode
            if (!state.isSelectionMode) {
                toggleSelectionMode();
            }
            
            // Select the papers first
            arr.forEach(id => {
                const row = document.querySelector(`.paper-row[data-id="${id}"]`);
                if (row) {
                    const cb = row.querySelector('.selection-checkbox');
                    if (cb) {
                        cb.checked = true;
                        togglePaperSelection(id, cb);
                    }
                }
            });
            
            // Then check if we should show only selected papers
            const showSelected = params.get('show_selected');
            if (showSelected === 'true') {
                state.onlyShowSelected = true;
                const button = document.querySelector('.preview-header-right .control-button.show-selected');
                if (button) {
                    button.innerHTML = '<i class="fas fa-list"></i> Show All Papers';
                }
                filterPapers(); // Apply the filter to show only selected papers
            }
        }
    }
    
    // Handle other filters
    const searchTerm = params.get('search');
    if (searchTerm) {
        searchInput.value = searchTerm;
    }
    
    const yr = params.get('year');
    if (yr) {
        yearFilter.value = yr;
    }

    const inc = params.get('include');
    if (inc) {
        state.includeTags = new Set(inc.split(','));
        state.includeTags.forEach(t => {
            const tf = document.querySelector(`.tag-filter[data-tag="${t}"]`);
            if (tf) tf.classList.add('include');
        });
    }
    
    const exc = params.get('exclude');
    if (exc) {
        state.excludeTags = new Set(exc.split(','));
        state.excludeTags.forEach(t => {
            const tf = document.querySelector(`.tag-filter[data-tag="${t}"]`);
            if (tf) tf.classList.add('exclude');
        });
    }
    
    // Final filter application
    filterPapers();
}
// Navigation controls
function scrollToTop() {
    window.scrollTo({
        top: 0,
        behavior: 'smooth'
    });
}

function scrollToBottom() {
    window.scrollTo({
        top: document.documentElement.scrollHeight,
        behavior: 'smooth'
    });
}

// Update scroll progress
function updateScrollProgress() {
    const winScroll = document.documentElement.scrollTop;
    const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    const scrolled = Math.round((winScroll / height) * 100);
    document.querySelector('.scroll-progress').textContent = `${scrolled}%`;
}

// Filter status functionality
function updateFilterStatus() {
    const visiblePapers = document.querySelectorAll('.paper-row.visible').length;
    const totalPapers = document.querySelectorAll('.paper-row').length;
    
    document.getElementById('visibleCount').textContent = visiblePapers;
    document.getElementById('totalCount').textContent = totalPapers;

    const activeFiltersEl = document.getElementById('activeFilters');
    activeFiltersEl.innerHTML = '';

    // Search filter
    const searchTerm = document.getElementById('searchInput').value;
    if (searchTerm) {
        const searchTag = createFilterTag('search', 'Search Filter', searchTerm);
        searchTag.querySelector('button').addEventListener('click', () => {
            document.getElementById('searchInput').value = '';
            filterPapers();
        });
        activeFiltersEl.appendChild(searchTag);
    }

    // Year filter
    const yearFilter = document.getElementById('yearFilter').value;
    if (yearFilter !== 'all') {
        const yearTag = createFilterTag('year', 'Year Filter', yearFilter);
        yearTag.querySelector('button').addEventListener('click', () => {
            document.getElementById('yearFilter').value = 'all';
            filterPapers();
        });
        activeFiltersEl.appendChild(yearTag);
    }

    // Tag filters
    document.querySelectorAll('.tag-filter').forEach(tagEl => {
        if (tagEl.classList.contains('include') || tagEl.classList.contains('exclude')) {
            const tagText = tagEl.getAttribute('data-tag');
            const type = tagEl.classList.contains('include') ? 'Including' : 'Excluding';
            const tagTag = createFilterTag('tag', `${type} Tag`, tagText);
            
            // Update the click handler to completely remove the tag
            tagTag.querySelector('button').addEventListener('click', () => {
                tagEl.classList.remove('include', 'exclude');
                state.includeTags.delete(tagText);
                state.excludeTags.delete(tagText);
                filterPapers();
            });
            
            activeFiltersEl.appendChild(tagTag);
        }
    });
}

function createFilterTag(type, title, info) {
    const tag = document.createElement('div');
    tag.className = `filter-tag ${type}`;
    
    tag.innerHTML = `
        <div class="filter-tag-content">
            <div class="filter-tag-title">${title}</div>
            <div class="filter-tag-info">${info}</div>
        </div>
        <button class="preview-remove" onclick="event.stopPropagation();" aria-label="Remove filter">
            <i class="fas fa-times"></i>
        </button>
    `;
    
    return tag;
}

function clearAllFilters() {
    // Clear search
    document.getElementById('searchInput').value = '';
    
    // Reset year filter
    document.getElementById('yearFilter').value = 'all';
    
    // Clear all tag filters completely (don't toggle through states)
    document.querySelectorAll('.tag-filter').forEach(tag => {
        tag.classList.remove('include', 'exclude');
        state.includeTags.delete(tag.getAttribute('data-tag'));
        state.excludeTags.delete(tag.getAttribute('data-tag'));
    });
    
    // Update the UI
    filterPapers();
    updateFilterStatus();
}


// Initialize
document.addEventListener('DOMContentLoaded', function() {
    // Set initial paper counts
    updateFilterStatus();
    
    // Add scroll listener
    window.addEventListener('scroll', updateScrollProgress);
    
    // Override the existing filterPapers function to update filter status
    const originalFilterPapers = window.filterPapers;
    window.filterPapers = function() {
        originalFilterPapers();
        updateFilterStatus();
    };
});
document.addEventListener('DOMContentLoaded', function() {
    // Initialize variables
    window.paperCards = document.querySelectorAll('.paper-row');
    window.searchInput = document.getElementById('searchInput');
    window.yearFilter = document.getElementById('yearFilter');
    window.tagFilters = document.querySelectorAll('.tag-filter');
    
    // Add toggleAbstract to window object so it's globally accessible
    window.toggleAbstract = function(button) {
        const abstract = button.nextElementSibling;
        const isShowing = abstract.classList.toggle('show');
        button.innerHTML = isShowing ? '📖 Hide Abstract' : '📖 Show Abstract';
    };

    // Initialize LazyLoad
    window.lazyLoadInstance = new LazyLoad({
        elements_selector: ".lazy",
        callback_error: (img) => {
            if (img.dataset.fallback) {
                img.src = img.dataset.fallback;
            }
        },
        callback_loaded: (img) => {
            img.classList.add('loaded');
        }
    });

    // Initialize filters
    initializeFilters();

    // Initialize paper card events
    document.querySelectorAll('.paper-card').forEach(card => {
        card.addEventListener('click', (ev) => {
            if (!state.isSelectionMode) return;
            // if click on link or abstract btn, ignore
            if (
                ev.target.classList.contains('paper-link') ||
                ev.target.closest('.paper-link') ||
                ev.target.classList.contains('abstract-toggle')
            ) {
                return;
            }
            const checkbox = card.querySelector('.selection-checkbox');
            if (checkbox && ev.target !== checkbox) {
                checkbox.checked = !checkbox.checked;
                const pid = card.parentElement.getAttribute('data-id');
                togglePaperSelection(pid, checkbox);
            }
        });
    });

    // Apply URL parameters
    applyURLParams();

    // Show initial papers
    filterPapers();
    updatePaperNumbers();

    // Expose global functions for HTML onclick handlers
    window.copyBitcoinAddress = copyBitcoinAddress;
    window.clearSearch = clearSearch;
    window.toggleSelectionMode = toggleSelectionMode;
    window.clearSelection = clearSelection;
    window.showShareModal = showShareModal;
    window.hideShareModal = hideShareModal;
    window.copyShareLink = copyShareLink;
    window.removeFromSelection = removeFromSelection;
    window.togglePaperSelection = togglePaperSelection;
    window.handleCheckboxClick = handleCheckboxClick;
    window.scrollToPaper = scrollToPaper;
});

    </script>
</body>
</html>